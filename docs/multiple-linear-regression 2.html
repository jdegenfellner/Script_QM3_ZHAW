<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Multiple Linear Regression | Quantitative Methods 2, ZHAW</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Multiple Linear Regression | Quantitative Methods 2, ZHAW" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Multiple Linear Regression | Quantitative Methods 2, ZHAW" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Jürgen Degenfellner" />


<meta name="date" content="2025-02-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>
<link rel="next" href="reliability-and-validity.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods 2</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#books-we-will-heavily-borrow-from-are"><i class="fa fa-check"></i><b>1.1</b> Books we will heavily borrow from are:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#if-you-need-a-good-reason-to-buy-great-books"><i class="fa fa-check"></i><b>1.2</b> If you need a good reason to buy great books…</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-statistical-modeling-and-what-do-we-need-this-for"><i class="fa fa-check"></i><b>2.1</b> What is statistical modeling and what do we need this for?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#explanatory-vs.-predictive-models"><i class="fa fa-check"></i><b>2.1.1</b> Explanatory vs. Predictive Models</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#individual-vs.-population-prediction"><i class="fa fa-check"></i><b>2.1.2</b> Individual vs. Population Prediction</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#practical-use-of-statistical-models"><i class="fa fa-check"></i><b>2.1.3</b> Practical Use of Statistical Models</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#start-at-the-beginning"><i class="fa fa-check"></i><b>2.1.4</b> Start at the beginning</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#a-simple-model-for-adult-body-heights-in-the-bayesian-framework"><i class="fa fa-check"></i><b>2.2</b> A (simple) model for adult body heights in the Bayesian framework</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#classical_simple_mean_model"><i class="fa fa-check"></i><b>2.3</b> Classical approach for the simplest model</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#exercise1_Intro"><i class="fa fa-check"></i><b>2.4.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#exercise2_Intro"><i class="fa fa-check"></i><b>2.4.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#exercise3_Intro"><i class="fa fa-check"></i><b>2.4.3</b> [H] Exercise 3</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro.html"><a href="intro.html#exercise4_Intro"><i class="fa fa-check"></i><b>2.4.4</b> [M] Exercise 4</a></li>
<li class="chapter" data-level="2.4.5" data-path="intro.html"><a href="intro.html#exercise5_Intro"><i class="fa fa-check"></i><b>2.4.5</b> [M] Exercise 5</a></li>
<li class="chapter" data-level="2.4.6" data-path="intro.html"><a href="intro.html#exercise6_Intro"><i class="fa fa-check"></i><b>2.4.6</b> [M] Exercise 6</a></li>
<li class="chapter" data-level="2.4.7" data-path="intro.html"><a href="intro.html#exercise7_Intro"><i class="fa fa-check"></i><b>2.4.7</b> [H] Exercise 7</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#addendum"><i class="fa fa-check"></i><b>2.5</b> Addendum</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#bivariate_normal"><i class="fa fa-check"></i><b>2.5.1</b> The bivariate normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple_lin_reg_bayes"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition"><i class="fa fa-check"></i><b>3.1.1</b> Model definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#priors"><i class="fa fa-check"></i><b>3.1.2</b> Priors</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit-model"><i class="fa fa-check"></i><b>3.1.3</b> Fit model</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#result"><i class="fa fa-check"></i><b>3.1.4</b> Result</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#credible-bands"><i class="fa fa-check"></i><b>3.1.5</b> Credible bands</a></li>
<li class="chapter" data-level="3.1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary"><i class="fa fa-check"></i><b>3.1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-in-the-frequentist-framework"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition-1"><i class="fa fa-check"></i><b>3.2.1</b> Model definition</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic"><i class="fa fa-check"></i><b>3.2.2</b> Fit the model</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence_intervals_frequentist"><i class="fa fa-check"></i><b>3.2.3</b> Confidence Intervals of coefficients (Frequentist)</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analysis_of_variance"><i class="fa fa-check"></i><b>3.2.4</b> ANOVA (Analysis of Variance)</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r2---coefficient-of-determination"><i class="fa fa-check"></i><b>3.2.5</b> <span class="math inline">\(R^2\)</span> - Coefficient of Determination</a></li>
<li class="chapter" data-level="3.2.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#check-regression-assumptions"><i class="fa fa-check"></i><b>3.2.6</b> Check regression assumptions</a></li>
<li class="chapter" data-level="3.2.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#bootstrap-fit"><i class="fa fa-check"></i><b>3.2.7</b> Bootstrap fit</a></li>
<li class="chapter" data-level="3.2.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-towards-the-mean"><i class="fa fa-check"></i><b>3.2.8</b> Regression towards the mean</a></li>
<li class="chapter" data-level="3.2.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#random-x-vs.-fixed-x"><i class="fa fa-check"></i><b>3.2.9</b> Random X vs. fixed X</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise1_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="3.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise2_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="3.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise3_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="3.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise4_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.4</b> [H] Exercise 4</a></li>
<li class="chapter" data-level="3.3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise5_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.5</b> [M] Exercise 5</a></li>
<li class="chapter" data-level="3.3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise6_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.6</b> [H] Exercise 6</a></li>
<li class="chapter" data-level="3.3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise7_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.7</b> [M] Exercise 7</a></li>
<li class="chapter" data-level="3.3.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise8_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.8</b> [E] Exercise 8</a></li>
<li class="chapter" data-level="3.3.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise9_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.9</b> [M] Exercise 9</a></li>
<li class="chapter" data-level="3.3.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise10_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.10</b> [M] Exercise 10</a></li>
<li class="chapter" data-level="3.3.11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise11_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.11</b> [E] Exercise 11</a></li>
<li class="chapter" data-level="3.3.12" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise12_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.12</b> [M] Exercise 12</a></li>
<li class="chapter" data-level="3.3.13" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise13_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.13</b> [M] Exercise 13</a></li>
<li class="chapter" data-level="3.3.14" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise14_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.14</b> [M] Exercise 14</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#elearning-1"><i class="fa fa-check"></i><b>3.4</b> eLearning 1</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-bayesian-framework"><i class="fa fa-check"></i><b>4.1</b> Linear Regression with 2 predictors in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#meaning-of-linear"><i class="fa fa-check"></i><b>4.1.1</b> Meaning of “linear”</a></li>
<li class="chapter" data-level="4.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_transformed_predictor_bayes"><i class="fa fa-check"></i><b>4.1.2</b> Adding a transformed predictor to the model</a></li>
<li class="chapter" data-level="4.1.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_predictor_bayes"><i class="fa fa-check"></i><b>4.1.3</b> Adding another predictor to the model</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-frequentist-framework"><i class="fa fa-check"></i><b>4.2</b> Linear regression with 2 predictors in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_transformed_predictor_freq"><i class="fa fa-check"></i><b>4.2.1</b> Adding a transformed predictor to the model</a></li>
<li class="chapter" data-level="4.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_predictor_freq"><i class="fa fa-check"></i><b>4.2.2</b> Adding another predictor to the model</a></li>
<li class="chapter" data-level="4.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interaction_term"><i class="fa fa-check"></i><b>4.2.3</b> Interaction Term <span class="math inline">\(X_1 \times X_2\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interaction_plot"><i class="fa fa-check"></i><b>4.2.4</b> Using an interaction plot to see a potential interaction</a></li>
<li class="chapter" data-level="4.2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simpsons_paradox"><i class="fa fa-check"></i><b>4.2.5</b> Simpsons Paradox</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#throwing_variables"><i class="fa fa-check"></i><b>4.3</b> What happens when you just throw variables into multiple regression?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#pipe"><i class="fa fa-check"></i><b>4.3.1</b> Pipe</a></li>
<li class="chapter" data-level="4.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#fork"><i class="fa fa-check"></i><b>4.3.2</b> Fork</a></li>
<li class="chapter" data-level="4.3.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#collider"><i class="fa fa-check"></i><b>4.3.3</b> Collider</a></li>
<li class="chapter" data-level="4.3.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>4.3.4</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#more-than-2-predictors"><i class="fa fa-check"></i><b>4.4</b> More than 2 predictors</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#example_nhanes"><i class="fa fa-check"></i><b>4.4.1</b> Example in NHANES data</a></li>
<li class="chapter" data-level="4.4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#concluding-remarks"><i class="fa fa-check"></i><b>4.4.2</b> Concluding remarks</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>4.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise1_multiple_regression"><i class="fa fa-check"></i><b>4.5.1</b> [M] Exercise 1</a></li>
<li class="chapter" data-level="4.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise2_multiple_regression"><i class="fa fa-check"></i><b>4.5.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="4.5.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise3_multiple_regression"><i class="fa fa-check"></i><b>4.5.3</b> [H] Exercise 3</a></li>
<li class="chapter" data-level="4.5.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise4_multiple_regression"><i class="fa fa-check"></i><b>4.5.4</b> [E] Exercise 4</a></li>
<li class="chapter" data-level="4.5.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise5_multiple_regression"><i class="fa fa-check"></i><b>4.5.5</b> [E] Exercise 5</a></li>
<li class="chapter" data-level="4.5.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise6_multiple_regression"><i class="fa fa-check"></i><b>4.5.6</b> [M] Exercise 6</a></li>
<li class="chapter" data-level="4.5.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise7_multiple_regression"><i class="fa fa-check"></i><b>4.5.7</b> [E] Exercise 7</a></li>
<li class="chapter" data-level="4.5.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise8_multiple_regression"><i class="fa fa-check"></i><b>4.5.8</b> [M] Exercise 8</a></li>
<li class="chapter" data-level="4.5.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise9_multiple_regression"><i class="fa fa-check"></i><b>4.5.9</b> [M] Exercise 9</a></li>
<li class="chapter" data-level="4.5.10" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise10_multiple_regression"><i class="fa fa-check"></i><b>4.5.10</b> [E] Exercise 10</a></li>
<li class="chapter" data-level="4.5.11" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise11_multiple_regression"><i class="fa fa-check"></i><b>4.5.11</b> [E] Exercise 11</a></li>
<li class="chapter" data-level="4.5.12" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise12_multiple_regression"><i class="fa fa-check"></i><b>4.5.12</b> [M] Exercise 12</a></li>
<li class="chapter" data-level="4.5.13" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise13_multiple_regression"><i class="fa fa-check"></i><b>4.5.13</b> [H] Exercise 13</a></li>
<li class="chapter" data-level="4.5.14" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise14_multiple_regression"><i class="fa fa-check"></i><b>4.5.14</b> [H] Exercise 14</a></li>
<li class="chapter" data-level="4.5.15" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise15_multiple_regression"><i class="fa fa-check"></i><b>4.5.15</b> [H] Exercise 15</a></li>
<li class="chapter" data-level="4.5.16" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise16_multiple_regression"><i class="fa fa-check"></i><b>4.5.16</b> [M] Exercise 16</a></li>
<li class="chapter" data-level="4.5.17" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise17_multiple_regression"><i class="fa fa-check"></i><b>4.5.17</b> [H] Exercise 17</a></li>
<li class="chapter" data-level="4.5.18" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise18_multiple_regression"><i class="fa fa-check"></i><b>4.5.18</b> [H] Exercise 18</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html"><i class="fa fa-check"></i><b>5</b> Reliability and Validity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#reliability"><i class="fa fa-check"></i><b>5.1</b> Reliability</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#peter-and-marys-rom-measurements"><i class="fa fa-check"></i><b>5.1.1</b> Peter and Mary’s ROM measurements</a></li>
<li class="chapter" data-level="5.1.2" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#intraclass-correlation-coefficient-icc"><i class="fa fa-check"></i><b>5.1.2</b> Intraclass Correlation Coefficient (ICC)</a></li>
<li class="chapter" data-level="5.1.3" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#explanation-of-iccs-in-the-psych-output"><i class="fa fa-check"></i><b>5.1.3</b> Explanation of ICCs in the <code>psych</code> output</a></li>
<li class="chapter" data-level="5.1.4" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#difference-between-correlation-and-icc"><i class="fa fa-check"></i><b>5.1.4</b> Difference between correlation and ICC</a></li>
<li class="chapter" data-level="5.1.5" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#standard-error-of-measurement-sem"><i class="fa fa-check"></i><b>5.1.5</b> Standard Error of Measurement (SEM)</a></li>
<li class="chapter" data-level="5.1.6" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#bland-altman-plot"><i class="fa fa-check"></i><b>5.1.6</b> Bland-Altman Plot</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#validity"><i class="fa fa-check"></i><b>5.2</b> Validity</a></li>
<li class="chapter" data-level="5.3" data-path="reliability-and-validity.html"><a href="reliability-and-validity.html#todos"><i class="fa fa-check"></i><b>5.3</b> TODOS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods 2, ZHAW</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Multiple Linear Regression<a href="multiple-linear-regression.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>So far, we have dealt with the simple mean model and the model with one predictor
in the Bayesian and Frequentist framework.
We will now add another predictor and subsequently an interaction term to the model.
Finally, we will add more than two predictors to the model.</p>
<p>If you feel confused at any point: As Richard McElreath repeatedly says:
This is normal, it means you are paying attention. I also refer to the
great <a href="https://www.youtube.com/watch?v=lytxafTXg6c&amp;ab_channel=sdfhsfh">Richard Feynman</a>.</p>
<div id="linear-regression-with-2-predictors-in-the-bayesian-framework" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear Regression with 2 predictors in the Bayesian Framework<a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-bayesian-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="meaning-of-linear" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Meaning of “linear”<a href="multiple-linear-regression.html#meaning-of-linear" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What is a linear model? The term “linear” refers to the relationship of the predictors
with the dependent variable (or outcome). The following model is also linear:</p>
<p><span class="math display">\[height_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2\]</span></p>
<p>The model is linear in the parameters <span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span> but not in the predictors <span class="math inline">\(x_i\)</span>.
The term <span class="math inline">\(x_i^2\)</span> is ok, since the heights are just sums of multiples of the predictors (which can be nonlinear).
This model is not a linear model anymore:</p>
<p><span class="math display">\[height_i = \beta_0 + \beta_1 x_i + e^{\beta_2 x_i^2}\]</span></p>
<p><span class="math inline">\(\beta_2\)</span> is now is the exponent of <span class="math inline">\(e\)</span>. It would also not be linear,
if the coefficients are in a square root or in the denominator of a fraction,
or in a sine or in a logarithm. You get the idea.</p>
<p>Here is an easy way to check if the model is linear: If I change the predictor-value
(i.e.,the value of <span class="math inline">\(x_i\)</span>, <span class="math inline">\(x_i^2\)</span> or whatever your predictor is) by one unit,
the change in the (expected value of the) dependent variable is the coefficient in front of
the predictor (<span class="math inline">\(\beta_i\)</span>).</p>
</div>
<div id="adding_transformed_predictor_bayes" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Adding a transformed predictor to the model<a href="multiple-linear-regression.html#adding_transformed_predictor_bayes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Around 4.5. in the book <a href="https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf">Statistical Rethinking</a>
there is are lineare regression using a quadratic term for weight.
It is a principle, called the “<strong>variable inclusion principle</strong>”, that we always include the lower order terms when fitting a model
with higher order terms. See <a href="https://vdoc.pub/documents/understanding-regression-analysis-a-conditional-distribution-approach-84oqjr8sqva0">Westfall</a>,
p. 213. If we do not include the lower order terms, the coefficient does not measure what
we want it to meausure (curvature in our case). For instance, if we want to model a quadratic relationship (parabola) between
weight and height, we also have to include the linear term for weight (<span class="math inline">\(x_i\)</span>).
Since we do not assume the relationship between weight and height to be linear but
quadratic (which is a polynomial of degree 2), we call this a
<a href="https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=In%20statistics%2C%20polynomial%20regression%20is,nth%20degree%20polynomial%20in%20x.">polynomial regression</a>.
This <a href="https://www.youtube.com/watch?v=QptI-vDle8Y&amp;ab_channel=MikeXCohen">video</a> could be instructive.
One has to be careful with fitting polynomials to data points since the regression coefficients
can become quite large. Using a polynomial of high degree implies to have a lot of parameters
to estimate. Increasing the degree of the polynomial increases <span class="math inline">\(R^2\)</span> but also the risk of overfitting.
(see Statistical Rethinking p. 200). So this is - of course - not the final solution to regression problems.</p>
<p>This time, lets look at the whole age range of data from the !Kung San people.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="multiple-linear-regression.html#cb109-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb109-2"><a href="multiple-linear-regression.html#cb109-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb109-3"><a href="multiple-linear-regression.html#cb109-3" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb109-4"><a href="multiple-linear-regression.html#cb109-4" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb109-5"><a href="multiple-linear-regression.html#cb109-5" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> weight, <span class="at">y =</span> height)) <span class="sc">+</span></span>
<span id="cb109-6"><a href="multiple-linear-regression.html#cb109-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb109-7"><a href="multiple-linear-regression.html#cb109-7" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb109-8"><a href="multiple-linear-regression.html#cb109-8" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>It would not be a good idea to fit a linear trend through this data,
because we would not caupture the relationship adequately.
The red line is a <a href="https://en.wikipedia.org/wiki/Local_regression">loess smothing</a> line
which is often used to capture non-linear relationships.
The blue line is the usual line from classic linear regression (from the previous chapter).
Which one describes the data more accurately?
In this case it is obvious, a non-linear relationship is present and it might be a good idea
to model it. Modeling the relationship with a linear trend leads to bad residuals with structure.
We will demonstrate this in the freuqentist setting.
Unfortunately, in more complex settings, with more predictors, it is not always so easy to see.</p>
<p>This time, we use the mean for the prior from the book (<span class="math inline">\(178 cm\)</span>).
The model equations are (see <a href="multiple-linear-regression.html#exercise2_multiple_regression">exercise 2</a>):</p>
<p><span class="math display">\[\begin{eqnarray*}
h_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;=&amp; \alpha + \beta_1 x_i + \beta_2 x_i^2 \\
\alpha &amp;\sim&amp; \text{Normal}(178, 20) \\
\beta_1 &amp;\sim&amp; \text{Log-Normal}(0, 1) \\
\beta_2 &amp;\sim&amp; \text{Normal}(0, 1) \\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<p>The prior for <span class="math inline">\(\beta_1\)</span> is log-normal, because we can reasonably assume
the the overall linear trend is positive.
The prior for <span class="math inline">\(\beta_2\)</span> is normal, because
we are not so sure about the sign yet. If we thought back to our school days to the topic of
“curve discussion” or parabolas, we could probably also assume that <span class="math inline">\(\beta_2\)</span> is negative.
But, data will show.</p>
<p>How can we interpret the model equations?
The model assumes that the <strong>expected</strong> height <span class="math inline">\(\mu_i\)</span> of a person <span class="math inline">\(i\)</span>
depends non-linearly (quadratically) on the (standardized) weight <span class="math inline">\(x_i\)</span> of the person.
We are in the business of mean-modeling.
The prior for <span class="math inline">\(\sigma\)</span> is uniform as before.
The prior for <span class="math inline">\(\alpha\)</span> is normal with mean <span class="math inline">\(178\)</span> and standard deviation <span class="math inline">\(20\)</span>
because this is what we can expect from body heights in our experience.</p>
<p>Let’s <strong>fit the model</strong>:</p>
<p>We standardize the weight again and add the squared weights to the data set.
Standardizing the predictors is a good idea, especially in polynomial regression
since squares and cubes of large numbers can get huge and cause numerical problems.</p>
<p>Let’s fit the model with the quadratic term for weight:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multiple-linear-regression.html#cb111-1" tabindex="-1"></a><span class="co"># Standardize weight</span></span>
<span id="cb111-2"><a href="multiple-linear-regression.html#cb111-2" tabindex="-1"></a>d<span class="sc">$</span>weight_s <span class="ot">&lt;-</span> (d<span class="sc">$</span>weight <span class="sc">-</span> <span class="fu">mean</span>(d<span class="sc">$</span>weight)) <span class="sc">/</span> <span class="fu">sd</span>(d<span class="sc">$</span>weight)</span>
<span id="cb111-3"><a href="multiple-linear-regression.html#cb111-3" tabindex="-1"></a><span class="co"># Square of standardized weight</span></span>
<span id="cb111-4"><a href="multiple-linear-regression.html#cb111-4" tabindex="-1"></a>d<span class="sc">$</span>weight_s2 <span class="ot">&lt;-</span> d<span class="sc">$</span>weight_s<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb111-5"><a href="multiple-linear-regression.html#cb111-5" tabindex="-1"></a>m4<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb111-6"><a href="multiple-linear-regression.html#cb111-6" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb111-7"><a href="multiple-linear-regression.html#cb111-7" tabindex="-1"></a>    height <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb111-8"><a href="multiple-linear-regression.html#cb111-8" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> b1<span class="sc">*</span>weight_s <span class="sc">+</span> b2<span class="sc">*</span>weight_s<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb111-9"><a href="multiple-linear-regression.html#cb111-9" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">178</span>, <span class="dv">20</span>),</span>
<span id="cb111-10"><a href="multiple-linear-regression.html#cb111-10" tabindex="-1"></a>    b1 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb111-11"><a href="multiple-linear-regression.html#cb111-11" tabindex="-1"></a>    b2 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb111-12"><a href="multiple-linear-regression.html#cb111-12" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb111-13"><a href="multiple-linear-regression.html#cb111-13" tabindex="-1"></a>  ), <span class="at">data =</span> d)</span>
<span id="cb111-14"><a href="multiple-linear-regression.html#cb111-14" tabindex="-1"></a><span class="fu">precis</span>(m4<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##             mean        sd       5.5%      94.5%
## a     146.672739 0.3736465 146.075580 147.269898
## b1     21.397637 0.2898827  20.934348  21.860925
## b2     -8.419933 0.2813308  -8.869554  -7.970312
## sigma   5.750550 0.1743749   5.471865   6.029235</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="multiple-linear-regression.html#cb113-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">precis</span>(m4<span class="fl">.1</span>, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;b1&quot;</span>, <span class="st">&quot;b2&quot;</span>, <span class="st">&quot;sigma&quot;</span>)))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p><span class="math inline">\(\beta_2\)</span> is indeed negative.
In the estimate-plot above I have left out the intercept <span class="math inline">\(\alpha\)</span> to
make the other coefficients more visible. As you can see, the credible intervals
are very tight. Using prior knowledge an data, the model is very sure about the
coefficients.
We get our <strong><a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint distribution</a></strong>
of the <strong>four model parameters</strong>.
Let’s look at the fit using the mean estimates of the posterior distribution:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="multiple-linear-regression.html#cb114-1" tabindex="-1"></a><span class="co"># Summarize the model parameters</span></span>
<span id="cb114-2"><a href="multiple-linear-regression.html#cb114-2" tabindex="-1"></a>model_summary <span class="ot">&lt;-</span> <span class="fu">precis</span>(m4<span class="fl">.1</span>)</span>
<span id="cb114-3"><a href="multiple-linear-regression.html#cb114-3" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(model_summary)</span>
<span id="cb114-4"><a href="multiple-linear-regression.html#cb114-4" tabindex="-1"></a></span>
<span id="cb114-5"><a href="multiple-linear-regression.html#cb114-5" tabindex="-1"></a><span class="co"># Extract parameter values</span></span>
<span id="cb114-6"><a href="multiple-linear-regression.html#cb114-6" tabindex="-1"></a>a <span class="ot">&lt;-</span> params[<span class="st">&quot;a&quot;</span>, <span class="st">&quot;mean&quot;</span>]       <span class="co"># Intercept</span></span>
<span id="cb114-7"><a href="multiple-linear-regression.html#cb114-7" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> params[<span class="st">&quot;b1&quot;</span>, <span class="st">&quot;mean&quot;</span>]     <span class="co"># Coefficient for standardized weight</span></span>
<span id="cb114-8"><a href="multiple-linear-regression.html#cb114-8" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> params[<span class="st">&quot;b2&quot;</span>, <span class="st">&quot;mean&quot;</span>]     <span class="co"># Coefficient for squared standardized weight</span></span>
<span id="cb114-9"><a href="multiple-linear-regression.html#cb114-9" tabindex="-1"></a></span>
<span id="cb114-10"><a href="multiple-linear-regression.html#cb114-10" tabindex="-1"></a><span class="co"># Generate a sequence of standardized weights for the fitted curve</span></span>
<span id="cb114-11"><a href="multiple-linear-regression.html#cb114-11" tabindex="-1"></a>weight_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>weight_s), <span class="fu">max</span>(d<span class="sc">$</span>weight_s), <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb114-12"><a href="multiple-linear-regression.html#cb114-12" tabindex="-1"></a></span>
<span id="cb114-13"><a href="multiple-linear-regression.html#cb114-13" tabindex="-1"></a><span class="co"># Calculate the fitted values using the quadratic equation</span></span>
<span id="cb114-14"><a href="multiple-linear-regression.html#cb114-14" tabindex="-1"></a>height_fitted <span class="ot">&lt;-</span> a <span class="sc">+</span> b1 <span class="sc">*</span> weight_seq <span class="sc">+</span> b2 <span class="sc">*</span> weight_seq<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb114-15"><a href="multiple-linear-regression.html#cb114-15" tabindex="-1"></a></span>
<span id="cb114-16"><a href="multiple-linear-regression.html#cb114-16" tabindex="-1"></a><span class="co"># Plot the scatterplot</span></span>
<span id="cb114-17"><a href="multiple-linear-regression.html#cb114-17" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>weight_s, d<span class="sc">$</span>height, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb114-18"><a href="multiple-linear-regression.html#cb114-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Standardized Weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Height (cm)&quot;</span>,</span>
<span id="cb114-19"><a href="multiple-linear-regression.html#cb114-19" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Scatterplot with Fitted Curve (Standardized Weight)&quot;</span>)</span>
<span id="cb114-20"><a href="multiple-linear-regression.html#cb114-20" tabindex="-1"></a></span>
<span id="cb114-21"><a href="multiple-linear-regression.html#cb114-21" tabindex="-1"></a><span class="co"># Add the fitted curve</span></span>
<span id="cb114-22"><a href="multiple-linear-regression.html#cb114-22" tabindex="-1"></a><span class="fu">lines</span>(weight_seq, height_fitted, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb114-23"><a href="multiple-linear-regression.html#cb114-23" tabindex="-1"></a></span>
<span id="cb114-24"><a href="multiple-linear-regression.html#cb114-24" tabindex="-1"></a><span class="co"># Add a legend</span></span>
<span id="cb114-25"><a href="multiple-linear-regression.html#cb114-25" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Observed data&quot;</span>, <span class="st">&quot;Fitted curve&quot;</span>),</span>
<span id="cb114-26"><a href="multiple-linear-regression.html#cb114-26" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="cn">NA</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multiple-linear-regression.html#cb115-1" tabindex="-1"></a>       <span class="co"># ======== Simulate Heights from Posterior ========</span></span>
<span id="cb115-2"><a href="multiple-linear-regression.html#cb115-2" tabindex="-1"></a></span>
<span id="cb115-3"><a href="multiple-linear-regression.html#cb115-3" tabindex="-1"></a><span class="co"># Prepare new data with the same number of rows as the original data</span></span>
<span id="cb115-4"><a href="multiple-linear-regression.html#cb115-4" tabindex="-1"></a>new_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">weight_s =</span> d<span class="sc">$</span>weight_s, <span class="at">weight_s2 =</span> d<span class="sc">$</span>weight_s2)</span>
<span id="cb115-5"><a href="multiple-linear-regression.html#cb115-5" tabindex="-1"></a></span>
<span id="cb115-6"><a href="multiple-linear-regression.html#cb115-6" tabindex="-1"></a><span class="co"># Simulate height values from the posterior (same number as original data)</span></span>
<span id="cb115-7"><a href="multiple-linear-regression.html#cb115-7" tabindex="-1"></a>sim_heights <span class="ot">&lt;-</span> <span class="fu">sim</span>(m4<span class="fl">.1</span>, <span class="at">data =</span> new_data, <span class="at">n =</span> <span class="fu">nrow</span>(d))  <span class="co"># Posterior samples</span></span>
<span id="cb115-8"><a href="multiple-linear-regression.html#cb115-8" tabindex="-1"></a></span>
<span id="cb115-9"><a href="multiple-linear-regression.html#cb115-9" tabindex="-1"></a><span class="co"># Extract random samples from simulated heights</span></span>
<span id="cb115-10"><a href="multiple-linear-regression.html#cb115-10" tabindex="-1"></a>height_samples <span class="ot">&lt;-</span> <span class="fu">apply</span>(sim_heights, <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">sample</span>(x, <span class="dv">1</span>))</span>
<span id="cb115-11"><a href="multiple-linear-regression.html#cb115-11" tabindex="-1"></a></span>
<span id="cb115-12"><a href="multiple-linear-regression.html#cb115-12" tabindex="-1"></a><span class="co"># ======== Plot Observed vs. Simulated Heights ========</span></span>
<span id="cb115-13"><a href="multiple-linear-regression.html#cb115-13" tabindex="-1"></a></span>
<span id="cb115-14"><a href="multiple-linear-regression.html#cb115-14" tabindex="-1"></a><span class="co"># Plot observed data</span></span>
<span id="cb115-15"><a href="multiple-linear-regression.html#cb115-15" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>weight_s, d<span class="sc">$</span>height, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb115-16"><a href="multiple-linear-regression.html#cb115-16" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Standardized Weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Height (cm)&quot;</span>,</span>
<span id="cb115-17"><a href="multiple-linear-regression.html#cb115-17" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Observed vs. Simulated Heights&quot;</span>)</span>
<span id="cb115-18"><a href="multiple-linear-regression.html#cb115-18" tabindex="-1"></a></span>
<span id="cb115-19"><a href="multiple-linear-regression.html#cb115-19" tabindex="-1"></a><span class="co"># Add simulated height points</span></span>
<span id="cb115-20"><a href="multiple-linear-regression.html#cb115-20" tabindex="-1"></a><span class="fu">points</span>(d<span class="sc">$</span>weight_s, height_samples, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb115-21"><a href="multiple-linear-regression.html#cb115-21" tabindex="-1"></a></span>
<span id="cb115-22"><a href="multiple-linear-regression.html#cb115-22" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb115-23"><a href="multiple-linear-regression.html#cb115-23" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Observed Data&quot;</span>, <span class="st">&quot;Simulated Data&quot;</span>),</span>
<span id="cb115-24"><a href="multiple-linear-regression.html#cb115-24" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">16</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-60-2.png" width="672" /></p>
<p>This fits much better than the linear model without the quadratic term. In the book,
there is also a polynomial regression with a cubic term for weight.
The second plot shows the original heights and simulated heights from the
posterior distribution in one plot. This fits quite well.
Maybe this fits even better (see <a href="multiple-linear-regression.html#exercise1_multiple_regression">exercise 1</a>).</p>
</div>
<div id="adding_predictor_bayes" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Adding another predictor to the model<a href="multiple-linear-regression.html#adding_predictor_bayes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the !Kung San data set has already such a high <span class="math inline">\(R^2\)</span> with the quadratic term
(and possibly higher with the cubic term), we will use the created data set from
<a href="multiple-linear-regression.html#adding_predictor_freq">below</a> in the frequentist setting
to estimate the coefficients of the model with two predictors here as well.
We <em>have</em> the true but (usually) unknown data generating
mechnisms (for didactic reasons).</p>
<p>We use rather uniformative priors and fit the model using <code>quap</code>:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="multiple-linear-regression.html#cb116-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb116-2"><a href="multiple-linear-regression.html#cb116-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb116-3"><a href="multiple-linear-regression.html#cb116-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb116-4"><a href="multiple-linear-regression.html#cb116-4" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb116-5"><a href="multiple-linear-regression.html#cb116-5" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb116-6"><a href="multiple-linear-regression.html#cb116-6" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">2</span>) <span class="co"># true model</span></span>
<span id="cb116-7"><a href="multiple-linear-regression.html#cb116-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb116-8"><a href="multiple-linear-regression.html#cb116-8" tabindex="-1"></a></span>
<span id="cb116-9"><a href="multiple-linear-regression.html#cb116-9" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb116-10"><a href="multiple-linear-regression.html#cb116-10" tabindex="-1"></a>m4<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb116-11"><a href="multiple-linear-regression.html#cb116-11" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb116-12"><a href="multiple-linear-regression.html#cb116-12" tabindex="-1"></a>    Y <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb116-13"><a href="multiple-linear-regression.html#cb116-13" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> b1<span class="sc">*</span>X1 <span class="sc">+</span> b2<span class="sc">*</span>X2,</span>
<span id="cb116-14"><a href="multiple-linear-regression.html#cb116-14" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb116-15"><a href="multiple-linear-regression.html#cb116-15" tabindex="-1"></a>    b1 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb116-16"><a href="multiple-linear-regression.html#cb116-16" tabindex="-1"></a>    b2 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb116-17"><a href="multiple-linear-regression.html#cb116-17" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb116-18"><a href="multiple-linear-regression.html#cb116-18" tabindex="-1"></a>  ), <span class="at">data =</span> df)</span>
<span id="cb116-19"><a href="multiple-linear-regression.html#cb116-19" tabindex="-1"></a><span class="fu">precis</span>(m4<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>##             mean         sd      5.5%      94.5%
## a     10.2700227 0.18934442 9.9674137 10.5726316
## b1     0.4467278 0.04131434 0.3806995  0.5127561
## b2     1.0095082 0.03899992 0.9471788  1.0718377
## sigma  1.8738833 0.13250803 1.6621099  2.0856567</code></pre>
<div id="check_model_bayes" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Checking model assumptions<a href="multiple-linear-regression.html#check_model_bayes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Andrew Gelman mentions in some of his talks (see <a href="https://sites.stat.columbia.edu/gelman/research/published/philosophy_chapter.pdf">here</a> for more details)
that many Bayesians he met do not check their models, since they reflect subjective probability.
As I said in the introduction, one should not be afraid to check model predictions against the observed
and probably new data. If a model for predicting BMI performs much worse on a new data set,
we should adapt. We <strong>do not ask</strong> the question if a model is true or false, but if it is useful or
how badly the model assumptions are violated.</p>
<p>For further, more detailed information on model checking, refer to chapter 6 of
<a href="https://sites.stat.columbia.edu/gelman/book/BDA3.pdf">Gelman’s book</a>.</p>
<p>Anyhow, we plot two posterior predictive checks here.
We test the model within the same data set.
In order to do this, we create new observations
by drawing from the posterior distribution and compare these with the acutally
observed values. This is called <strong>posterior predictive checks</strong>.</p>
<p><strong>First, we plot the observed <span class="math inline">\(Y\)</span> values against the predicted <span class="math inline">\(Y\)</span> values (<span class="math inline">\(=\hat{Y}\)</span>)</strong>
from the model (as in Statistical rethinking, Chapter 5).
Although these practically never lie on the line <span class="math inline">\(y=x\)</span>, they should be sufficiently
close to it. We could also compare these two plots with the mean-model (see <a href="multiple-linear-regression.html#exercise6_multiple_regression">exercise 6</a>).</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="multiple-linear-regression.html#cb118-1" tabindex="-1"></a><span class="co"># 1) Posterior predictive checks Y vs Y_hat</span></span>
<span id="cb118-2"><a href="multiple-linear-regression.html#cb118-2" tabindex="-1"></a><span class="co"># see Statstical Rethinking p 138.</span></span>
<span id="cb118-3"><a href="multiple-linear-regression.html#cb118-3" tabindex="-1"></a><span class="co"># call link without specifying new data</span></span>
<span id="cb118-4"><a href="multiple-linear-regression.html#cb118-4" tabindex="-1"></a><span class="co"># so it uses the original data</span></span>
<span id="cb118-5"><a href="multiple-linear-regression.html#cb118-5" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">link</span>(m4<span class="fl">.2</span>)</span>
<span id="cb118-6"><a href="multiple-linear-regression.html#cb118-6" tabindex="-1"></a></span>
<span id="cb118-7"><a href="multiple-linear-regression.html#cb118-7" tabindex="-1"></a><span class="co"># summarize samples accross cases</span></span>
<span id="cb118-8"><a href="multiple-linear-regression.html#cb118-8" tabindex="-1"></a>mu_mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, mean)</span>
<span id="cb118-9"><a href="multiple-linear-regression.html#cb118-9" tabindex="-1"></a>mu_PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb118-10"><a href="multiple-linear-regression.html#cb118-10" tabindex="-1"></a></span>
<span id="cb118-11"><a href="multiple-linear-regression.html#cb118-11" tabindex="-1"></a><span class="co"># simulate observations</span></span>
<span id="cb118-12"><a href="multiple-linear-regression.html#cb118-12" tabindex="-1"></a><span class="co"># again, no new data, so uses original data</span></span>
<span id="cb118-13"><a href="multiple-linear-regression.html#cb118-13" tabindex="-1"></a>D_sim <span class="ot">&lt;-</span> <span class="fu">sim</span>(m4<span class="fl">.2</span>, <span class="at">n =</span> <span class="fl">1e4</span>)</span>
<span id="cb118-14"><a href="multiple-linear-regression.html#cb118-14" tabindex="-1"></a>D_PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(D_sim, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb118-15"><a href="multiple-linear-regression.html#cb118-15" tabindex="-1"></a></span>
<span id="cb118-16"><a href="multiple-linear-regression.html#cb118-16" tabindex="-1"></a><span class="fu">plot</span>(mu_mean <span class="sc">~</span> df<span class="sc">$</span>Y, <span class="at">col =</span> rangi2, <span class="at">ylim =</span> <span class="fu">range</span>(mu_PI), </span>
<span id="cb118-17"><a href="multiple-linear-regression.html#cb118-17" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Observed Y&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Model-Predicted Y&quot;</span>)</span>
<span id="cb118-18"><a href="multiple-linear-regression.html#cb118-18" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb118-19"><a href="multiple-linear-regression.html#cb118-19" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)) <span class="fu">lines</span>(<span class="fu">rep</span>(df<span class="sc">$</span>Y[i], <span class="dv">2</span>), mu_PI[,i], <span class="at">col =</span> rangi2)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>As we can see, the model fits the data quite well. The points are close to the dashed line (<span class="math inline">\(y=x\)</span>).
No under- or overestimation is visible. The model seems to capture the relationship between the predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
and the dependent variable <span class="math inline">\(Y\)</span> quite well - at least in a predictive sense. If there were patches of data points above or below the dashed line,
we would probably have to reconsider the model definition and think about why these points are not captured by the model.</p>
<p><strong>Next, we plot the posterior predictive plots</strong> analog to the upper left in the <code>check_model</code> output.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="multiple-linear-regression.html#cb119-1" tabindex="-1"></a><span class="fu">library</span>(scales)  <span class="co"># For the alpha function to adjust transparency</span></span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;scales&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard</code></pre>
<pre><code>## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="multiple-linear-regression.html#cb123-1" tabindex="-1"></a><span class="co"># 2) Posterior predictive densities</span></span>
<span id="cb123-2"><a href="multiple-linear-regression.html#cb123-2" tabindex="-1"></a><span class="co"># Simulate observations using the posterior predictive distribution</span></span>
<span id="cb123-3"><a href="multiple-linear-regression.html#cb123-3" tabindex="-1"></a>D_sim <span class="ot">&lt;-</span> <span class="fu">sim</span>(m4<span class="fl">.2</span>, <span class="at">n =</span> <span class="fl">1e4</span>)  <span class="co"># Generate 10,000 simulated datasets</span></span>
<span id="cb123-4"><a href="multiple-linear-regression.html#cb123-4" tabindex="-1"></a></span>
<span id="cb123-5"><a href="multiple-linear-regression.html#cb123-5" tabindex="-1"></a><span class="co"># Calculate densities for all samples</span></span>
<span id="cb123-6"><a href="multiple-linear-regression.html#cb123-6" tabindex="-1"></a>densities <span class="ot">&lt;-</span> <span class="fu">apply</span>(D_sim, <span class="dv">1</span>, density)</span>
<span id="cb123-7"><a href="multiple-linear-regression.html#cb123-7" tabindex="-1"></a></span>
<span id="cb123-8"><a href="multiple-linear-regression.html#cb123-8" tabindex="-1"></a><span class="co"># Find the maximum density value for setting the y-axis limits</span></span>
<span id="cb123-9"><a href="multiple-linear-regression.html#cb123-9" tabindex="-1"></a>max_density <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">sapply</span>(densities, <span class="cf">function</span>(d) <span class="fu">max</span>(d<span class="sc">$</span>y)))</span>
<span id="cb123-10"><a href="multiple-linear-regression.html#cb123-10" tabindex="-1"></a></span>
<span id="cb123-11"><a href="multiple-linear-regression.html#cb123-11" tabindex="-1"></a><span class="co"># Create the density plot with predefined ylim</span></span>
<span id="cb123-12"><a href="multiple-linear-regression.html#cb123-12" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlim =</span> <span class="fu">range</span>(df<span class="sc">$</span>Y), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, max_density),</span>
<span id="cb123-13"><a href="multiple-linear-regression.html#cb123-13" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Y&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Density&quot;</span>,</span>
<span id="cb123-14"><a href="multiple-linear-regression.html#cb123-14" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Comparison of Observed and Predicted Densities&quot;</span>)</span>
<span id="cb123-15"><a href="multiple-linear-regression.html#cb123-15" tabindex="-1"></a></span>
<span id="cb123-16"><a href="multiple-linear-regression.html#cb123-16" tabindex="-1"></a><span class="co"># Add 100 posterior predictive density lines</span></span>
<span id="cb123-17"><a href="multiple-linear-regression.html#cb123-17" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb123-18"><a href="multiple-linear-regression.html#cb123-18" tabindex="-1"></a>n_lines <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb123-19"><a href="multiple-linear-regression.html#cb123-19" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fl">1e4</span>, n_lines)  <span class="co"># Randomly sample 100 posterior predictive datasets</span></span>
<span id="cb123-20"><a href="multiple-linear-regression.html#cb123-20" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> samples) {</span>
<span id="cb123-21"><a href="multiple-linear-regression.html#cb123-21" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">density</span>(D_sim[s, ]), <span class="at">col =</span> <span class="fu">alpha</span>(<span class="st">&quot;lightblue&quot;</span>, <span class="fl">0.3</span>), <span class="at">lwd =</span> <span class="dv">1</span>)</span>
<span id="cb123-22"><a href="multiple-linear-regression.html#cb123-22" tabindex="-1"></a>}</span>
<span id="cb123-23"><a href="multiple-linear-regression.html#cb123-23" tabindex="-1"></a></span>
<span id="cb123-24"><a href="multiple-linear-regression.html#cb123-24" tabindex="-1"></a><span class="co"># Add the density line for the observed Y values</span></span>
<span id="cb123-25"><a href="multiple-linear-regression.html#cb123-25" tabindex="-1"></a>obs_density <span class="ot">&lt;-</span> <span class="fu">density</span>(df<span class="sc">$</span>Y)</span>
<span id="cb123-26"><a href="multiple-linear-regression.html#cb123-26" tabindex="-1"></a><span class="fu">lines</span>(obs_density<span class="sc">$</span>x, obs_density<span class="sc">$</span>y, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb123-27"><a href="multiple-linear-regression.html#cb123-27" tabindex="-1"></a></span>
<span id="cb123-28"><a href="multiple-linear-regression.html#cb123-28" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb123-29"><a href="multiple-linear-regression.html#cb123-29" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Posterior Predictive Densities&quot;</span>, <span class="st">&quot;Observed Density&quot;</span>),</span>
<span id="cb123-30"><a href="multiple-linear-regression.html#cb123-30" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;lightblue&quot;</span>, <span class="st">&quot;green&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>The light blue lines show distributions of model predicted <span class="math inline">\(Y\)</span> values.
The green line shows the distribution of the observed <span class="math inline">\(Y\)</span> values.
As we can see, there seem to be no systematic differences between the observed and predicted values.
The model seems to capture the relationship well.
If we see systematic deviations here, we need to reconsider the model definition.</p>
<p><strong>Example</strong>: If you want to predict pain (<span class="math inline">\(Y\)</span> variable) and you have a lot of zeros (pain-free participants)
you will probably see a discrepancy between the observed and predicted values in this plot.
What could you do? You could use a two step process (model the probability that a person is pain-free
and then model the pain intensity for the people who have pain) or use a different model (like a zero-inflated model).</p>
<p>Note that we did not explicitely assume normally distributed errors in the model definition above,
so we won’t check this here but in the Frequentist framework below.</p>
</div>
</div>
</div>
<div id="linear-regression-with-2-predictors-in-the-frequentist-framework" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Linear regression with 2 predictors in the Frequentist Framework<a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-frequentist-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To reiterate from the last chapter:
In full, the <strong>classical linear regression model</strong> can be written as
(see p. 21-22 in Westfall):</p>
<p><span class="math display">\[ Y_i|X_i = x_i \sim_{independent} N(\beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik},\sigma^2)\]</span>
for <span class="math inline">\(i = 1, \dots, n\)</span>.</p>
<p>The <span class="math inline">\(Y_i\)</span> are independently normally distributed <em>conditioned</em> on the predictors
having the values <span class="math inline">\(X_i = x_i\)</span>. Each conditional distribution has an expected value (<span class="math inline">\(\mu\)</span>)
that is a linear function of the predictors and a constant variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If the assumptions of the classical linear regression model are met, the least squares
estimators (OLS) are the best (smallest variance) linear unbiased (on average correct) estimators
- so-called: <strong>BLUE</strong> - of the parameters.</p>
<div id="adding_transformed_predictor_freq" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Adding a transformed predictor to the model<a href="multiple-linear-regression.html#adding_transformed_predictor_freq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>No, let’s fit the same model as <a href="multiple-linear-regression.html#adding_transformed_predictor_bayes">above</a> in the Bayesian framework.</p>
<p>The model is:</p>
<p><span class="math display">\[height_i = \alpha + \beta_1 weight_i + \beta_2 weight_i^2 + \varepsilon_i\]</span>
whereas
<span class="math display">\[\varepsilon_i \sim N(0, \sigma)\]</span></p>
<p>And if you build the expectation on both sides for fixed <span class="math inline">\(weight_i\)</span>, you get:</p>
<p><span class="math display">\[\mathbb{E}(height_i|weight_i) = \alpha + \beta_1 weight_i + \beta_2 weight_i^2\]</span></p>
<p>The last line means, the expected height of a person given a certain weight depends
quadratically on the weight. The error term <span class="math inline">\(\varepsilon_i\)</span> is on average zero, hence it goes away here.
Remember the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law">law of large numbers</a>:
The sample mean <span class="math inline">\(\bar{\varepsilon_i}\)</span> approaches the expected value <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0\)</span>
as the sample size increases. If you drew many samples (from the true model)
and average over the error terms, the average will approach zero.
Think of <a href="https://github.com/jdegenfellner/ZHAW_Teaching/blob/main/Wiggle_in_simple_linear_regression.R">this</a> animated
graph if you need a dynamic image of the regression model.
The weights are considered fixed and therefore do not change when building the expectation.</p>
<p>We are looking for fixed, but unknown, parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\sigma\)</span>.
The fixed <span class="math inline">\(\sigma\)</span> indicates that the observations <em>wiggle</em> around the expected value
equally strong not matter which weight we have. This is called <strong>homoscedasticity</strong>.</p>
<p>Let’s fit the model using the <code>lm</code> function in R which uses
<a href="https://en.wikipedia.org/wiki/Least_squares">least squares</a> to estimate the parameters.
At this point I could torture you with <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix algebra</a>
and show you the <a href="https://en.wikipedia.org/wiki/Linear_least_squares">normal equations</a> for linear regression,
but I will spare you for now.
Note that the least squares algorithm for fitting the curve works for all
kinds of functional forms. For example, we could also fit an exponential curve using the same
technique (see <a href="multiple-linear-regression.html#exercise9_multiple_regression">exercise 9</a>).</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="multiple-linear-regression.html#cb124-1" tabindex="-1"></a><span class="co"># scale weight</span></span>
<span id="cb124-2"><a href="multiple-linear-regression.html#cb124-2" tabindex="-1"></a>d<span class="sc">$</span>weight_s <span class="ot">&lt;-</span> <span class="fu">scale</span>(d<span class="sc">$</span>weight)</span>
<span id="cb124-3"><a href="multiple-linear-regression.html#cb124-3" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb124-4"><a href="multiple-linear-regression.html#cb124-4" tabindex="-1"></a>m4<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight_s <span class="sc">+</span> <span class="fu">I</span>(weight_s<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> d)</span>
<span id="cb124-5"><a href="multiple-linear-regression.html#cb124-5" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = height ~ weight_s + I(weight_s^2), data = d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.9689  -3.9794   0.2364   3.9262  19.5182 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   146.6604     0.3748  391.30   &lt;2e-16 ***
## weight_s       21.4149     0.2908   73.64   &lt;2e-16 ***
## I(weight_s^2)  -8.4123     0.2822  -29.80   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.766 on 541 degrees of freedom
## Multiple R-squared:  0.9565, Adjusted R-squared:  0.9564 
## F-statistic:  5952 on 2 and 541 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="multiple-linear-regression.html#cb126-1" tabindex="-1"></a><span class="fu">mean</span>(d<span class="sc">$</span>height)</span></code></pre></div>
<pre><code>## [1] 138.2636</code></pre>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="multiple-linear-regression.html#cb128-1" tabindex="-1"></a><span class="fu">confint</span>(m4<span class="fl">.2</span>, <span class="at">level =</span> <span class="fl">0.94</span>)</span></code></pre></div>
<pre><code>##                      3 %       97 %
## (Intercept)   145.954018 147.366836
## weight_s       20.866788  21.962979
## I(weight_s^2)  -8.944251  -7.880337</code></pre>
<p>See <code>?I</code> in R. This command is used so that R knows that it should
treat the “^2” as “square” and not as formula syntax.
We could also create a new variable as before. Whatever you prefer.</p>
<div id="interpretation_output_freq_quadratic" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Interpretation of output and coefficients<a href="multiple-linear-regression.html#interpretation_output_freq_quadratic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The intercept <span class="math inline">\(\alpha\)</span> is the <strong>model-predicted height</strong> of a person of <strong>average weight</strong>
(<span class="math inline">\(weight_s=0\)</span> for a person of average weight).
Note that this is not equal to the average height (<span class="math inline">\(138.2636~cm\)</span>) of the people in the data set
(see <a href="multiple-linear-regression.html#exercise12_multiple_regression">exercise 12</a>).</li>
<li>The residuals have range from <span class="math inline">\(-19.97\)</span> to <span class="math inline">\(19.51\)</span>. So, the model maximally
overestimates the heights by <span class="math inline">\(19.97\)</span> cm and underestimates by <span class="math inline">\(19.51\)</span> cm.
These numbers are plausible when you look at the scatterplot with the fitted
curve.</li>
<li>The coefficients <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> agree with the Bayes estimates.
Specifically, <span class="math inline">\(\beta_2\)</span> is non-zero indicating curvature. You cannot directly interpret the coefficients
as in the non-quadratic case since, for instance, you cannot change <span class="math inline">\(weight^2\)</span> by one unit
and hold <span class="math inline">\(weight\)</span> constant at the same time. Refer to Peter Westfall’s book section 9.1. for all the details.</li>
<li>If you like <span class="math inline">\(p\)</span>-values: All the hypotheses that the coefficients are zero
are rejected. The <span class="math inline">\(p\)</span>-values are very small. The values of the test statistics can not be explained
by chance alone. On the other hand, for at least <span class="math inline">\(\beta_1\)</span> and
and the global test this is not a surprise when you look at the scatterplot.
After having fit many models, you would have guessed that all three parameters
are solidly non-zero. The intercept is not zero since a person of average weight probably
has non-zero height. <span class="math inline">\(\beta_1\)</span> is non-zero since you can easily imagine a linear
trend line with positive slope going through the data, and <span class="math inline">\(\beta_2\)</span> is non-zero
since there is clearly (non-trivial) curvature in the scatterplot.</li>
<li>The <span class="math inline">\(R^2\)</span> is a whopping <span class="math inline">\(0.96\)</span> which could be a sign of overfitting, but
in this case we conclude that the true relationship is caputured rather well.
<a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting</a> would occur if
our curve would rather fit the noise in the data than
the underying trend.</li>
</ul>
</div>
<div id="checking-model-assumptions" class="section level4 hasAnchor" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Checking model assumptions<a href="multiple-linear-regression.html#checking-model-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="multiple-linear-regression.html#cb130-1" tabindex="-1"></a><span class="fu">check_model</span>(m4<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## Some of the variables were in matrix-format - probably you used
##   `scale()` on your data?
##   If so, and you get an error, please try `datawizard::standardize()` to
##   standardize your data.
## Some of the variables were in matrix-format - probably you used
##   `scale()` on your data?
##   If so, and you get an error, please try `datawizard::standardize()` to
##   standardize your data.</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>If we want to be perfectionists, we could remark that (upper right plot)
in the lower fitted values the residuals are more negative,
meaning that the model overestimates the heights in this region.
In the middle region the model underestimates a bit and we can see
a positive tendency in the residuals. Apart from that,
the diagnostic plots look excellent.</p>
</div>
</div>
<div id="adding_predictor_freq" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Adding another predictor to the model<a href="multiple-linear-regression.html#adding_predictor_freq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, we add another predictor to the model. We use <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
<strong>simultaneously</strong> to predict <span class="math inline">\(Y\)</span>. We are now in the lucky situation that
we can still visualize the situation in 3D. The regression line from simple
linear regression
becomes a <a href="https://stackoverflow.com/questions/47344850/scatterplot3d-regression-plane-with-residuals">plane</a>.
The vertical distances between the data points and the plane are the residuals.
See <a href="https://rpubs.com/pjozefek/576206">here</a> or
<a href="https://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization">here</a>
at the end for examples.
Minimizing the sum of the squared errors gives again
the estimates for the coefficients.</p>
<p>For demonstration purposes, we can <strong>create data ourselves</strong> with known
coefficients. This is the same as <a href="multiple-linear-regression.html#adding_predictor_bayes">above</a>.
This is the true model, which we usually do not know:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, \sigma^2)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>for example:</p>
<p><span class="math display">\[ Y_i = 10 + 0.5 \cdot X_{1i} + 1 \cdot X_{2i} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, 5)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = 10 + 0.5 x_1 + 1 x_2\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>According to the model, the conditional expected value of <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(X_1 = x_1\)</span> and <span class="math inline">\(X_2 = x_2\)</span>
is a linear function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. Note, that small letters are realized values
of random variables. Also note, that in the expectation the error term goes away, since
<span class="math inline">\(\mathbb{E}(\varepsilon_i) = 0\)</span>.</p>
<ul>
<li>If <span class="math inline">\(X_1\)</span> increases by one unit, <span class="math inline">\(Y\)</span> increases by <span class="math inline">\(0.5\)</span> units on average (in expectation).</li>
<li>If <span class="math inline">\(X_2\)</span> increases by one unit, <span class="math inline">\(Y\)</span> increases by <span class="math inline">\(1\)</span> unit on average (in expectation).</li>
<li>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are zero, <span class="math inline">\(Y\)</span> is <span class="math inline">\(10\)</span> on average (in expectation).</li>
</ul>
<p>Why in expectation? Because there is still the error term which makes the whole thing random!
We can see that an increase in <span class="math inline">\(X_1\)</span> does not influence the relationship between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y\)</span>.
Hence, there is <strong>no interaction</strong> between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with respect to <span class="math inline">\(Y\)</span>.</p>
<p>Now lets’s draw 100 points from this model, fit the model and add the plane:</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="multiple-linear-regression.html#cb132-1" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb132-2"><a href="multiple-linear-regression.html#cb132-2" tabindex="-1"></a></span>
<span id="cb132-3"><a href="multiple-linear-regression.html#cb132-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb132-4"><a href="multiple-linear-regression.html#cb132-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb132-5"><a href="multiple-linear-regression.html#cb132-5" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb132-6"><a href="multiple-linear-regression.html#cb132-6" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb132-7"><a href="multiple-linear-regression.html#cb132-7" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb132-8"><a href="multiple-linear-regression.html#cb132-8" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb132-9"><a href="multiple-linear-regression.html#cb132-9" tabindex="-1"></a></span>
<span id="cb132-10"><a href="multiple-linear-regression.html#cb132-10" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb132-11"><a href="multiple-linear-regression.html#cb132-11" tabindex="-1"></a>m4<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2, <span class="at">data =</span> d)</span>
<span id="cb132-12"><a href="multiple-linear-regression.html#cb132-12" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7460 -1.3215 -0.2489  1.2427  4.1597 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.27013    0.19228   53.41   &lt;2e-16 ***
## X1           0.44673    0.04195   10.65   &lt;2e-16 ***
## X2           1.00952    0.03960   25.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.903 on 97 degrees of freedom
## Multiple R-squared:  0.8839, Adjusted R-squared:  0.8815 
## F-statistic: 369.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="multiple-linear-regression.html#cb134-1" tabindex="-1"></a><span class="co"># Create a grid for the plane</span></span>
<span id="cb134-2"><a href="multiple-linear-regression.html#cb134-2" tabindex="-1"></a>X1_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X1), <span class="fu">max</span>(d<span class="sc">$</span>X1), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb134-3"><a href="multiple-linear-regression.html#cb134-3" tabindex="-1"></a>X2_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X2), <span class="fu">max</span>(d<span class="sc">$</span>X2), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb134-4"><a href="multiple-linear-regression.html#cb134-4" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> X1_grid, <span class="at">X2 =</span> X2_grid)</span>
<span id="cb134-5"><a href="multiple-linear-regression.html#cb134-5" tabindex="-1"></a></span>
<span id="cb134-6"><a href="multiple-linear-regression.html#cb134-6" tabindex="-1"></a><span class="co"># Predict the values for the grid</span></span>
<span id="cb134-7"><a href="multiple-linear-regression.html#cb134-7" tabindex="-1"></a>grid<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">predict</span>(m4<span class="fl">.3</span>, <span class="at">newdata =</span> grid)</span>
<span id="cb134-8"><a href="multiple-linear-regression.html#cb134-8" tabindex="-1"></a></span>
<span id="cb134-9"><a href="multiple-linear-regression.html#cb134-9" tabindex="-1"></a><span class="co"># Convert the grid into a matrix for the plane</span></span>
<span id="cb134-10"><a href="multiple-linear-regression.html#cb134-10" tabindex="-1"></a>plane_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(grid<span class="sc">$</span>Y, <span class="at">nrow =</span> <span class="fu">length</span>(X1_grid), <span class="at">ncol =</span> <span class="fu">length</span>(X2_grid))</span>
<span id="cb134-11"><a href="multiple-linear-regression.html#cb134-11" tabindex="-1"></a></span>
<span id="cb134-12"><a href="multiple-linear-regression.html#cb134-12" tabindex="-1"></a><span class="co"># Create the interactive 3D plot</span></span>
<span id="cb134-13"><a href="multiple-linear-regression.html#cb134-13" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb134-14"><a href="multiple-linear-regression.html#cb134-14" tabindex="-1"></a>  <span class="fu">add_markers</span>(</span>
<span id="cb134-15"><a href="multiple-linear-regression.html#cb134-15" tabindex="-1"></a>    <span class="at">x =</span> d<span class="sc">$</span>X2, <span class="at">y =</span> d<span class="sc">$</span>X1, <span class="at">z =</span> d<span class="sc">$</span>Y,</span>
<span id="cb134-16"><a href="multiple-linear-regression.html#cb134-16" tabindex="-1"></a>    <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>),</span>
<span id="cb134-17"><a href="multiple-linear-regression.html#cb134-17" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Data Points&quot;</span></span>
<span id="cb134-18"><a href="multiple-linear-regression.html#cb134-18" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb134-19"><a href="multiple-linear-regression.html#cb134-19" tabindex="-1"></a>  <span class="fu">add_surface</span>(</span>
<span id="cb134-20"><a href="multiple-linear-regression.html#cb134-20" tabindex="-1"></a>    <span class="at">x =</span> X1_grid, <span class="at">y =</span> X2_grid, <span class="at">z =</span> plane_matrix,</span>
<span id="cb134-21"><a href="multiple-linear-regression.html#cb134-21" tabindex="-1"></a>    <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;pink&quot;</span>)),</span>
<span id="cb134-22"><a href="multiple-linear-regression.html#cb134-22" tabindex="-1"></a>    <span class="at">showscale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb134-23"><a href="multiple-linear-regression.html#cb134-23" tabindex="-1"></a>    <span class="at">opacity =</span> <span class="fl">0.7</span>,</span>
<span id="cb134-24"><a href="multiple-linear-regression.html#cb134-24" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Fitted Plane&quot;</span></span>
<span id="cb134-25"><a href="multiple-linear-regression.html#cb134-25" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb134-26"><a href="multiple-linear-regression.html#cb134-26" tabindex="-1"></a>  plotly<span class="sc">::</span><span class="fu">layout</span>(</span>
<span id="cb134-27"><a href="multiple-linear-regression.html#cb134-27" tabindex="-1"></a>    <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb134-28"><a href="multiple-linear-regression.html#cb134-28" tabindex="-1"></a>      <span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X1&quot;</span>),</span>
<span id="cb134-29"><a href="multiple-linear-regression.html#cb134-29" tabindex="-1"></a>      <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb134-30"><a href="multiple-linear-regression.html#cb134-30" tabindex="-1"></a>      <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb134-31"><a href="multiple-linear-regression.html#cb134-31" tabindex="-1"></a>    ),</span>
<span id="cb134-32"><a href="multiple-linear-regression.html#cb134-32" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Interactive 3D Scatterplot with Fitted Plane&quot;</span></span>
<span id="cb134-33"><a href="multiple-linear-regression.html#cb134-33" tabindex="-1"></a>  )</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-6b661d7482f01c7590c7" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-6b661d7482f01c7590c7">{"x":{"visdat":{"15b72f1ce24c":["function () ","plotlyVisDat"]},"cur_data":"15b72f1ce24c","attrs":{"15b72f1ce24c":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[9.4443987628904011,13.333800774941469,12.133021279668236,9.5249461000369529,4.7364466057395678,13.110030053932444,5.6505624919871416,-2.6915973043760397,9.6835502035007526,13.372771869883055,10.421960153411863,14.426731037966759,5.3774668420099854,8.966769300350542,9.2224188820480499,19.324443517647648,10.890680732507244,0.43379512701927547,5.0323217859747178,1.1279460830200065,6.7707267619375422,5.9536612767633095,7.2018979385879955,8.3124876734520878,16.92939726234739,2.6430170894358751,11.862207545300455,9.3388007192618208,4.1146754847733282,10.746811714849734,22.199502776793707,11.339202369983692,12.873056415835688,8.6057941476553346,0.63893921686459931,14.744254437710621,3.7157430024744071,14.382923087793795,19.429219875060383,0.26628371850054355,10.194910287864269,7.1648219212529405,1.9679096650446017,5.5744444031060336,4.6541209391291138,8.342419574497967,1.482060216115193,9.5532640731783047,21.120918627921032,4.327344911454289,13.820784779023231,12.650091589533226,10.866002283265379,8.560615962929548,12.03532704499019,12.212069610944848,11.10466464782988,10.860848824872161,14.967222752027357,5.6011456272436657,15.170421399691325,2.5185654276194671,2.9605141825589425,26.259158572277943,9.8223919406710856,15.34512167918176,14.037070888084919,4.2005526471672674,14.112419163123903,17.148448797139881,9.3855455547850664,6.4785989154930874,13.712628901296251,16.078708889025989,6.2725840687341741,6.1908326560139839,9.8266148785503837,8.6497118209268162,13.492209623864802,7.4102948959991402,1.3628295997224362,18.75211881281016,8.0971511174505544,6.7520738074737681,8.503674772841002,10.111652728783406,18.733737918563573,14.79333252002546,12.517339204227589,10.711689728035747,15.892753933951489,11.85592488806984,13.360273199407175,2.79848201789475,10.852588915270818,18.618819697732381,22.205580290317254,4.8733643852768296,6.3968866915551779,4.0063764681849268],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5},"name":"Data Points","inherit":true},"15b72f1ce24c.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[-5.2517772599504369,-3.845273924496897,-2.4387705890433566,-1.0322672535898163,0.37423608186372337,1.7807394173172628,3.1872427527708043,4.5937460882243437,6.0002494236778832,7.4067527591314226,8.813256094584963,10.219759430038504,11.626262765492045,13.032766100945583,14.439269436399124,15.845772771852662,17.252276107306205,18.658779442759744,20.065282778213284,21.471786113666823],[-4.7231644904891299,-3.31666115503559,-1.9101578195820497,-0.50365448412850922,0.90284885132503034,2.30935218677857,3.7158555222321112,5.1223588576856507,6.5288621931391901,7.9353655285927296,9.341868864046269,10.74837219949981,12.154875534953351,13.561378870406889,14.96788220586043,16.37438554131397,17.780888876767509,19.187392212221052,20.593895547674588,22.000398883128131],[-4.194551721027822,-2.7880483855742821,-1.3815450501207418,0.024958285332798636,1.4314616207863382,2.8379649562398779,4.2444682916934191,5.6509716271469586,7.057474962600498,8.4639782980540375,9.8704816335075769,11.276984968961118,12.683488304414659,14.089991639868197,15.496494975321738,16.902998310775278,18.309501646228817,19.71600498168236,21.122508317135896,22.529011652589439],[-3.6659389515665137,-2.2594356161129743,-0.85293228065943394,0.55357105479410651,1.9600743902476461,3.3665777257011857,4.773081061154727,6.1795843966082664,7.5860877320618059,8.9925910675153453,10.399094402968885,11.805597738422426,13.212101073875967,14.618604409329505,16.025107744783046,17.431611080236586,18.838114415690125,20.244617751143668,21.651121086597204,23.057624422050747],[-3.1373261821052059,-1.7308228466516666,-0.32431951119812608,1.0821838242554143,2.4886871597089537,3.8951904951624936,5.3016938306160348,6.7081971660695743,8.1147005015231137,9.5212038369766532,10.927707172430193,12.334210507883734,13.740713843337275,15.147217178790813,16.553720514244354,17.960223849697893,19.366727185151433,20.773230520604976,22.179733856058512,23.586237191512055],[-2.6087134126438989,-1.2022100771903597,0.2042932582631809,1.6107965937167212,3.0172999291702607,4.4238032646238006,5.8303066000773418,7.2368099355308813,8.6433132709844216,10.049816606437961,11.4563199418915,12.862823277345042,14.269326612798583,15.675829948252121,17.082333283705662,18.488836619159201,19.895339954612741,21.301843290066284,22.70834662551982,24.114849960973363],[-2.080100643182591,-0.67359730772905169,0.73290602772448876,2.1394093631780291,3.5459126986315685,4.9524160340851084,6.3589193695386497,7.7654227049921891,9.1719260404457295,10.578429375899269,11.984932711352808,13.39143604680635,14.797939382259891,16.204442717713427,17.61094605316697,19.017449388620509,20.423952724074049,21.830456059527592,23.236959394981128,24.643462730434671],[-1.5514878737212834,-0.14498453826774382,1.2615187971857966,2.668022132639337,4.0745254680928769,5.4810288035464163,6.8875321389999575,8.2940354744534979,9.7005388099070373,11.107042145360577,12.513545480814116,13.920048816267657,15.326552151721199,16.733055487174735,18.139558822628278,19.546062158081817,20.952565493535356,22.359068828988899,23.765572164442435,25.172075499895978],[-1.0228751042599755,0.38362823119356404,1.7901315666471045,3.1966349021006448,4.6031382375541847,6.0096415730077242,7.4161449084612654,8.8226482439148057,10.229151579368345,11.635654914821885,13.042158250275424,14.448661585728965,15.855164921182507,17.261668256636042,18.668171592089585,20.074674927543125,21.481178262996664,22.887681598450207,24.294184933903743,25.700688269357286],[-0.49426233479866943,0.91224100065487013,2.3187443361084106,3.7252476715619509,5.1317510070154908,6.5382543424690303,7.9447576779225715,9.3512610133761118,10.757764348829651,12.164267684283191,13.57077101973673,14.977274355190271,16.383777690643811,17.79028102609735,19.19678436155089,20.603287697004429,22.009791032457972,23.416294367911512,24.822797703365051,26.229301038818591],[0.034350434662638436,1.4408537701161779,2.8473571055697184,4.2538604410232592,5.6603637764767987,7.0668671119303381,8.4733704473838785,9.8798737828374197,11.286377118290959,12.692880453744499,14.099383789198038,15.505887124651579,16.912390460105119,18.318893795558658,19.725397131012198,21.131900466465737,22.53840380191928,23.94490713737282,25.351410472826359,26.757913808279898],[0.56296320412394629,1.9694665395774857,3.3759698750310263,4.7824732104845671,6.1889765459381065,7.595479881391646,9.0019832168451863,10.408486552298728,11.814989887752267,13.221493223205806,14.627996558659346,16.034499894112887,17.441003229566427,18.847506565019966,20.254009900473505,21.660513235927045,23.067016571380588,24.473519906834127,25.880023242287667,27.286526577741206],[1.0915759735852542,2.4980793090387938,3.9045826444923342,5.3110859799458749,6.7175893153994144,8.124092650852953,9.5305959863064942,10.937099321760035,12.343602657213575,13.750105992667114,15.156609328120654,16.563112663574195,17.969615999027734,19.376119334481274,20.782622669934813,22.189126005388353,23.595629340841896,25.002132676295435,26.408636011748975,27.815139347202514],[1.620188743046562,3.0266920785001017,4.4331954139536416,5.8396987494071828,7.2462020848607223,8.6527054203142608,10.059208755767802,11.465712091221343,12.872215426674883,14.278718762128422,15.685222097581962,17.091725433035503,18.498228768489042,19.904732103942582,21.311235439396121,22.717738774849661,24.124242110303204,25.530745445756743,26.937248781210283,28.343752116663822],[2.1488015125078701,3.5553048479614096,4.9618081834149494,6.3683115188684907,7.7748148543220301,9.1813181897755687,10.58782152522911,11.994324860682651,13.400828196136191,14.80733153158973,16.213834867043268,17.620338202496811,19.02684153795035,20.43334487340389,21.839848208857429,23.246351544310969,24.652854879764512,26.059358215218051,27.46586155067159,28.87236488612513],[2.6774142819691762,4.0839176174227152,5.4904209528762555,6.8969242883297968,8.3034276237833353,9.7099309592368748,11.116434294690416,12.522937630143957,13.929440965597497,15.335944301051036,16.742447636504576,18.148950971958115,19.555454307411658,20.961957642865194,22.368460978318737,23.774964313772276,25.181467649225816,26.587970984679359,27.994474320132895,29.400977655586438],[3.2060270514304841,4.6125303868840231,6.0190337223375634,7.4255370577911046,8.8320403932446432,10.238543728698183,11.645047064151724,13.051550399605265,14.458053735058805,15.864557070512344,17.271060405965883,18.677563741419423,20.084067076872966,21.490570412326502,22.897073747780045,24.303577083233584,25.710080418687124,27.116583754140667,28.523087089594203,29.929590425047746],[3.7346398208917919,5.1411431563453309,6.5476464917988713,7.9541498272524125,9.360653162705951,10.76715649815949,12.173659833613032,13.580163169066573,14.986666504520112,16.393169839973652,17.799673175427191,19.206176510880731,20.612679846334274,22.01918318178781,23.425686517241353,24.832189852694892,26.238693188148432,27.645196523601975,29.05169985905551,30.458203194509053],[4.2632525903530993,5.6697559258066388,7.0762592612601791,8.4827625967137195,9.8892659321672589,11.295769267620798,12.70227260307434,14.108775938527881,15.51527927398142,16.92178260943496,18.328285944888499,19.734789280342039,21.141292615795582,22.547795951249118,23.954299286702661,25.3608026221562,26.767305957609739,28.173809293063282,29.580312628516818,30.986815963970361],[4.7918653598144072,6.1983686952679466,7.604872030721487,9.0113753661750273,10.417878701628567,11.824382037082106,13.230885372535647,14.637388707989189,16.043892043442728,17.450395378896268,18.856898714349807,20.263402049803346,21.669905385256889,23.076408720710425,24.482912056163968,25.889415391617508,27.295918727071047,28.70242206252459,30.108925397978126,31.515428733431669]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"colorscale":[[0,1],["red","pink"]],"showscale":false,"opacity":0.69999999999999996,"name":"Fitted Plane","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Y"}},"title":"Interactive 3D Scatterplot with Fitted Plane","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[9.4443987628904011,13.333800774941469,12.133021279668236,9.5249461000369529,4.7364466057395678,13.110030053932444,5.6505624919871416,-2.6915973043760397,9.6835502035007526,13.372771869883055,10.421960153411863,14.426731037966759,5.3774668420099854,8.966769300350542,9.2224188820480499,19.324443517647648,10.890680732507244,0.43379512701927547,5.0323217859747178,1.1279460830200065,6.7707267619375422,5.9536612767633095,7.2018979385879955,8.3124876734520878,16.92939726234739,2.6430170894358751,11.862207545300455,9.3388007192618208,4.1146754847733282,10.746811714849734,22.199502776793707,11.339202369983692,12.873056415835688,8.6057941476553346,0.63893921686459931,14.744254437710621,3.7157430024744071,14.382923087793795,19.429219875060383,0.26628371850054355,10.194910287864269,7.1648219212529405,1.9679096650446017,5.5744444031060336,4.6541209391291138,8.342419574497967,1.482060216115193,9.5532640731783047,21.120918627921032,4.327344911454289,13.820784779023231,12.650091589533226,10.866002283265379,8.560615962929548,12.03532704499019,12.212069610944848,11.10466464782988,10.860848824872161,14.967222752027357,5.6011456272436657,15.170421399691325,2.5185654276194671,2.9605141825589425,26.259158572277943,9.8223919406710856,15.34512167918176,14.037070888084919,4.2005526471672674,14.112419163123903,17.148448797139881,9.3855455547850664,6.4785989154930874,13.712628901296251,16.078708889025989,6.2725840687341741,6.1908326560139839,9.8266148785503837,8.6497118209268162,13.492209623864802,7.4102948959991402,1.3628295997224362,18.75211881281016,8.0971511174505544,6.7520738074737681,8.503674772841002,10.111652728783406,18.733737918563573,14.79333252002546,12.517339204227589,10.711689728035747,15.892753933951489,11.85592488806984,13.360273199407175,2.79848201789475,10.852588915270818,18.618819697732381,22.205580290317254,4.8733643852768296,6.3968866915551779,4.0063764681849268],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Data Points","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"red"],[1,"pink"]],"showscale":false,"z":[[-5.2517772599504369,-3.845273924496897,-2.4387705890433566,-1.0322672535898163,0.37423608186372337,1.7807394173172628,3.1872427527708043,4.5937460882243437,6.0002494236778832,7.4067527591314226,8.813256094584963,10.219759430038504,11.626262765492045,13.032766100945583,14.439269436399124,15.845772771852662,17.252276107306205,18.658779442759744,20.065282778213284,21.471786113666823],[-4.7231644904891299,-3.31666115503559,-1.9101578195820497,-0.50365448412850922,0.90284885132503034,2.30935218677857,3.7158555222321112,5.1223588576856507,6.5288621931391901,7.9353655285927296,9.341868864046269,10.74837219949981,12.154875534953351,13.561378870406889,14.96788220586043,16.37438554131397,17.780888876767509,19.187392212221052,20.593895547674588,22.000398883128131],[-4.194551721027822,-2.7880483855742821,-1.3815450501207418,0.024958285332798636,1.4314616207863382,2.8379649562398779,4.2444682916934191,5.6509716271469586,7.057474962600498,8.4639782980540375,9.8704816335075769,11.276984968961118,12.683488304414659,14.089991639868197,15.496494975321738,16.902998310775278,18.309501646228817,19.71600498168236,21.122508317135896,22.529011652589439],[-3.6659389515665137,-2.2594356161129743,-0.85293228065943394,0.55357105479410651,1.9600743902476461,3.3665777257011857,4.773081061154727,6.1795843966082664,7.5860877320618059,8.9925910675153453,10.399094402968885,11.805597738422426,13.212101073875967,14.618604409329505,16.025107744783046,17.431611080236586,18.838114415690125,20.244617751143668,21.651121086597204,23.057624422050747],[-3.1373261821052059,-1.7308228466516666,-0.32431951119812608,1.0821838242554143,2.4886871597089537,3.8951904951624936,5.3016938306160348,6.7081971660695743,8.1147005015231137,9.5212038369766532,10.927707172430193,12.334210507883734,13.740713843337275,15.147217178790813,16.553720514244354,17.960223849697893,19.366727185151433,20.773230520604976,22.179733856058512,23.586237191512055],[-2.6087134126438989,-1.2022100771903597,0.2042932582631809,1.6107965937167212,3.0172999291702607,4.4238032646238006,5.8303066000773418,7.2368099355308813,8.6433132709844216,10.049816606437961,11.4563199418915,12.862823277345042,14.269326612798583,15.675829948252121,17.082333283705662,18.488836619159201,19.895339954612741,21.301843290066284,22.70834662551982,24.114849960973363],[-2.080100643182591,-0.67359730772905169,0.73290602772448876,2.1394093631780291,3.5459126986315685,4.9524160340851084,6.3589193695386497,7.7654227049921891,9.1719260404457295,10.578429375899269,11.984932711352808,13.39143604680635,14.797939382259891,16.204442717713427,17.61094605316697,19.017449388620509,20.423952724074049,21.830456059527592,23.236959394981128,24.643462730434671],[-1.5514878737212834,-0.14498453826774382,1.2615187971857966,2.668022132639337,4.0745254680928769,5.4810288035464163,6.8875321389999575,8.2940354744534979,9.7005388099070373,11.107042145360577,12.513545480814116,13.920048816267657,15.326552151721199,16.733055487174735,18.139558822628278,19.546062158081817,20.952565493535356,22.359068828988899,23.765572164442435,25.172075499895978],[-1.0228751042599755,0.38362823119356404,1.7901315666471045,3.1966349021006448,4.6031382375541847,6.0096415730077242,7.4161449084612654,8.8226482439148057,10.229151579368345,11.635654914821885,13.042158250275424,14.448661585728965,15.855164921182507,17.261668256636042,18.668171592089585,20.074674927543125,21.481178262996664,22.887681598450207,24.294184933903743,25.700688269357286],[-0.49426233479866943,0.91224100065487013,2.3187443361084106,3.7252476715619509,5.1317510070154908,6.5382543424690303,7.9447576779225715,9.3512610133761118,10.757764348829651,12.164267684283191,13.57077101973673,14.977274355190271,16.383777690643811,17.79028102609735,19.19678436155089,20.603287697004429,22.009791032457972,23.416294367911512,24.822797703365051,26.229301038818591],[0.034350434662638436,1.4408537701161779,2.8473571055697184,4.2538604410232592,5.6603637764767987,7.0668671119303381,8.4733704473838785,9.8798737828374197,11.286377118290959,12.692880453744499,14.099383789198038,15.505887124651579,16.912390460105119,18.318893795558658,19.725397131012198,21.131900466465737,22.53840380191928,23.94490713737282,25.351410472826359,26.757913808279898],[0.56296320412394629,1.9694665395774857,3.3759698750310263,4.7824732104845671,6.1889765459381065,7.595479881391646,9.0019832168451863,10.408486552298728,11.814989887752267,13.221493223205806,14.627996558659346,16.034499894112887,17.441003229566427,18.847506565019966,20.254009900473505,21.660513235927045,23.067016571380588,24.473519906834127,25.880023242287667,27.286526577741206],[1.0915759735852542,2.4980793090387938,3.9045826444923342,5.3110859799458749,6.7175893153994144,8.124092650852953,9.5305959863064942,10.937099321760035,12.343602657213575,13.750105992667114,15.156609328120654,16.563112663574195,17.969615999027734,19.376119334481274,20.782622669934813,22.189126005388353,23.595629340841896,25.002132676295435,26.408636011748975,27.815139347202514],[1.620188743046562,3.0266920785001017,4.4331954139536416,5.8396987494071828,7.2462020848607223,8.6527054203142608,10.059208755767802,11.465712091221343,12.872215426674883,14.278718762128422,15.685222097581962,17.091725433035503,18.498228768489042,19.904732103942582,21.311235439396121,22.717738774849661,24.124242110303204,25.530745445756743,26.937248781210283,28.343752116663822],[2.1488015125078701,3.5553048479614096,4.9618081834149494,6.3683115188684907,7.7748148543220301,9.1813181897755687,10.58782152522911,11.994324860682651,13.400828196136191,14.80733153158973,16.213834867043268,17.620338202496811,19.02684153795035,20.43334487340389,21.839848208857429,23.246351544310969,24.652854879764512,26.059358215218051,27.46586155067159,28.87236488612513],[2.6774142819691762,4.0839176174227152,5.4904209528762555,6.8969242883297968,8.3034276237833353,9.7099309592368748,11.116434294690416,12.522937630143957,13.929440965597497,15.335944301051036,16.742447636504576,18.148950971958115,19.555454307411658,20.961957642865194,22.368460978318737,23.774964313772276,25.181467649225816,26.587970984679359,27.994474320132895,29.400977655586438],[3.2060270514304841,4.6125303868840231,6.0190337223375634,7.4255370577911046,8.8320403932446432,10.238543728698183,11.645047064151724,13.051550399605265,14.458053735058805,15.864557070512344,17.271060405965883,18.677563741419423,20.084067076872966,21.490570412326502,22.897073747780045,24.303577083233584,25.710080418687124,27.116583754140667,28.523087089594203,29.929590425047746],[3.7346398208917919,5.1411431563453309,6.5476464917988713,7.9541498272524125,9.360653162705951,10.76715649815949,12.173659833613032,13.580163169066573,14.986666504520112,16.393169839973652,17.799673175427191,19.206176510880731,20.612679846334274,22.01918318178781,23.425686517241353,24.832189852694892,26.238693188148432,27.645196523601975,29.05169985905551,30.458203194509053],[4.2632525903530993,5.6697559258066388,7.0762592612601791,8.4827625967137195,9.8892659321672589,11.295769267620798,12.70227260307434,14.108775938527881,15.51527927398142,16.92178260943496,18.328285944888499,19.734789280342039,21.141292615795582,22.547795951249118,23.954299286702661,25.3608026221562,26.767305957609739,28.173809293063282,29.580312628516818,30.986815963970361],[4.7918653598144072,6.1983686952679466,7.604872030721487,9.0113753661750273,10.417878701628567,11.824382037082106,13.230885372535647,14.637388707989189,16.043892043442728,17.450395378896268,18.856898714349807,20.263402049803346,21.669905385256889,23.076408720710425,24.482912056163968,25.889415391617508,27.295918727071047,28.70242206252459,30.108925397978126,31.515428733431669]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"opacity":0.69999999999999996,"name":"Fitted Plane","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>This is, of course, a very idealized situation. There is no curvature in the plane,
no interaction, no outliers, no heteroscadasticity. It’s the simplest case of multiple regression
with 2 predictors. Reality is - usually - more complicated.</p>
<p>Let’s look at the summary output and check model assumptions:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="multiple-linear-regression.html#cb135-1" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7460 -1.3215 -0.2489  1.2427  4.1597 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.27013    0.19228   53.41   &lt;2e-16 ***
## X1           0.44673    0.04195   10.65   &lt;2e-16 ***
## X2           1.00952    0.03960   25.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.903 on 97 degrees of freedom
## Multiple R-squared:  0.8839, Adjusted R-squared:  0.8815 
## F-statistic: 369.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="multiple-linear-regression.html#cb137-1" tabindex="-1"></a><span class="fu">check_model</span>(m4<span class="fl">.3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>We could repeat this simulation to get a feeling for the variability.
The posterior predictive checks look nice. In this case, we <em>know</em> that the model is true.</p>
<div id="adding-variables-to-the-model-and-why" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> Adding variables to the model and why<a href="multiple-linear-regression.html#adding-variables-to-the-model-and-why" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is a very complex question. We will go into it in later chapters and the next course (Methodenvertiefung).
At this point we can say this:
We add variables to the model (and probably use
other models apart from linear regression) <em>depending</em> on the goal at hand (prediction or explanation).
Prediction seems to be easier than explanation. For instance, within linear models and
just a handful of predictors, one can even brute force the problem by searching through
all subsets of predictors. If that is not possible, one could use clever algortithms, like
<a href="https://www.sthda.com/english/articles/37-model-selection-essentials-in-r/155-best-subsets-regression-essentials-in-r/">best subest selection</a>.</p>
<ul>
<li>What is <strong>not</strong> a good idea is to throw all variables into the model and hope for the best -
especially if we want to learn the true relationships between the variables.</li>
<li>What is also not a good idea is to select variables depending on the <span class="math inline">\(p\)</span>-values of the coefficients (Westfall Chapter 11).</li>
<li><strong>Leaving variables out</strong>, that are important, can lead to biased estimates of the coefficients
(<a href="https://en.wikipedia.org/wiki/Omitted-variable_bias">omitted variable bias</a>).</li>
<li>Importantly, also <strong>adding variables</strong> can hurt conclusions from the model (see Statistical Rethinking 6.2).</li>
</ul>
</div>
</div>
<div id="interaction_term" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Interaction Term <span class="math inline">\(X_1 \times X_2\)</span><a href="multiple-linear-regression.html#interaction_term" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I recommend reading the excellent explanations about interactions
in John Kruschke’s book <a href="https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf">Doing Bayesian Data Analysis</a>,
15.2.2 und 15.2.3. Peter Westfall also has a nice explanation in his <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">book</a>
in section 9.3.</p>
<p>Our statistical model is now:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \mathbf{\beta_3 X_{1i} \times X_{2i}} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, \sigma^2)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1} \times x_{2}\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>for example:</p>
<p><span class="math display">\[ Y_i = 10 + 0.5 \cdot X_{1i} + 1 \cdot X_{2i} + 0.89 \cdot X_{1i} \times X_{2i} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, 5)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = 10 + 0.5 x_1 + 1 x_2 + 0.89 x_1 \times x_2\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>The second equation states that the conditional expectation of <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(X_1=x_1\)</span> and <span class="math inline">\(X_2=x_2\)</span>
is a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and their interaction <span class="math inline">\(x_1 \times x_2\)</span> (i.e., the product). We are in a different situation now.
Set for instance <span class="math inline">\(x_2\)</span> to a certain value, say <span class="math inline">\(x_2 = 7\)</span>. Then the relationship (in expectation)
between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> is:</p>
<p><span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = 7) = 10 + 0.5 x_1 + 1 \cdot 7 + 0.89 x_1 \cdot 7\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = 7) = 10 + (0.5 + 0.89 \cdot \mathbf{7}) \cdot x_1 + 1 \cdot 7\]</span></p>
<p>Depending on the value of <span class="math inline">\(x_2\)</span>, the <em>effect</em> of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> changes.
Hence, <span class="math inline">\(X_2\)</span> <strong>modifies</strong> the relationship between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y\)</span>, or stated otherwise,
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> <strong>interact</strong> with respect to <span class="math inline">\(Y\)</span>. Remember, the word <em>effect</em> is
used in a strictly technical/statistical sense and <strong>not in a causal</strong> sense.
It does not mean that if we <em>do</em> change <span class="math inline">\(X_1\)</span> by one unit,
<span class="math inline">\(Y\)</span> will also change in an experiment. We are purely describing the relationship
in an associative way. We will probably touch causality in a later chapter.
Bayesian statistics and causal inference are gaining popularity. Hence, we should try to keep up.</p>
<p>Let’s draw 100 points from this model, fit the model and add the plane (see also <a href="multiple-linear-regression.html#exercise3_multiple_regression">exercise 4</a>):</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="multiple-linear-regression.html#cb138-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb138-2"><a href="multiple-linear-regression.html#cb138-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb138-3"><a href="multiple-linear-regression.html#cb138-3" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb138-4"><a href="multiple-linear-regression.html#cb138-4" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb138-5"><a href="multiple-linear-regression.html#cb138-5" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fl">0.89</span> <span class="sc">*</span> X1 <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb138-6"><a href="multiple-linear-regression.html#cb138-6" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb138-7"><a href="multiple-linear-regression.html#cb138-7" tabindex="-1"></a></span>
<span id="cb138-8"><a href="multiple-linear-regression.html#cb138-8" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb138-9"><a href="multiple-linear-regression.html#cb138-9" tabindex="-1"></a>m4<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">*</span> X2, <span class="at">data =</span> d)</span>
<span id="cb138-10"><a href="multiple-linear-regression.html#cb138-10" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 * X2, data = d)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.360 -3.389 -0.543  2.949 11.583 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.70491    0.47888  22.354  &lt; 2e-16 ***
## X1           0.40719    0.10834   3.759 0.000293 ***
## X2           1.03434    0.09881  10.468  &lt; 2e-16 ***
## X1:X2        0.92182    0.02290  40.257  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.734 on 96 degrees of freedom
## Multiple R-squared:  0.9476, Adjusted R-squared:  0.9459 
## F-statistic: 578.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="multiple-linear-regression.html#cb140-1" tabindex="-1"></a><span class="co"># Create a grid for the plane</span></span>
<span id="cb140-2"><a href="multiple-linear-regression.html#cb140-2" tabindex="-1"></a>X1_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X1), <span class="fu">max</span>(d<span class="sc">$</span>X1), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb140-3"><a href="multiple-linear-regression.html#cb140-3" tabindex="-1"></a>X2_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X2), <span class="fu">max</span>(d<span class="sc">$</span>X2), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb140-4"><a href="multiple-linear-regression.html#cb140-4" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> X1_grid, <span class="at">X2 =</span> X2_grid)</span>
<span id="cb140-5"><a href="multiple-linear-regression.html#cb140-5" tabindex="-1"></a></span>
<span id="cb140-6"><a href="multiple-linear-regression.html#cb140-6" tabindex="-1"></a><span class="co"># Predict the values for the grid</span></span>
<span id="cb140-7"><a href="multiple-linear-regression.html#cb140-7" tabindex="-1"></a>grid<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">predict</span>(m4<span class="fl">.4</span>, <span class="at">newdata =</span> grid)</span>
<span id="cb140-8"><a href="multiple-linear-regression.html#cb140-8" tabindex="-1"></a></span>
<span id="cb140-9"><a href="multiple-linear-regression.html#cb140-9" tabindex="-1"></a><span class="co"># Convert the grid into a matrix for the plane</span></span>
<span id="cb140-10"><a href="multiple-linear-regression.html#cb140-10" tabindex="-1"></a>plane_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(grid<span class="sc">$</span>Y, <span class="at">nrow =</span> <span class="fu">length</span>(X1_grid), <span class="at">ncol =</span> <span class="fu">length</span>(X2_grid))</span>
<span id="cb140-11"><a href="multiple-linear-regression.html#cb140-11" tabindex="-1"></a></span>
<span id="cb140-12"><a href="multiple-linear-regression.html#cb140-12" tabindex="-1"></a><span class="co"># Create the interactive 3D plot</span></span>
<span id="cb140-13"><a href="multiple-linear-regression.html#cb140-13" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb140-14"><a href="multiple-linear-regression.html#cb140-14" tabindex="-1"></a>  <span class="fu">add_markers</span>(</span>
<span id="cb140-15"><a href="multiple-linear-regression.html#cb140-15" tabindex="-1"></a>    <span class="at">x =</span> d<span class="sc">$</span>X2, <span class="at">y =</span> d<span class="sc">$</span>X1, <span class="at">z =</span> d<span class="sc">$</span>Y,</span>
<span id="cb140-16"><a href="multiple-linear-regression.html#cb140-16" tabindex="-1"></a>    <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>),</span>
<span id="cb140-17"><a href="multiple-linear-regression.html#cb140-17" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Data Points&quot;</span></span>
<span id="cb140-18"><a href="multiple-linear-regression.html#cb140-18" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb140-19"><a href="multiple-linear-regression.html#cb140-19" tabindex="-1"></a>  <span class="fu">add_surface</span>(</span>
<span id="cb140-20"><a href="multiple-linear-regression.html#cb140-20" tabindex="-1"></a>    <span class="at">x =</span> X1_grid, <span class="at">y =</span> X2_grid, <span class="at">z =</span> plane_matrix,</span>
<span id="cb140-21"><a href="multiple-linear-regression.html#cb140-21" tabindex="-1"></a>    <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;pink&quot;</span>)),</span>
<span id="cb140-22"><a href="multiple-linear-regression.html#cb140-22" tabindex="-1"></a>    <span class="at">showscale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb140-23"><a href="multiple-linear-regression.html#cb140-23" tabindex="-1"></a>    <span class="at">opacity =</span> <span class="fl">0.7</span>,</span>
<span id="cb140-24"><a href="multiple-linear-regression.html#cb140-24" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Fitted Plane&quot;</span></span>
<span id="cb140-25"><a href="multiple-linear-regression.html#cb140-25" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb140-26"><a href="multiple-linear-regression.html#cb140-26" tabindex="-1"></a>  plotly<span class="sc">::</span><span class="fu">layout</span>(</span>
<span id="cb140-27"><a href="multiple-linear-regression.html#cb140-27" tabindex="-1"></a>    <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb140-28"><a href="multiple-linear-regression.html#cb140-28" tabindex="-1"></a>      <span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X1&quot;</span>),</span>
<span id="cb140-29"><a href="multiple-linear-regression.html#cb140-29" tabindex="-1"></a>      <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb140-30"><a href="multiple-linear-regression.html#cb140-30" tabindex="-1"></a>      <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb140-31"><a href="multiple-linear-regression.html#cb140-31" tabindex="-1"></a>    ),</span>
<span id="cb140-32"><a href="multiple-linear-regression.html#cb140-32" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Interactive 3D Scatterplot with Fitted Plane&quot;</span></span>
<span id="cb140-33"><a href="multiple-linear-regression.html#cb140-33" tabindex="-1"></a>  )</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-a5f5467af5736aaa4a5f" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-a5f5467af5736aaa4a5f">{"x":{"visdat":{"15b725b087589":["function () ","plotlyVisDat"]},"cur_data":"15b725b087589","attrs":{"15b725b087589":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[24.900013922362266,15.955422852644634,2.7820009352490125,10.609299378353359,0.75595120508472347,9.9630225678275242,-4.7647440018698264,42.473134146781916,20.447075259121547,4.097936569996989,-4.8903504444429711,20.025069148008328,-5.3520322201328261,7.2817458867592117,-0.17885311360416445,36.325035818677819,10.737784581218953,26.300118496630901,-11.936277328943131,8.0472331765234948,2.2536317108471851,12.402815512497188,21.730181511740437,14.588512334386017,-9.8043916392185455,27.289246624957791,14.136203165018296,7.4531916776504197,31.126248517654151,5.7107250612018614,41.772480646442276,8.1039657440360511,14.337890281588734,-1.8647306915661703,-38.617953729844096,28.12783274192989,-14.834936799323591,14.620566293554468,7.405587209546626,10.144922605230985,-3.0185977041701286,6.8711931559023345,50.719918674049417,-70.934222069539814,-38.92776895811437,27.316416239935116,14.28258682804462,-1.6689540385652979,55.572371054635738,8.1711171252685162,17.133937726262264,10.475993548160442,9.517373279356951,-21.87437437112408,17.430912111860906,2.4854232187703431,-5.0534153715020311,7.9085559776505816,17.318604241938697,-0.79730899298882818,22.499300554440318,12.775287394482241,12.444597445583575,-43.292913122856767,26.642579663547046,22.001948555227052,19.985915742248498,-1.6395733394024781,23.552321907871388,34.246160195042805,14.273712122329943,6.0114915628604297,15.003211013005902,-21.693482705696752,20.170028293665311,-20.158337233206346,10.111588146619228,0.44040731140359535,16.537646691015635,8.9008212730779004,-3.7759706045968535,31.790247639458684,12.138862504610398,-6.4540663211198979,10.017254758671459,9.0581868933193217,46.484032548424409,20.536364268491919,6.3918067737995958,-1.546502148104872,24.138317936820194,11.056704900089466,17.298469528461155,13.575119078116852,-22.82384617172664,-7.8554119089381178,57.041528424468389,-41.848430890569404,9.6649950298198188,34.829954351043469],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5},"name":"Data Points","inherit":true},"15b725b087589.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[104.65039627750222,91.263006074593648,77.875615871685056,64.488225668776479,51.100835465867902,37.713445262959318,24.326055060050731,10.938664857142152,-2.4487253457664253,-15.836115548675004,-29.223505751583577,-42.610895954492172,-55.998286157400777,-69.385676360309333,-82.773066563217924,-96.160456766126487,-109.54784696903508,-122.93523717194367,-136.32262737485223,-149.71001757776082],[93.933996537108982,82.066320422806172,70.198644308503347,58.330968194200537,46.463292079897727,34.595615965594916,22.727939851292092,10.860263736989284,-1.0074123773135248,-12.875088491616333,-24.742764605919142,-36.610440720221973,-48.478116834524791,-60.345792948827587,-72.213469063130404,-84.0811451774332,-95.948821291736024,-107.81649740603885,-119.68417352034164,-131.55184963464447],[83.217596796715711,72.869634771018667,62.521672745321624,52.17371071962458,41.825748693927537,31.477786668230504,21.129824642533453,10.781862616836415,0.43390059113937662,-9.9140614345576612,-20.262023460254699,-30.609985485951754,-40.957947511648804,-51.305909537345833,-61.653871563042877,-72.001833588739913,-82.349795614436957,-92.697757640134014,-103.04571966583103,-113.39368169152807],[72.501197056322454,63.672949119231184,54.844701182139907,46.016453245048638,37.188205307957368,28.359957370866095,19.531709433774814,10.703461496683545,1.8752135595922754,-6.9530343774989918,-15.781282314590266,-24.609530251681544,-33.437778188772825,-42.266026125864087,-51.094274062955364,-59.922522000046627,-68.750769937137903,-77.579017874229194,-86.407265811320443,-95.235513748411734],[61.784797315929197,54.476263467443694,47.167729618958191,39.859195770472688,32.550661921987185,25.242128073501686,17.933594225016179,10.625060376530678,3.3165265280451779,-3.9920073204403237,-11.300541168925823,-18.609075017411335,-25.917608865896842,-33.226142714382334,-40.534676562867844,-47.843210411353333,-55.151744259838843,-62.46027810832436,-69.768811956809841,-77.077345805295352],[51.06839757553594,45.279577815656204,39.490758055776475,33.701938295896738,27.913118536017009,22.12429877613728,16.335479016257541,10.54665925637781,4.7578394964980779,-1.0309802633816521,-6.819800023261382,-12.608619783141119,-18.397439543020859,-24.186259302900581,-29.975079062780321,-35.763898822660039,-41.552718582539789,-47.341538342419526,-53.13035810229924,-58.919177862178984],[40.35199783514269,36.082892163868728,31.813786492594758,27.544680821320792,23.275575150046834,19.006469478772871,14.737363807498902,10.468258136224941,6.199152464950977,1.9300467936770163,-2.3390588775969454,-6.6081645488709135,-10.877270220144883,-15.146375891418838,-19.415481562692808,-23.684587233966763,-27.953692905240725,-32.222798576514698,-36.491904247788653,-40.76100991906263],[29.635598094749415,26.886206512081227,24.136814929413035,21.387423346744839,18.638031764076647,15.888640181408457,13.139248598740263,10.389857016072071,7.6404654334038788,4.8910738507356877,2.1416822680674952,-0.60770931460069832,-3.3571008972688974,-6.1064924799370832,-8.8558840626052788,-11.60527564527346,-14.354667227941663,-17.104058810609857,-19.853450393278045,-22.602841975946241],[18.919198354356169,17.689520860293744,16.459843366231318,15.230165872168897,14.000488378106473,12.77081088404405,11.541133389981624,10.311455895919202,9.0817784018567789,7.8521009077943562,6.6224234137319327,5.3927459196695091,4.1630684256070829,2.9333909315446629,1.7037134374822367,0.47403594341981381,-0.75564155064260885,-1.9853190447050315,-3.2149965387674579,-4.444674032829881],[8.2027986139629157,8.4928352085062606,8.7828718030496056,9.0729083975929523,9.3629449921362973,9.6529815866796422,9.9430181812229872,10.233054775766334,10.523091370309677,10.813127964853024,11.103164559396369,11.393201153939716,11.683237748483059,11.973274343026405,12.26331093756975,12.553347532113097,12.84338412665644,13.133420721199787,13.423457315743134,13.713493910286475],[-2.5136011264303506,-0.70385044328123481,1.1059002398678821,2.9156509230169991,4.7254016061661153,6.5351522893152314,8.3449029724643484,10.154653655613465,11.964404338762581,13.774155021911698,15.583905705060813,17.393656388209934,19.203407071359049,21.013157754508164,22.822908437657283,24.632659120806398,26.442409803955513,28.252160487104632,30.061911170253747,31.871661853402863],[-13.230000866823602,-9.900536095068718,-6.5710713233138316,-3.2416065515589447,0.087858220195938974,3.4173229919508237,6.7467877637057114,10.076252535460597,13.405717307215481,16.735182078970364,20.06464685072525,23.394111622480139,26.723576394235028,30.053041165989907,33.382505937744796,36.711970709499674,40.041435481254567,43.370900253009452,46.700365024764331,50.029829796519223],[-23.946400607216852,-19.097221746856199,-14.248042886495544,-9.3988640261348877,-4.5496851657742372,0.29949369458641656,5.1486725549470744,9.9978514153077285,14.847030275668381,19.696209136029033,24.545387996389685,29.394566856750345,34.243745717111004,39.092924577471649,43.942103437832309,48.791282298192954,53.640461158553613,58.489640018914272,63.338818879274918,68.187997739635577],[-34.662800347610123,-28.293907398643697,-21.925014449677271,-15.556121500710844,-9.1872285517444201,-2.8183356027779962,3.5505573461884339,9.9194502951548582,16.288343244121283,22.657236193087705,29.026129142054131,35.395022091020557,41.763915039986991,48.13280798895341,54.501700937919836,60.870593886886248,67.239486835852688,73.608379784819121,79.977272733785526,86.346165682751959],[-45.379200088003387,-37.490593050431194,-29.601986012858998,-21.713378975286794,-13.824771937714601,-5.9361649001424066,1.9524421374297949,9.8410491750019897,17.729656212574184,25.618263250146381,33.506870287718577,41.395477325290777,49.284084362862977,57.172691400435163,65.06129843800737,72.949905475579556,80.838512513151755,88.72711955072397,96.615726588296155,104.50433362586836],[-56.09559982839663,-46.687278702218663,-37.278957576040696,-27.870636449862729,-18.46231532368477,-9.0539941975068139,0.35432692867115967,9.7626480548491212,19.170969181027079,28.579290307205042,37.987611433383002,47.395932559560968,56.804253685738942,66.212574811916895,75.620895938094861,85.029217064272814,94.437538190450795,103.84585931662876,113.25418044280671,122.66250156898468],[-66.811999568789901,-55.88396435400616,-44.95592913922242,-34.027893924438679,-23.099858709654956,-12.171823494871223,-1.2437882800874784,9.6842469346962528,20.612282149479984,31.540317364263714,42.468352579047448,53.396387793831195,64.324423008614929,75.252458223398648,86.180493438182395,97.108528652966115,108.03656386774985,118.96459908253361,129.89263429731733,140.82066951210106],[-77.528399309183158,-65.080650005793657,-52.63290070240415,-40.185151399014643,-27.737402095625136,-15.289652792235637,-2.8419034888461203,9.6058458145433843,22.053595117932883,34.501344421322386,46.949093724711886,59.396843028101408,71.844592331490915,84.292341634880401,96.74009093826993,109.18784024165942,121.63558954504893,134.08333884843844,146.53108815182793,158.97883745521744],[-88.2447990495764,-74.277335657581119,-60.309872265585852,-46.342408873590578,-32.374945481595304,-18.407482089600038,-4.440018697604752,9.5274446943905158,23.494908086385781,37.462371478381051,51.429834870376318,65.397298262371606,79.364761654366887,93.33222504636214,107.29968843835742,121.26715183035267,135.23461522234797,149.20207861434326,163.16954200633847,177.13700539833377],[-98.961198789969657,-83.474021309368624,-67.986843828767576,-52.499666348166521,-37.012488867565487,-21.525311386964454,-6.0381339063633934,9.4490435742376455,24.936221054838686,40.423398535439723,55.910576016040757,71.397753496641826,86.884930977242874,102.37210845784391,117.85928593844496,133.34646341904599,148.83364089964704,164.32081838024808,179.8079958608491,195.29517334145018]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"colorscale":[[0,1],["red","pink"]],"showscale":false,"opacity":0.69999999999999996,"name":"Fitted Plane","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Y"}},"title":"Interactive 3D Scatterplot with Fitted Plane","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[24.900013922362266,15.955422852644634,2.7820009352490125,10.609299378353359,0.75595120508472347,9.9630225678275242,-4.7647440018698264,42.473134146781916,20.447075259121547,4.097936569996989,-4.8903504444429711,20.025069148008328,-5.3520322201328261,7.2817458867592117,-0.17885311360416445,36.325035818677819,10.737784581218953,26.300118496630901,-11.936277328943131,8.0472331765234948,2.2536317108471851,12.402815512497188,21.730181511740437,14.588512334386017,-9.8043916392185455,27.289246624957791,14.136203165018296,7.4531916776504197,31.126248517654151,5.7107250612018614,41.772480646442276,8.1039657440360511,14.337890281588734,-1.8647306915661703,-38.617953729844096,28.12783274192989,-14.834936799323591,14.620566293554468,7.405587209546626,10.144922605230985,-3.0185977041701286,6.8711931559023345,50.719918674049417,-70.934222069539814,-38.92776895811437,27.316416239935116,14.28258682804462,-1.6689540385652979,55.572371054635738,8.1711171252685162,17.133937726262264,10.475993548160442,9.517373279356951,-21.87437437112408,17.430912111860906,2.4854232187703431,-5.0534153715020311,7.9085559776505816,17.318604241938697,-0.79730899298882818,22.499300554440318,12.775287394482241,12.444597445583575,-43.292913122856767,26.642579663547046,22.001948555227052,19.985915742248498,-1.6395733394024781,23.552321907871388,34.246160195042805,14.273712122329943,6.0114915628604297,15.003211013005902,-21.693482705696752,20.170028293665311,-20.158337233206346,10.111588146619228,0.44040731140359535,16.537646691015635,8.9008212730779004,-3.7759706045968535,31.790247639458684,12.138862504610398,-6.4540663211198979,10.017254758671459,9.0581868933193217,46.484032548424409,20.536364268491919,6.3918067737995958,-1.546502148104872,24.138317936820194,11.056704900089466,17.298469528461155,13.575119078116852,-22.82384617172664,-7.8554119089381178,57.041528424468389,-41.848430890569404,9.6649950298198188,34.829954351043469],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Data Points","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"red"],[1,"pink"]],"showscale":false,"z":[[104.65039627750222,91.263006074593648,77.875615871685056,64.488225668776479,51.100835465867902,37.713445262959318,24.326055060050731,10.938664857142152,-2.4487253457664253,-15.836115548675004,-29.223505751583577,-42.610895954492172,-55.998286157400777,-69.385676360309333,-82.773066563217924,-96.160456766126487,-109.54784696903508,-122.93523717194367,-136.32262737485223,-149.71001757776082],[93.933996537108982,82.066320422806172,70.198644308503347,58.330968194200537,46.463292079897727,34.595615965594916,22.727939851292092,10.860263736989284,-1.0074123773135248,-12.875088491616333,-24.742764605919142,-36.610440720221973,-48.478116834524791,-60.345792948827587,-72.213469063130404,-84.0811451774332,-95.948821291736024,-107.81649740603885,-119.68417352034164,-131.55184963464447],[83.217596796715711,72.869634771018667,62.521672745321624,52.17371071962458,41.825748693927537,31.477786668230504,21.129824642533453,10.781862616836415,0.43390059113937662,-9.9140614345576612,-20.262023460254699,-30.609985485951754,-40.957947511648804,-51.305909537345833,-61.653871563042877,-72.001833588739913,-82.349795614436957,-92.697757640134014,-103.04571966583103,-113.39368169152807],[72.501197056322454,63.672949119231184,54.844701182139907,46.016453245048638,37.188205307957368,28.359957370866095,19.531709433774814,10.703461496683545,1.8752135595922754,-6.9530343774989918,-15.781282314590266,-24.609530251681544,-33.437778188772825,-42.266026125864087,-51.094274062955364,-59.922522000046627,-68.750769937137903,-77.579017874229194,-86.407265811320443,-95.235513748411734],[61.784797315929197,54.476263467443694,47.167729618958191,39.859195770472688,32.550661921987185,25.242128073501686,17.933594225016179,10.625060376530678,3.3165265280451779,-3.9920073204403237,-11.300541168925823,-18.609075017411335,-25.917608865896842,-33.226142714382334,-40.534676562867844,-47.843210411353333,-55.151744259838843,-62.46027810832436,-69.768811956809841,-77.077345805295352],[51.06839757553594,45.279577815656204,39.490758055776475,33.701938295896738,27.913118536017009,22.12429877613728,16.335479016257541,10.54665925637781,4.7578394964980779,-1.0309802633816521,-6.819800023261382,-12.608619783141119,-18.397439543020859,-24.186259302900581,-29.975079062780321,-35.763898822660039,-41.552718582539789,-47.341538342419526,-53.13035810229924,-58.919177862178984],[40.35199783514269,36.082892163868728,31.813786492594758,27.544680821320792,23.275575150046834,19.006469478772871,14.737363807498902,10.468258136224941,6.199152464950977,1.9300467936770163,-2.3390588775969454,-6.6081645488709135,-10.877270220144883,-15.146375891418838,-19.415481562692808,-23.684587233966763,-27.953692905240725,-32.222798576514698,-36.491904247788653,-40.76100991906263],[29.635598094749415,26.886206512081227,24.136814929413035,21.387423346744839,18.638031764076647,15.888640181408457,13.139248598740263,10.389857016072071,7.6404654334038788,4.8910738507356877,2.1416822680674952,-0.60770931460069832,-3.3571008972688974,-6.1064924799370832,-8.8558840626052788,-11.60527564527346,-14.354667227941663,-17.104058810609857,-19.853450393278045,-22.602841975946241],[18.919198354356169,17.689520860293744,16.459843366231318,15.230165872168897,14.000488378106473,12.77081088404405,11.541133389981624,10.311455895919202,9.0817784018567789,7.8521009077943562,6.6224234137319327,5.3927459196695091,4.1630684256070829,2.9333909315446629,1.7037134374822367,0.47403594341981381,-0.75564155064260885,-1.9853190447050315,-3.2149965387674579,-4.444674032829881],[8.2027986139629157,8.4928352085062606,8.7828718030496056,9.0729083975929523,9.3629449921362973,9.6529815866796422,9.9430181812229872,10.233054775766334,10.523091370309677,10.813127964853024,11.103164559396369,11.393201153939716,11.683237748483059,11.973274343026405,12.26331093756975,12.553347532113097,12.84338412665644,13.133420721199787,13.423457315743134,13.713493910286475],[-2.5136011264303506,-0.70385044328123481,1.1059002398678821,2.9156509230169991,4.7254016061661153,6.5351522893152314,8.3449029724643484,10.154653655613465,11.964404338762581,13.774155021911698,15.583905705060813,17.393656388209934,19.203407071359049,21.013157754508164,22.822908437657283,24.632659120806398,26.442409803955513,28.252160487104632,30.061911170253747,31.871661853402863],[-13.230000866823602,-9.900536095068718,-6.5710713233138316,-3.2416065515589447,0.087858220195938974,3.4173229919508237,6.7467877637057114,10.076252535460597,13.405717307215481,16.735182078970364,20.06464685072525,23.394111622480139,26.723576394235028,30.053041165989907,33.382505937744796,36.711970709499674,40.041435481254567,43.370900253009452,46.700365024764331,50.029829796519223],[-23.946400607216852,-19.097221746856199,-14.248042886495544,-9.3988640261348877,-4.5496851657742372,0.29949369458641656,5.1486725549470744,9.9978514153077285,14.847030275668381,19.696209136029033,24.545387996389685,29.394566856750345,34.243745717111004,39.092924577471649,43.942103437832309,48.791282298192954,53.640461158553613,58.489640018914272,63.338818879274918,68.187997739635577],[-34.662800347610123,-28.293907398643697,-21.925014449677271,-15.556121500710844,-9.1872285517444201,-2.8183356027779962,3.5505573461884339,9.9194502951548582,16.288343244121283,22.657236193087705,29.026129142054131,35.395022091020557,41.763915039986991,48.13280798895341,54.501700937919836,60.870593886886248,67.239486835852688,73.608379784819121,79.977272733785526,86.346165682751959],[-45.379200088003387,-37.490593050431194,-29.601986012858998,-21.713378975286794,-13.824771937714601,-5.9361649001424066,1.9524421374297949,9.8410491750019897,17.729656212574184,25.618263250146381,33.506870287718577,41.395477325290777,49.284084362862977,57.172691400435163,65.06129843800737,72.949905475579556,80.838512513151755,88.72711955072397,96.615726588296155,104.50433362586836],[-56.09559982839663,-46.687278702218663,-37.278957576040696,-27.870636449862729,-18.46231532368477,-9.0539941975068139,0.35432692867115967,9.7626480548491212,19.170969181027079,28.579290307205042,37.987611433383002,47.395932559560968,56.804253685738942,66.212574811916895,75.620895938094861,85.029217064272814,94.437538190450795,103.84585931662876,113.25418044280671,122.66250156898468],[-66.811999568789901,-55.88396435400616,-44.95592913922242,-34.027893924438679,-23.099858709654956,-12.171823494871223,-1.2437882800874784,9.6842469346962528,20.612282149479984,31.540317364263714,42.468352579047448,53.396387793831195,64.324423008614929,75.252458223398648,86.180493438182395,97.108528652966115,108.03656386774985,118.96459908253361,129.89263429731733,140.82066951210106],[-77.528399309183158,-65.080650005793657,-52.63290070240415,-40.185151399014643,-27.737402095625136,-15.289652792235637,-2.8419034888461203,9.6058458145433843,22.053595117932883,34.501344421322386,46.949093724711886,59.396843028101408,71.844592331490915,84.292341634880401,96.74009093826993,109.18784024165942,121.63558954504893,134.08333884843844,146.53108815182793,158.97883745521744],[-88.2447990495764,-74.277335657581119,-60.309872265585852,-46.342408873590578,-32.374945481595304,-18.407482089600038,-4.440018697604752,9.5274446943905158,23.494908086385781,37.462371478381051,51.429834870376318,65.397298262371606,79.364761654366887,93.33222504636214,107.29968843835742,121.26715183035267,135.23461522234797,149.20207861434326,163.16954200633847,177.13700539833377],[-98.961198789969657,-83.474021309368624,-67.986843828767576,-52.499666348166521,-37.012488867565487,-21.525311386964454,-6.0381339063633934,9.4490435742376455,24.936221054838686,40.423398535439723,55.910576016040757,71.397753496641826,86.884930977242874,102.37210845784391,117.85928593844496,133.34646341904599,148.83364089964704,164.32081838024808,179.8079958608491,195.29517334145018]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"opacity":0.69999999999999996,"name":"Fitted Plane","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The term <code>X1 * X2</code> is a shortcut for <code>X1 + X2 + X1:X2</code> where <code>X1:X2</code> is the interaction term.
R automatically includes the main effects of the predictors when an interaction
term is included (variable inclusion principle).
The true but usually unknown <span class="math inline">\(\beta\)</span>s are estimated quite precisely.</p>
<div id="formal-test-for-interaction" class="section level4 hasAnchor" number="4.2.3.1">
<h4><span class="header-section-number">4.2.3.1</span> Formal test for interaction<a href="multiple-linear-regression.html#formal-test-for-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We could apply a formal test for the interaction term by model comparison.
The command <code>anova(., .)</code> would compare the two models and test if the change in the
residual sum of squares is statistically interesting.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="multiple-linear-regression.html#cb141-1" tabindex="-1"></a>m4<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2, <span class="at">data =</span> d) <span class="co"># without interaction</span></span>
<span id="cb141-2"><a href="multiple-linear-regression.html#cb141-2" tabindex="-1"></a><span class="fu">anova</span>(m4<span class="fl">.5</span>, m4<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Y ~ X1 + X2
## Model 2: Y ~ X1 * X2
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     97 38468                                  
## 2     96  2151  1     36316 1620.6 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>One can show that the following test statistic is <span class="math inline">\(F\)</span> distributed under the null hypothesis (that <span class="math inline">\(\beta_3=0\)</span>):</p>
<p><span class="math display">\[ F = \frac{\left(RSS_{\text{Model 1}} - RSS_{\text{Model 2}}\right) / \left(df_{\text{Model 1}} - df_{\text{Model 2}}\right)}{RSS_{\text{Model 2}} / df_{\text{Model 2}}}\]</span></p>
<p>where <span class="math inline">\(RSS\)</span> is the residual sum of squares,
<span class="math inline">\(df\)</span> are the degrees of freedom of the residual sum of squares for both models.</p>
<p>The output of the <code>anova</code> command shows us the residual degress of freedom (<code>Res.Df</code>)
of both models, the residual sum of squares errors of both models (<code>RSS</code>),
the sum of squared errors between model 1 and model 2 (<code>Sum of Sq</code>), the value of the
F-statistic and the <span class="math inline">\(p\)</span>-value for the hypothesis, that the coefficient for
the interaction term is zero (<span class="math inline">\(\beta_3=0\)</span>). Model 1 RSS has 97 degrees of freedom, since we have 100 data points
and 3 parameters to estimate (<span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span>). Model 2 has 96 degrees of freedom, since
we have 100 data points and 4 parameters to estimate (<span class="math inline">\(\beta_0, \beta_1, \beta_2, \beta_3\)</span>).</p>
<p>Let’s verify the value of the <span class="math inline">\(F\)</span> statistic:</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="multiple-linear-regression.html#cb143-1" tabindex="-1"></a>RSS_model1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(m4<span class="fl">.5</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb143-2"><a href="multiple-linear-regression.html#cb143-2" tabindex="-1"></a>RSS_model2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(m4<span class="fl">.4</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb143-3"><a href="multiple-linear-regression.html#cb143-3" tabindex="-1"></a>df_model1 <span class="ot">&lt;-</span> n <span class="sc">-</span> <span class="fu">length</span>(<span class="fu">coef</span>(m4<span class="fl">.5</span>))</span>
<span id="cb143-4"><a href="multiple-linear-regression.html#cb143-4" tabindex="-1"></a>df_model2 <span class="ot">&lt;-</span> n <span class="sc">-</span> <span class="fu">length</span>(<span class="fu">coef</span>(m4<span class="fl">.4</span>))</span>
<span id="cb143-5"><a href="multiple-linear-regression.html#cb143-5" tabindex="-1"></a>F <span class="ot">&lt;-</span> ((RSS_model1 <span class="sc">-</span> RSS_model2) <span class="sc">/</span> (df_model1 <span class="sc">-</span> df_model2)) <span class="sc">/</span> (RSS_model2 <span class="sc">/</span> df_model2)</span>
<span id="cb143-6"><a href="multiple-linear-regression.html#cb143-6" tabindex="-1"></a>F</span></code></pre></div>
<pre><code>## [1] 1620.606</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="multiple-linear-regression.html#cb145-1" tabindex="-1"></a><span class="co"># Sum of Sq</span></span>
<span id="cb145-2"><a href="multiple-linear-regression.html#cb145-2" tabindex="-1"></a>RSS_model1 <span class="sc">-</span> RSS_model2</span></code></pre></div>
<pre><code>## [1] 36316.28</code></pre>
<p>In the numerator of the <span class="math inline">\(F\)</span> statistic, we have the change in the residual sum of squares
(from the small (model 1) model to the larger one (model 2), <code>Sum of Sq</code>)
per additional parameter in the model (one additional parameter <span class="math inline">\(\beta_3\)</span>).</p>
<p>In the denominator, we have the residual sum of squares per residual degree of freedom of
the larger model (model 2). Hence, in the numerator we have the information on how much
better we get with respect to the number of variables added, and in the denominator
we have information on how good the full model is with respect to its degrees of freedom.</p>
<p>The <span class="math inline">\(p\)</span>-value is the probability of observing a value of the F statistic as extreme or more
extreme than the one we observed, given that the null hypothesis is true. Here,
the <span class="math inline">\(p\)</span>-value is extremely small. So, statistically we would see an improvement in RSS
which is not explainable by chance alone.
But <strong>let’s be careful with <span class="math inline">\(p\)</span>-values</strong> and especially with fixed cutoff values for <span class="math inline">\(\alpha\)</span>,
which we will <strong>never</strong> use in this script.
Even for a rather small effect <span class="math inline">\(\beta_3\)</span>, we would reject the null hypothesis, if only the sample
size is large enough. Since a very small effect relative to <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> would
probably not be of practical interest, one should be careful with looking at <span class="math inline">\(p\)</span>-values alone.
For instance, in Richard McElreath’s book <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>,
there are no <span class="math inline">\(p\)</span>-values at all. I like that.</p>
<p>If you again look at the comparison of the RSS between the two models, you would
immediately see that the model with the interaction term is better (at least with respect to this metric).
The difference is huge. We have already mentioned in the context of <span class="math inline">\(R^2\)</span> not to overinterpret
such metric, because RSS is monotonically descreasing with number of variables added and reaches
zero when the number of variables equals the number of data points (see <a href="multiple-linear-regression.html#exercise3_multiple_regression">exercise 3</a>).</p>
</div>
</div>
<div id="interaction_plot" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Using an interaction plot to see a potential interaction<a href="multiple-linear-regression.html#interaction_plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chronologically before we include an interaction term in the model, we can use an interaction plot
to see if there is a potential interaction between the predictors.
We can just create a catorical predictor out of the continuous predictors.
Just categorize the predictors into quartiles and plot the means of the dependent variable (<span class="math inline">\(Y\)</span>).
If the lines are parallel, there is probably no interaction. If the lines are not parallel,
there might be an interaction.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="multiple-linear-regression.html#cb147-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb147-2"><a href="multiple-linear-regression.html#cb147-2" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb147-3"><a href="multiple-linear-regression.html#cb147-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb147-4"><a href="multiple-linear-regression.html#cb147-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fl">0.89</span> <span class="sc">*</span> X1 <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb147-5"><a href="multiple-linear-regression.html#cb147-5" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb147-6"><a href="multiple-linear-regression.html#cb147-6" tabindex="-1"></a></span>
<span id="cb147-7"><a href="multiple-linear-regression.html#cb147-7" tabindex="-1"></a><span class="co"># Create categorical variables based on quartiles</span></span>
<span id="cb147-8"><a href="multiple-linear-regression.html#cb147-8" tabindex="-1"></a>d<span class="sc">$</span>X2_cat <span class="ot">&lt;-</span> <span class="fu">cut</span>(d<span class="sc">$</span>X2, </span>
<span id="cb147-9"><a href="multiple-linear-regression.html#cb147-9" tabindex="-1"></a>                <span class="at">breaks =</span> <span class="fu">quantile</span>(d<span class="sc">$</span>X2, <span class="at">probs =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>), <span class="at">na.rm =</span> <span class="cn">TRUE</span>), </span>
<span id="cb147-10"><a href="multiple-linear-regression.html#cb147-10" tabindex="-1"></a>                <span class="at">include.lowest =</span> <span class="cn">TRUE</span>, </span>
<span id="cb147-11"><a href="multiple-linear-regression.html#cb147-11" tabindex="-1"></a>                <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Q1&quot;</span>, <span class="st">&quot;Q2&quot;</span>, <span class="st">&quot;Q3&quot;</span>, <span class="st">&quot;Q4&quot;</span>))</span>
<span id="cb147-12"><a href="multiple-linear-regression.html#cb147-12" tabindex="-1"></a></span>
<span id="cb147-13"><a href="multiple-linear-regression.html#cb147-13" tabindex="-1"></a>d<span class="sc">$</span>X1_cat <span class="ot">&lt;-</span> <span class="fu">cut</span>(d<span class="sc">$</span>X1, </span>
<span id="cb147-14"><a href="multiple-linear-regression.html#cb147-14" tabindex="-1"></a>                <span class="at">breaks =</span> <span class="fu">quantile</span>(d<span class="sc">$</span>X1, <span class="at">probs =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>), <span class="at">na.rm =</span> <span class="cn">TRUE</span>), </span>
<span id="cb147-15"><a href="multiple-linear-regression.html#cb147-15" tabindex="-1"></a>                <span class="at">include.lowest =</span> <span class="cn">TRUE</span>, </span>
<span id="cb147-16"><a href="multiple-linear-regression.html#cb147-16" tabindex="-1"></a>                <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Q1&quot;</span>, <span class="st">&quot;Q2&quot;</span>, <span class="st">&quot;Q3&quot;</span>, <span class="st">&quot;Q4&quot;</span>))</span>
<span id="cb147-17"><a href="multiple-linear-regression.html#cb147-17" tabindex="-1"></a></span>
<span id="cb147-18"><a href="multiple-linear-regression.html#cb147-18" tabindex="-1"></a><span class="co"># Create the interaction plot</span></span>
<span id="cb147-19"><a href="multiple-linear-regression.html#cb147-19" tabindex="-1"></a><span class="fu">interaction.plot</span>(d<span class="sc">$</span>X2_cat, d<span class="sc">$</span>X1_cat, d<span class="sc">$</span>Y)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p>There seems to be an interaction of the predictors with respect to <span class="math inline">\(Y\)</span>. The lines are not parallel.
If there was no interaction, the change in <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X_2\)</span> would be the same
for all levels of <span class="math inline">\(X_1\)</span>.
This seems not to be the case here.
See <a href="multiple-linear-regression.html#exercise5_multiple_regression">exercise 5</a>.</p>
<p>If we had one or both predictors already categorical, we would not have to discretize them before.</p>
</div>
<div id="simpsons_paradox" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Simpsons Paradox<a href="multiple-linear-regression.html#simpsons_paradox" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Simpsons paradox</a> is a phenomenon,
in which a trend appears in several different groups of data but disappears
or reverses when these groups are combined. I agree with the criticism that this is not really
a paradox but a failure to consider confounding variables adequately. Let’s quickly invent an example.
We are interested in the relationship <em>hours of muscle training</em> and <em>strength</em> (not based on evidence)
in children vs. adults. Within both groups there will be an increasing relationship. The more training,
the more muscle strength. But if we combine the groups, we will see a decreasing relationship.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="multiple-linear-regression.html#cb148-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb148-2"><a href="multiple-linear-regression.html#cb148-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb148-3"><a href="multiple-linear-regression.html#cb148-3" tabindex="-1"></a>age <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;child&quot;</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="st">&quot;adult&quot;</span>, n<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb148-4"><a href="multiple-linear-regression.html#cb148-4" tabindex="-1"></a>training <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">5</span>) <span class="sc">+</span> <span class="dv">30</span>, <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">5</span>)<span class="sc">+</span> <span class="dv">10</span>)</span>
<span id="cb148-5"><a href="multiple-linear-regression.html#cb148-5" tabindex="-1"></a>strength <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb148-6"><a href="multiple-linear-regression.html#cb148-6" tabindex="-1"></a>  <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> training[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">/</span><span class="dv">2</span>)] <span class="sc">+</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>), <span class="co"># For children</span></span>
<span id="cb148-7"><a href="multiple-linear-regression.html#cb148-7" tabindex="-1"></a>  <span class="dv">25</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> training[(n<span class="sc">/</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>n] <span class="sc">+</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>) <span class="co"># For adults</span></span>
<span id="cb148-8"><a href="multiple-linear-regression.html#cb148-8" tabindex="-1"></a>)</span>
<span id="cb148-9"><a href="multiple-linear-regression.html#cb148-9" tabindex="-1"></a></span>
<span id="cb148-10"><a href="multiple-linear-regression.html#cb148-10" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">age =</span> age, <span class="at">training =</span> training, <span class="at">strength =</span> strength)</span>
<span id="cb148-11"><a href="multiple-linear-regression.html#cb148-11" tabindex="-1"></a></span>
<span id="cb148-12"><a href="multiple-linear-regression.html#cb148-12" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="at">x =</span> training, <span class="at">y =</span> strength, <span class="at">color =</span> age)) <span class="sc">+</span></span>
<span id="cb148-13"><a href="multiple-linear-regression.html#cb148-13" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb148-14"><a href="multiple-linear-regression.html#cb148-14" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> <span class="co"># Group-specific regression lines</span></span>
<span id="cb148-15"><a href="multiple-linear-regression.html#cb148-15" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">data =</span> d, <span class="fu">aes</span>(<span class="at">x =</span> training, <span class="at">y =</span> strength), </span>
<span id="cb148-16"><a href="multiple-linear-regression.html#cb148-16" tabindex="-1"></a>              <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span> <span class="co"># Overall regression line</span></span>
<span id="cb148-17"><a href="multiple-linear-regression.html#cb148-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Regression Lines for Training and Strength&quot;</span>,</span>
<span id="cb148-18"><a href="multiple-linear-regression.html#cb148-18" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Training&quot;</span>,</span>
<span id="cb148-19"><a href="multiple-linear-regression.html#cb148-19" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Strength&quot;</span>) <span class="sc">+</span></span>
<span id="cb148-20"><a href="multiple-linear-regression.html#cb148-20" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb148-21"><a href="multiple-linear-regression.html#cb148-21" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<ul>
<li><strong>Group-Specific Trends</strong>:
<ul>
<li>In the group of <strong>children</strong> (blue line), strength <strong>increases</strong> with training, as indicated by the positive slope of the regression line.</li>
<li>Similarly, in the group of <strong>adults</strong> (red line), strength also <strong>increases</strong> with training.</li>
</ul></li>
<li><strong>Overall Trend</strong>:
<ul>
<li>When both groups are combined and the categorical variable <em>age</em> is neglected,
the overall regression line (black, dashed) shows a <strong>negative slope</strong>, suggesting that <strong>strength decreases</strong> with training.</li>
<li>This overall trend is opposite to the trends observed within the individual groups.</li>
</ul></li>
<li><strong>Why Does This Happen?</strong>
<ul>
<li>This paradox occurs because the relationship between the grouping variable (<code>age</code>) and the independent variable (<code>training</code>) creates a confounding effect.</li>
<li>In this case: Children tend to have higher training values overall,
while adults tend to have lower training values. <em>Age</em> is associated with both <em>strength</em> and <em>training</em> and
is therefore a <a href="https://en.wikipedia.org/wiki/Confounding"><em>confounder</em></a> in the relationship between <em>training</em> and <em>strength</em>.
Not adjusting (=including it in the regression model as predictor) for <em>age</em> leads to a misleading association.</li>
</ul></li>
</ul>
<p>This (fictitious) example shows that throwing variables into a multiple regression
model without thinking about it, is not a good idea.</p>
</div>
</div>
<div id="throwing_variables" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> What happens when you just throw variables into multiple regression?<a href="multiple-linear-regression.html#throwing_variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This sub-chapter is <strong>important</strong>. I can guarantee you that not too many applied
scientists using regression models know about this.</p>
<p>A first taste of causality.</p>
<p>Richard McElreath has 3 cool examples on
<a href="https://github.com/rmcelreath/causal_salad_2021/blob/main/1_causal_salad.r">github</a>
that show what happens in the context of <em>explanation</em> if you include variables the wrong
way and explains this in a
<a href="https://www.youtube.com/watch?v=KNPYUVmY3NM&amp;ab_channel=RichardMcElreath">video</a>.</p>
<p>We will look at these below - <strong>the pipe, the fork and the collider</strong>. These are causal graphs showing the
relationships between the variables. One is interested in the <strong>effect</strong> of X on Y.
In this case, it is truly an <em>effect</em>, since we create the models in such a way that changing
one variable, changes the other - which is indicated by an arrow in the graph.
These graphs are called DAGs - directed acyclic graphs. <em>Directed</em> because of the arrows,
<em>acyclic</em> because there are no cycles in the graph. One nice tool for drawing them is
<a href="https://www.dagitty.net/">dagitty</a>, which is also
<a href="https://cran.r-project.org/web/packages/dagitty/index.html">implemented in R</a>.
The <a href="https://www.dagitty.net/dags.html">online drawing tool</a> is handy.</p>
<p>Note: Not only can it hurt to add variables to the model in an explanatory (causal) context,
but also in a predictive context. The model can become unstable and the predictions
can become worse. This is called <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p>
<div id="pipe" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Pipe<a href="multiple-linear-regression.html#pipe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this setting <span class="math inline">\(X\)</span> is associated with <span class="math inline">\(Z\)</span> and <span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(Y\)</span>.
<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not directly associated, but through <span class="math inline">\(Z\)</span> (see graph below).
If we condition on <span class="math inline">\(Z\)</span>, the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> <em>disappears</em>.
This means, if we <strong>know</strong> the value of <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span> does not give us any <em>additional</em> information about <span class="math inline">\(Y\)</span>.</p>
<p>This can be seen in the scatterplot: Once we are within <span class="math inline">\(Z=0\)</span> (black dots) or <span class="math inline">\(Z=1\)</span> (red dots),
<span class="math inline">\(X\)</span> does not give us any information about <span class="math inline">\(Y\)</span>,
i.e., the point cloud is horizontal and there is no correlation.</p>
<p>The <code>inv_logit</code> function is the inverse of the logit function.
It assigns higher probability of <span class="math inline">\(Z\)</span> being <span class="math inline">\(1\)</span> if X has a higher value.
Let’s plot this for understanding:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="multiple-linear-regression.html#cb150-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="fl">0.1</span>)</span>
<span id="cb150-2"><a href="multiple-linear-regression.html#cb150-2" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">inv_logit</span>(x), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;P(Z=1|X)&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>Higher <span class="math inline">\(X\)</span> values lead to higher probabilities of <span class="math inline">\(Z=1\)</span>.
As you can see, more red dots are on the right side of the scatterplot below.</p>
<p>Now to the pipe and a mini simulation for it:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="multiple-linear-regression.html#cb151-1" tabindex="-1"></a><span class="co"># pipe</span></span>
<span id="cb151-2"><a href="multiple-linear-regression.html#cb151-2" tabindex="-1"></a><span class="fu">library</span>(dagitty)</span>
<span id="cb151-3"><a href="multiple-linear-regression.html#cb151-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb151-4"><a href="multiple-linear-regression.html#cb151-4" tabindex="-1"></a><span class="fu">library</span>(ggdag)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;ggdag&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="multiple-linear-regression.html#cb154-1" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>( <span class="st">&#39;dag {</span></span>
<span id="cb154-2"><a href="multiple-linear-regression.html#cb154-2" tabindex="-1"></a><span class="st">  X -&gt; Z -&gt; Y</span></span>
<span id="cb154-3"><a href="multiple-linear-regression.html#cb154-3" tabindex="-1"></a><span class="st">}&#39;</span> )</span>
<span id="cb154-4"><a href="multiple-linear-regression.html#cb154-4" tabindex="-1"></a></span>
<span id="cb154-5"><a href="multiple-linear-regression.html#cb154-5" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>( dag ) <span class="ot">&lt;-</span></span>
<span id="cb154-6"><a href="multiple-linear-regression.html#cb154-6" tabindex="-1"></a>  <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">2</span>, <span class="at">Z=</span><span class="dv">1</span>),</span>
<span id="cb154-7"><a href="multiple-linear-regression.html#cb154-7" tabindex="-1"></a>        <span class="at">y=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">0</span>, <span class="at">Z=</span><span class="dv">0</span>) )</span>
<span id="cb154-8"><a href="multiple-linear-regression.html#cb154-8" tabindex="-1"></a></span>
<span id="cb154-9"><a href="multiple-linear-regression.html#cb154-9" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> </span>
<span id="cb154-10"><a href="multiple-linear-regression.html#cb154-10" tabindex="-1"></a>  <span class="fu">theme_dag</span>()</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="multiple-linear-regression.html#cb155-1" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb155-2"><a href="multiple-linear-regression.html#cb155-2" tabindex="-1"></a>cols <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="fu">col.alpha</span>(<span class="dv">1</span>,a) , <span class="fu">col.alpha</span>(<span class="dv">2</span>,a) )</span>
<span id="cb155-3"><a href="multiple-linear-regression.html#cb155-3" tabindex="-1"></a></span>
<span id="cb155-4"><a href="multiple-linear-regression.html#cb155-4" tabindex="-1"></a><span class="co"># pipe</span></span>
<span id="cb155-5"><a href="multiple-linear-regression.html#cb155-5" tabindex="-1"></a><span class="co"># X -&gt; Z -&gt; Y</span></span>
<span id="cb155-6"><a href="multiple-linear-regression.html#cb155-6" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb155-7"><a href="multiple-linear-regression.html#cb155-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb155-8"><a href="multiple-linear-regression.html#cb155-8" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N,<span class="fu">inv_logit</span>(X))</span>
<span id="cb155-9"><a href="multiple-linear-regression.html#cb155-9" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,(<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>))</span>
<span id="cb155-10"><a href="multiple-linear-regression.html#cb155-10" tabindex="-1"></a></span>
<span id="cb155-11"><a href="multiple-linear-regression.html#cb155-11" tabindex="-1"></a><span class="fu">plot</span>( X , Y , <span class="at">col=</span>cols[Z<span class="sc">+</span><span class="dv">1</span>] , <span class="at">pch=</span><span class="dv">16</span> )</span>
<span id="cb155-12"><a href="multiple-linear-regression.html#cb155-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">1</span>]),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb155-13"><a href="multiple-linear-regression.html#cb155-13" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">0</span>]),<span class="at">col=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb155-14"><a href="multiple-linear-regression.html#cb155-14" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X),<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-74-2.png" width="672" /></p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="multiple-linear-regression.html#cb156-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">1</span>],Y[Z<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.007540748</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="multiple-linear-regression.html#cb158-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">0</span>],Y[Z<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] 0.02629344</code></pre>
<p>Or in the framework of Simpsons paradox:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="multiple-linear-regression.html#cb160-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb160-2"><a href="multiple-linear-regression.html#cb160-2" tabindex="-1"></a></span>
<span id="cb160-3"><a href="multiple-linear-regression.html#cb160-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb160-4"><a href="multiple-linear-regression.html#cb160-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb160-5"><a href="multiple-linear-regression.html#cb160-5" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N,<span class="fu">inv_logit</span>(X))</span>
<span id="cb160-6"><a href="multiple-linear-regression.html#cb160-6" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,(<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>))</span>
<span id="cb160-7"><a href="multiple-linear-regression.html#cb160-7" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X) <span class="co"># without conditioning on Z</span></span>
<span id="cb160-8"><a href="multiple-linear-regression.html#cb160-8" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7717 -0.8634 -0.0037  0.9204  3.5437 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.00123    0.04210  -0.029    0.977    
## X            0.46041    0.04296  10.718   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.331 on 998 degrees of freedom
## Multiple R-squared:  0.1032, Adjusted R-squared:  0.1023 
## F-statistic: 114.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="multiple-linear-regression.html#cb162-1" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> Z) <span class="co"># with conditioning on Z</span></span>
<span id="cb162-2"><a href="multiple-linear-regression.html#cb162-2" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2979 -0.6399  0.0011  0.7047  2.8701 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.98781    0.04662 -21.190   &lt;2e-16 ***
## X            0.02121    0.03542   0.599    0.549    
## Z            1.97980    0.06941  28.523   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9883 on 997 degrees of freedom
## Multiple R-squared:  0.5062, Adjusted R-squared:  0.5052 
## F-statistic:   511 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Adding <span class="math inline">\(Z\)</span> to the model (i.e., conditioning on <span class="math inline">\(Z\)</span>) makes the coefficient for <span class="math inline">\(X\)</span> disappear.
Knowing <span class="math inline">\(Z\)</span> means that <span class="math inline">\(X\)</span> does not give us any additional information about <span class="math inline">\(Y\)</span>.
See <a href="multiple-linear-regression.html#exercise7_multiple_regression">exercise 7</a>.</p>
<p><span class="math inline">\(Z\)</span> is also called a <a href="https://en.wikipedia.org/wiki/Mediation_(statistics)"><strong>mediator</strong></a>.</p>
<p>We could easily verify that <span class="math inline">\(Z\)</span> constitutes a mediator by using Baron and
Kenny’s 1986 approach (see Wiki):</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="multiple-linear-regression.html#cb164-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X))<span class="co"># Step 1</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7717 -0.8634 -0.0037  0.9204  3.5437 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.00123    0.04210  -0.029    0.977    
## X            0.46041    0.04296  10.718   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.331 on 998 degrees of freedom
## Multiple R-squared:  0.1032, Adjusted R-squared:  0.1023 
## F-statistic: 114.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="multiple-linear-regression.html#cb166-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Z <span class="sc">~</span> X))<span class="co"># Step 2</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Z ~ X)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.97876 -0.40728 -0.00555  0.41502  1.09581 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.49832    0.01425   34.96   &lt;2e-16 ***
## X            0.22184    0.01454   15.25   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4507 on 998 degrees of freedom
## Multiple R-squared:  0.189,  Adjusted R-squared:  0.1882 
## F-statistic: 232.6 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="multiple-linear-regression.html#cb168-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> Z))<span class="co"># Step 3</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2979 -0.6399  0.0011  0.7047  2.8701 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.98781    0.04662 -21.190   &lt;2e-16 ***
## X            0.02121    0.03542   0.599    0.549    
## Z            1.97980    0.06941  28.523   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9883 on 997 degrees of freedom
## Multiple R-squared:  0.5062, Adjusted R-squared:  0.5052 
## F-statistic:   511 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Both coefficients for <span class="math inline">\(X\)</span> are “significant” in steps 1 and 2.
The coefficients for <span class="math inline">\(Z\)</span> in step 3 is “significant” and the coefficient for
<span class="math inline">\(X\)</span> is smaller compared to step 1. And not to forget: the very small <span class="math inline">\(p\)</span>-values are strongly
related to the very large smaple size and a strict cutoff (<span class="math inline">\(\alpha = 0.05\)</span>) makes no sense.</p>
<p><strong>Example</strong>: <em>Hours of studying</em> is associated with <em>exam performance</em>.
This observation makes sense. A mediator in this context: <em>understanding/knowledge</em>.
One could study for hours inefficiently with low levels of concentration
and achieve low exam performance or one could study for a comparatively
low number of hours and achieve good results. If we know that a person has gained the
<em>understanding/knowledge</em>, the <em>hours of studying</em> do not give us any additional
information with respect to <em>exam performance</em>.</p>
</div>
<div id="fork" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Fork<a href="multiple-linear-regression.html#fork" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In health science, this is the classical <a href="https://en.wikipedia.org/wiki/Confounding">confounder</a>.
<span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>actually</em> not associated (no arrow) but an association is
still shown: <code>cor(X,Y) = 0.5097437</code>.
But if we condition on <span class="math inline">\(Z\)</span>, the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> disappears. The assocation is
spurious in this case and <em>adjusting</em> for the confounder yields the correct result.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="multiple-linear-regression.html#cb170-1" tabindex="-1"></a><span class="co"># fork</span></span>
<span id="cb170-2"><a href="multiple-linear-regression.html#cb170-2" tabindex="-1"></a><span class="co"># X &lt;- Z -&gt; Y</span></span>
<span id="cb170-3"><a href="multiple-linear-regression.html#cb170-3" tabindex="-1"></a></span>
<span id="cb170-4"><a href="multiple-linear-regression.html#cb170-4" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>( <span class="st">&#39;dag {</span></span>
<span id="cb170-5"><a href="multiple-linear-regression.html#cb170-5" tabindex="-1"></a><span class="st">  X &lt;- Z -&gt; Y</span></span>
<span id="cb170-6"><a href="multiple-linear-regression.html#cb170-6" tabindex="-1"></a><span class="st">}&#39;</span> )</span>
<span id="cb170-7"><a href="multiple-linear-regression.html#cb170-7" tabindex="-1"></a></span>
<span id="cb170-8"><a href="multiple-linear-regression.html#cb170-8" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>( dag ) <span class="ot">&lt;-</span></span>
<span id="cb170-9"><a href="multiple-linear-regression.html#cb170-9" tabindex="-1"></a>  <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">2</span>, <span class="at">Z=</span><span class="dv">1</span>),</span>
<span id="cb170-10"><a href="multiple-linear-regression.html#cb170-10" tabindex="-1"></a>        <span class="at">y=</span><span class="fu">c</span>(<span class="at">X=</span><span class="fl">0.5</span>, <span class="at">Y=</span><span class="fl">0.5</span>, <span class="at">Z=</span><span class="dv">0</span>) )</span>
<span id="cb170-11"><a href="multiple-linear-regression.html#cb170-11" tabindex="-1"></a></span>
<span id="cb170-12"><a href="multiple-linear-regression.html#cb170-12" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> <span class="fu">theme_dag</span>()</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="multiple-linear-regression.html#cb171-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb171-2"><a href="multiple-linear-regression.html#cb171-2" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N)</span>
<span id="cb171-3"><a href="multiple-linear-regression.html#cb171-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>)</span>
<span id="cb171-4"><a href="multiple-linear-regression.html#cb171-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,(<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>))</span>
<span id="cb171-5"><a href="multiple-linear-regression.html#cb171-5" tabindex="-1"></a></span>
<span id="cb171-6"><a href="multiple-linear-regression.html#cb171-6" tabindex="-1"></a><span class="fu">plot</span>( X , Y , <span class="at">col=</span>cols[Z<span class="sc">+</span><span class="dv">1</span>] , <span class="at">pch=</span><span class="dv">16</span> )</span>
<span id="cb171-7"><a href="multiple-linear-regression.html#cb171-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">1</span>]),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb171-8"><a href="multiple-linear-regression.html#cb171-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">0</span>]),<span class="at">col=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb171-9"><a href="multiple-linear-regression.html#cb171-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X),<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-77-2.png" width="672" /></p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="multiple-linear-regression.html#cb172-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">1</span>],Y[Z<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.05903899</code></pre>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="multiple-linear-regression.html#cb174-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">0</span>],Y[Z<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] -0.03401368</code></pre>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="multiple-linear-regression.html#cb176-1" tabindex="-1"></a><span class="fu">cor</span>(X,Y)</span></code></pre></div>
<pre><code>## [1] 0.5097437</code></pre>
<p>In the example, we know that <span class="math inline">\(Z\)</span> is associated with both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (per construction).
As you can see in the definitions of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would not be associated
weren’t it for <span class="math inline">\(Z\)</span>.</p>
<p><strong>Example</strong>: <em>Carrying lighters</em> is (spuriously) associated with <em>lung cancer</em>. Obviously, <em>carrying a lighter</em>
does <em>not</em> cause lung cancer. <em>Carrying a lighter</em> is associated with <em>smoking</em>, since smokers need to light their
cigarrettes and how often do non-smokers carry a lighter just for fun?
<em>Smoking</em> is (causally) associated with <em>lung cancer</em>. If you just look at the association between
<em>carrying a ligher</em> and <em>lung cancer</em>, you would find an association. But if you condition on <em>smoking</em>, the association
disappears. See <a href="multiple-linear-regression.html#exercise13_multiple_regression">exercise 13</a>.</p>
</div>
<div id="collider" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Collider<a href="multiple-linear-regression.html#collider" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is rather interesting. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independently associated
with <span class="math inline">\(Z\)</span>. This can be seen in the toy example below, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are used
for the definition of <span class="math inline">\(Z\)</span>. <span class="math inline">\(Z\)</span> is defined as a <strong>sum
score</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Sum scores are <em>very</em> often used in health sciences (and others).
The higher the sum score, the higher the probability that <span class="math inline">\(Z=1\)</span>.
Now, <em>if</em> we know the value of <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are negatively associated (see graph).
The reason for this association is that there is a compensatory effect. In order to get a high
score, you can either have a high value of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> or both.
This induces the negative correlation.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="multiple-linear-regression.html#cb178-1" tabindex="-1"></a><span class="co"># collider</span></span>
<span id="cb178-2"><a href="multiple-linear-regression.html#cb178-2" tabindex="-1"></a><span class="co"># X -&gt; Z &lt;- Y</span></span>
<span id="cb178-3"><a href="multiple-linear-regression.html#cb178-3" tabindex="-1"></a></span>
<span id="cb178-4"><a href="multiple-linear-regression.html#cb178-4" tabindex="-1"></a><span class="co">#dag</span></span>
<span id="cb178-5"><a href="multiple-linear-regression.html#cb178-5" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>( <span class="st">&#39;dag {</span></span>
<span id="cb178-6"><a href="multiple-linear-regression.html#cb178-6" tabindex="-1"></a><span class="st">  X -&gt; Z &lt;- Y</span></span>
<span id="cb178-7"><a href="multiple-linear-regression.html#cb178-7" tabindex="-1"></a><span class="st">}&#39;</span> )</span>
<span id="cb178-8"><a href="multiple-linear-regression.html#cb178-8" tabindex="-1"></a></span>
<span id="cb178-9"><a href="multiple-linear-regression.html#cb178-9" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>( dag ) <span class="ot">&lt;-</span></span>
<span id="cb178-10"><a href="multiple-linear-regression.html#cb178-10" tabindex="-1"></a>  <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">2</span>, <span class="at">Z=</span><span class="dv">1</span>),</span>
<span id="cb178-11"><a href="multiple-linear-regression.html#cb178-11" tabindex="-1"></a>        <span class="at">y=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">0</span>, <span class="at">Z=</span><span class="dv">1</span>) )</span>
<span id="cb178-12"><a href="multiple-linear-regression.html#cb178-12" tabindex="-1"></a></span>
<span id="cb178-13"><a href="multiple-linear-regression.html#cb178-13" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> <span class="fu">theme_dag</span>()</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="multiple-linear-regression.html#cb179-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb179-2"><a href="multiple-linear-regression.html#cb179-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb179-3"><a href="multiple-linear-regression.html#cb179-3" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb179-4"><a href="multiple-linear-regression.html#cb179-4" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N,<span class="fu">inv_logit</span>(<span class="dv">2</span><span class="sc">*</span>X<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Y<span class="dv">-2</span>))</span>
<span id="cb179-5"><a href="multiple-linear-regression.html#cb179-5" tabindex="-1"></a></span>
<span id="cb179-6"><a href="multiple-linear-regression.html#cb179-6" tabindex="-1"></a><span class="fu">plot</span>( X , Y , <span class="at">col=</span>cols[Z<span class="sc">+</span><span class="dv">1</span>] , <span class="at">pch=</span><span class="dv">16</span> )</span>
<span id="cb179-7"><a href="multiple-linear-regression.html#cb179-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">1</span>]),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb179-8"><a href="multiple-linear-regression.html#cb179-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">0</span>]),<span class="at">col=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb179-9"><a href="multiple-linear-regression.html#cb179-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X),<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-78-2.png" width="672" /></p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="multiple-linear-regression.html#cb180-1" tabindex="-1"></a><span class="fu">cor</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>], X[Z<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] -0.326671</code></pre>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="multiple-linear-regression.html#cb182-1" tabindex="-1"></a><span class="fu">cor</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>], X[Z<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] -0.1768203</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="multiple-linear-regression.html#cb184-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X)) <span class="co"># mod1</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3892 -0.6079 -0.0269  0.6462  3.1038 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.01882    0.03055  -0.616    0.538
## X            0.04061    0.02948   1.378    0.169
## 
## Residual standard error: 0.9662 on 998 degrees of freedom
## Multiple R-squared:  0.001898,   Adjusted R-squared:  0.0008978 
## F-statistic: 1.898 on 1 and 998 DF,  p-value: 0.1686</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="multiple-linear-regression.html#cb186-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> Z)) <span class="co"># mod2</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.03484 -0.55761  0.02022  0.55813  2.41336 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.33787    0.03215  -10.51  &lt; 2e-16 ***
## X           -0.19963    0.02906   -6.87 1.13e-11 ***
## Z            1.21398    0.06840   17.75  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8427 on 997 degrees of freedom
## Multiple R-squared:  0.2415, Adjusted R-squared:   0.24 
## F-statistic: 158.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>If you think of Simpsons paradox again, conditioning on <span class="math inline">\(Z\)</span> <em>creates</em> an association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
which would otherwise be independent (by definition). So by learning the value of <span class="math inline">\(Z\)</span>, we can learn something
from <span class="math inline">\(X\)</span> about <span class="math inline">\(Y\)</span>. On the one hand, adding <span class="math inline">\(Z\)</span> to the model creates an association where there is none,
on the other hand, <em>prediction</em> of <span class="math inline">\(Y\)</span> is better with <span class="math inline">\(Z\)</span> in the model! In prediction, we largely
do not care about the causal structure. We just want to predict <span class="math inline">\(Y\)</span> as accurately as possible.</p>
<p>The sum score example is also called <a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox">Berkson’s paradox</a>.
Let’s check if this also works for three variables in a sum score
in <a href="multiple-linear-regression.html#exercise10_multiple_regression">exercise 10</a>.</p>
</div>
<div id="multicollinearity" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Multicollinearity<a href="multiple-linear-regression.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Westfall section 8.4 and McElreath section 6.1 cover this topic.
<a href="https://en.wikipedia.org/wiki/Multicollinearity">Multicollinearity</a> means that there is a strong
correlation between two or more predictors. It is <em>perfect</em> multicollinearity when the correlation is 1.
For example, if you accidentally include the same variable twice in the model, or when you include
the elements of a sum score and the sum score itself as predictors in the model. R just gives you
an <em>NA</em> if you do this in <code>lm</code>.</p>
<div id="mcElreath_6.1" class="section level4 hasAnchor" number="4.3.4.1">
<h4><span class="header-section-number">4.3.4.1</span> Example from McElreath 6.1<a href="multiple-linear-regression.html#mcElreath_6.1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The code creates body heights and leg lengths (left and right) for 100 people.</p>
<p><code>runif</code> draws a uniform random number between 40 and 50% of height as leg length, hence on average 45%.
The slope in the linear regression should therefore be around the average height divided by the average leg length:
<span class="math inline">\(\frac{10}{0.45 \cdot 10} \sim 2.2\)</span>.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="multiple-linear-regression.html#cb188-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb188-2"><a href="multiple-linear-regression.html#cb188-2" tabindex="-1"></a></span>
<span id="cb188-3"><a href="multiple-linear-regression.html#cb188-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb188-4"><a href="multiple-linear-regression.html#cb188-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">909</span>)</span>
<span id="cb188-5"><a href="multiple-linear-regression.html#cb188-5" tabindex="-1"></a>height <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb188-6"><a href="multiple-linear-regression.html#cb188-6" tabindex="-1"></a>leg_prop <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="fl">0.4</span>, <span class="fl">0.5</span>)</span>
<span id="cb188-7"><a href="multiple-linear-regression.html#cb188-7" tabindex="-1"></a>leg_left <span class="ot">&lt;-</span> leg_prop <span class="sc">*</span> height <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb188-8"><a href="multiple-linear-regression.html#cb188-8" tabindex="-1"></a>leg_right <span class="ot">&lt;-</span> leg_prop <span class="sc">*</span> height <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb188-9"><a href="multiple-linear-regression.html#cb188-9" tabindex="-1"></a></span>
<span id="cb188-10"><a href="multiple-linear-regression.html#cb188-10" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">height =</span> height, <span class="at">leg_left =</span> leg_left, <span class="at">leg_right =</span> leg_right)</span>
<span id="cb188-11"><a href="multiple-linear-regression.html#cb188-11" tabindex="-1"></a><span class="fu">cor</span>(d<span class="sc">$</span>leg_left, d<span class="sc">$</span>leg_right)</span></code></pre></div>
<pre><code>## [1] 0.9997458</code></pre>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="multiple-linear-regression.html#cb190-1" tabindex="-1"></a>m4<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb190-2"><a href="multiple-linear-regression.html#cb190-2" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb190-3"><a href="multiple-linear-regression.html#cb190-3" tabindex="-1"></a>    height <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb190-4"><a href="multiple-linear-regression.html#cb190-4" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> bl <span class="sc">*</span> leg_left <span class="sc">+</span> br <span class="sc">*</span> leg_right,</span>
<span id="cb190-5"><a href="multiple-linear-regression.html#cb190-5" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">10</span>, <span class="dv">100</span>),</span>
<span id="cb190-6"><a href="multiple-linear-regression.html#cb190-6" tabindex="-1"></a>    bl <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb190-7"><a href="multiple-linear-regression.html#cb190-7" tabindex="-1"></a>    br <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb190-8"><a href="multiple-linear-regression.html#cb190-8" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dexp</span>(<span class="dv">1</span>)</span>
<span id="cb190-9"><a href="multiple-linear-regression.html#cb190-9" tabindex="-1"></a>  ),    <span class="at">data =</span> d</span>
<span id="cb190-10"><a href="multiple-linear-regression.html#cb190-10" tabindex="-1"></a>)</span>
<span id="cb190-11"><a href="multiple-linear-regression.html#cb190-11" tabindex="-1"></a><span class="fu">precis</span>(m4<span class="fl">.6</span>)</span></code></pre></div>
<pre><code>##            mean         sd       5.5%     94.5%
## a     0.9811938 0.28396068  0.5273698 1.4350178
## bl    0.2138475 2.52707954 -3.8249137 4.2526087
## br    1.7817046 2.53129314 -2.2637907 5.8271999
## sigma 0.6171141 0.04343629  0.5476945 0.6865337</code></pre>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="multiple-linear-regression.html#cb192-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">precis</span>(m4<span class="fl">.6</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<p>With the given regression model, one asks the question: “What is the value of knowing each leg’s length, after
already knowing the other leg’s length?” The answer is: “Not much.”, since they are highly correlated.
Both coefficients are not around the expected <span class="math inline">\(\beta\)</span> and the credible intervals are wide and include the
credible value <span class="math inline">\(0\)</span>.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="multiple-linear-regression.html#cb193-1" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="fu">extract.samples</span>(m4<span class="fl">.6</span>)</span>
<span id="cb193-2"><a href="multiple-linear-regression.html#cb193-2" tabindex="-1"></a><span class="fu">plot</span>( bl <span class="sc">~</span> br, post, <span class="at">col=</span><span class="fu">col.alpha</span>(rangi2,<span class="fl">0.1</span>), <span class="at">pch =</span> <span class="dv">16</span> )</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
<p>Since the two coefficients are almost perfectly multicollinear
(<code>cor(d$leg_left, d$leg_right)=0.9997458</code>), knowing one leg’s length does not
give us any additional information about the other leg’s length.
Leaving out the right leg length would give the correct result (<a href="multiple-linear-regression.html#exercise11_multiple_regression">exercise 11</a>).</p>
</div>
<div id="example-from-westfall-8.4" class="section level4 hasAnchor" number="4.3.4.2">
<h4><span class="header-section-number">4.3.4.2</span> Example from Westfall 8.4<a href="multiple-linear-regression.html#example-from-westfall-8.4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the example below, the second prector <span class="math inline">\(X2\)</span> is a perfect linear function of the first predictor <span class="math inline">\(X1\)</span>.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="multiple-linear-regression.html#cb194-1" tabindex="-1"></a><span class="co"># Westfall 8.4.</span></span>
<span id="cb194-2"><a href="multiple-linear-regression.html#cb194-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb194-3"><a href="multiple-linear-regression.html#cb194-3" tabindex="-1"></a>X1 <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb194-4"><a href="multiple-linear-regression.html#cb194-4" tabindex="-1"></a>X2 <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>X1 <span class="sc">-</span><span class="dv">1</span>     <span class="co"># Perfect collinearity</span></span>
<span id="cb194-5"><a href="multiple-linear-regression.html#cb194-5" tabindex="-1"></a>Y <span class="ot">=</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>X1 <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>X2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb194-6"><a href="multiple-linear-regression.html#cb194-6" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X1<span class="sc">+</span>X2))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.20347 -0.60278 -0.01114  0.61898  2.60970 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.97795    0.10353  -19.11   &lt;2e-16 ***
## X1           8.09454    0.09114   88.82   &lt;2e-16 ***
## X2                NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.011 on 98 degrees of freedom
## Multiple R-squared:  0.9877, Adjusted R-squared:  0.9876 
## F-statistic:  7888 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Let’s look at a 3D plot:</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="multiple-linear-regression.html#cb196-1" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb196-2"><a href="multiple-linear-regression.html#cb196-2" tabindex="-1"></a></span>
<span id="cb196-3"><a href="multiple-linear-regression.html#cb196-3" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb196-4"><a href="multiple-linear-regression.html#cb196-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb196-5"><a href="multiple-linear-regression.html#cb196-5" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb196-6"><a href="multiple-linear-regression.html#cb196-6" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> X1 <span class="sc">-</span> <span class="dv">1</span>  <span class="co"># Perfect collinearity</span></span>
<span id="cb196-7"><a href="multiple-linear-regression.html#cb196-7" tabindex="-1"></a>Y  <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb196-8"><a href="multiple-linear-regression.html#cb196-8" tabindex="-1"></a></span>
<span id="cb196-9"><a href="multiple-linear-regression.html#cb196-9" tabindex="-1"></a><span class="co"># Create the 3D scatter plot with vertical lines</span></span>
<span id="cb196-10"><a href="multiple-linear-regression.html#cb196-10" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb196-11"><a href="multiple-linear-regression.html#cb196-11" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x =</span> X1, <span class="at">y =</span> X2, <span class="at">z =</span> Y, </span>
<span id="cb196-12"><a href="multiple-linear-regression.html#cb196-12" tabindex="-1"></a>              <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">color =</span> Y, <span class="at">colorscale =</span> <span class="st">&quot;Viridis&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>),</span>
<span id="cb196-13"><a href="multiple-linear-regression.html#cb196-13" tabindex="-1"></a>              <span class="at">name =</span> <span class="st">&quot;Data Points&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb196-14"><a href="multiple-linear-regression.html#cb196-14" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">title =</span> <span class="st">&quot;3D Scatter Plot&quot;</span>,</span>
<span id="cb196-15"><a href="multiple-linear-regression.html#cb196-15" tabindex="-1"></a>         <span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X1&quot;</span>),</span>
<span id="cb196-16"><a href="multiple-linear-regression.html#cb196-16" tabindex="-1"></a>                      <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb196-17"><a href="multiple-linear-regression.html#cb196-17" tabindex="-1"></a>                      <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;Y&quot;</span>)))</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-e316f399ada590f26fbe" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-e316f399ada590f26fbe">{"x":{"visdat":{"15b723ab6c39f":["function () ","plotlyVisDat"]},"cur_data":"15b723ab6c39f","attrs":{"15b723ab6c39f":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[1.3709584471466685,-0.56469817139608869,0.3631284113373392,0.63286260496104041,0.40426832314099903,-0.10612451609148403,1.5115219974389389,-0.094659038413097557,2.0184237138770418,-0.062714099052420993,1.3048696542234852,2.2866453927011068,-1.3888607011123393,-0.27878876681737136,-0.13332133639365804,0.63595039807007436,-0.28425292141607239,-2.6564554209047757,-2.4404669285755194,1.3201133457301921,-0.30663859407847455,-1.78130843398,-0.17191735575962136,1.2146746991725987,1.8951934612649652,-0.43046913160619965,-0.25726938276892963,-1.7631630851947802,0.4600973548312714,-0.63999487596011917,0.45545012324121936,0.70483733722881914,1.0351035219699223,-0.60892637540721106,0.50495512329797032,-1.7170086790733425,-0.784459008379496,-0.85090759417651829,-2.4142076499466318,0.036122606892255632,0.20599860020025385,-0.36105729854866631,0.75816323569951694,-0.72670482707657524,-1.3682810444192945,0.43281802588871715,-0.81139317618667162,1.4441012617212527,-0.43144620261334543,0.65564788340220681,0.32192526520394654,-0.78383894088037542,1.5757275197919773,0.64289930571731635,0.08976064659960567,0.27655074729146301,0.67928881605527081,0.0898328865790817,-2.9930900831529348,0.2848829535306594,-0.36723464274097534,0.18523056486560932,0.58182372736550669,1.3997368272926782,-0.72729205947446507,1.3025426320441438,0.33584811975207446,1.0385060986976211,0.92072856829064631,0.72087816286686246,-1.043118938567855,-0.090186386610706687,0.62351816199954357,-0.95352335777234398,-0.54282881457385668,0.58099649768168249,0.76817873783459101,0.4637675885401672,-0.88577629740967945,-1.0997808986478557,1.5127070098049282,0.25792143753203084,0.088440229159586353,-0.12089653753908947,-1.1943288951605282,0.61199689804038693,-0.21713984574652084,-0.18275670633192173,0.93334632857116007,0.82177311050824919,1.392116375934271,-0.47617392305467421,0.65034856072630509,1.3911104563900005,-1.1107888794478988,-0.8607925868778421,-1.1317386808537699,-1.4592139995023958,0.079982553241161172,0.65320433964919],"y":[1.741916894293337,-2.1293963427921776,-0.27374317732532161,0.26572520992208082,-0.19146335371800194,-1.212249032182968,2.0230439948778778,-1.1893180768261951,3.0368474277540836,-1.125428198104842,1.6097393084469704,3.5732907854022136,-3.7777214022246786,-1.5575775336347428,-1.2666426727873161,0.27190079614014873,-1.5685058428321448,-6.3129108418095514,-5.8809338571510388,1.6402266914603842,-1.6132771881569492,-4.5626168679600001,-1.3438347115192428,1.4293493983451975,2.7903869225299305,-1.8609382632123994,-1.5145387655378593,-4.5263261703895603,-0.079805290337457202,-2.2799897519202386,-0.089099753517561275,0.40967467445763828,1.0702070439398446,-2.2178527508144219,0.0099102465959406416,-4.4340173581466846,-2.568918016758992,-2.7018151883530366,-5.8284152998932637,-0.92775478621548868,-0.58800279959949231,-1.7221145970973326,0.51632647139903387,-2.4534096541531505,-3.7365620888385891,-0.1343639482225657,-2.6227863523733435,1.8882025234425055,-1.8628924052266909,0.31129576680441362,-0.35614946959210692,-2.5676778817607508,2.1514550395839547,0.2857986114346327,-0.8204787068007886,-0.44689850541707399,0.35857763211054161,-0.82033422684183654,-6.9861801663058696,-0.4302340929386812,-1.7344692854819508,-0.62953887026878141,0.16364745473101339,1.7994736545853565,-2.4545841189489304,1.6050852640882876,-0.32830376049585108,1.0770121973952422,0.84145713658129262,0.44175632573372492,-3.08623787713571,-1.1803727732214133,0.24703632399908715,-2.9070467155446877,-2.0856576291477134,0.16199299536336498,0.53635747566918202,-0.072464822919665606,-2.7715525948193589,-3.1995617972957113,2.0254140196098565,-0.48415712493593832,-0.82311954168082724,-1.2417930750781789,-3.3886577903210564,0.22399379608077385,-1.4342796914930416,-1.3655134126638435,0.86669265714232013,0.64354622101649839,1.7842327518685419,-1.9523478461093484,0.30069712145261018,1.7822209127800011,-3.2215777588957977,-2.7215851737556842,-3.2634773617075399,-3.9184279990047917,-0.84003489351767768,0.30640867929838],"z":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"type":"scatter3d","mode":"markers","marker":{"color":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"colorscale":"Viridis","size":5},"name":"Data Points","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"3D Scatter Plot","scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Y"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[1.3709584471466685,-0.56469817139608869,0.3631284113373392,0.63286260496104041,0.40426832314099903,-0.10612451609148403,1.5115219974389389,-0.094659038413097557,2.0184237138770418,-0.062714099052420993,1.3048696542234852,2.2866453927011068,-1.3888607011123393,-0.27878876681737136,-0.13332133639365804,0.63595039807007436,-0.28425292141607239,-2.6564554209047757,-2.4404669285755194,1.3201133457301921,-0.30663859407847455,-1.78130843398,-0.17191735575962136,1.2146746991725987,1.8951934612649652,-0.43046913160619965,-0.25726938276892963,-1.7631630851947802,0.4600973548312714,-0.63999487596011917,0.45545012324121936,0.70483733722881914,1.0351035219699223,-0.60892637540721106,0.50495512329797032,-1.7170086790733425,-0.784459008379496,-0.85090759417651829,-2.4142076499466318,0.036122606892255632,0.20599860020025385,-0.36105729854866631,0.75816323569951694,-0.72670482707657524,-1.3682810444192945,0.43281802588871715,-0.81139317618667162,1.4441012617212527,-0.43144620261334543,0.65564788340220681,0.32192526520394654,-0.78383894088037542,1.5757275197919773,0.64289930571731635,0.08976064659960567,0.27655074729146301,0.67928881605527081,0.0898328865790817,-2.9930900831529348,0.2848829535306594,-0.36723464274097534,0.18523056486560932,0.58182372736550669,1.3997368272926782,-0.72729205947446507,1.3025426320441438,0.33584811975207446,1.0385060986976211,0.92072856829064631,0.72087816286686246,-1.043118938567855,-0.090186386610706687,0.62351816199954357,-0.95352335777234398,-0.54282881457385668,0.58099649768168249,0.76817873783459101,0.4637675885401672,-0.88577629740967945,-1.0997808986478557,1.5127070098049282,0.25792143753203084,0.088440229159586353,-0.12089653753908947,-1.1943288951605282,0.61199689804038693,-0.21713984574652084,-0.18275670633192173,0.93334632857116007,0.82177311050824919,1.392116375934271,-0.47617392305467421,0.65034856072630509,1.3911104563900005,-1.1107888794478988,-0.8607925868778421,-1.1317386808537699,-1.4592139995023958,0.079982553241161172,0.65320433964919],"y":[1.741916894293337,-2.1293963427921776,-0.27374317732532161,0.26572520992208082,-0.19146335371800194,-1.212249032182968,2.0230439948778778,-1.1893180768261951,3.0368474277540836,-1.125428198104842,1.6097393084469704,3.5732907854022136,-3.7777214022246786,-1.5575775336347428,-1.2666426727873161,0.27190079614014873,-1.5685058428321448,-6.3129108418095514,-5.8809338571510388,1.6402266914603842,-1.6132771881569492,-4.5626168679600001,-1.3438347115192428,1.4293493983451975,2.7903869225299305,-1.8609382632123994,-1.5145387655378593,-4.5263261703895603,-0.079805290337457202,-2.2799897519202386,-0.089099753517561275,0.40967467445763828,1.0702070439398446,-2.2178527508144219,0.0099102465959406416,-4.4340173581466846,-2.568918016758992,-2.7018151883530366,-5.8284152998932637,-0.92775478621548868,-0.58800279959949231,-1.7221145970973326,0.51632647139903387,-2.4534096541531505,-3.7365620888385891,-0.1343639482225657,-2.6227863523733435,1.8882025234425055,-1.8628924052266909,0.31129576680441362,-0.35614946959210692,-2.5676778817607508,2.1514550395839547,0.2857986114346327,-0.8204787068007886,-0.44689850541707399,0.35857763211054161,-0.82033422684183654,-6.9861801663058696,-0.4302340929386812,-1.7344692854819508,-0.62953887026878141,0.16364745473101339,1.7994736545853565,-2.4545841189489304,1.6050852640882876,-0.32830376049585108,1.0770121973952422,0.84145713658129262,0.44175632573372492,-3.08623787713571,-1.1803727732214133,0.24703632399908715,-2.9070467155446877,-2.0856576291477134,0.16199299536336498,0.53635747566918202,-0.072464822919665606,-2.7715525948193589,-3.1995617972957113,2.0254140196098565,-0.48415712493593832,-0.82311954168082724,-1.2417930750781789,-3.3886577903210564,0.22399379608077385,-1.4342796914930416,-1.3655134126638435,0.86669265714232013,0.64354622101649839,1.7842327518685419,-1.9523478461093484,0.30069712145261018,1.7822209127800011,-3.2215777588957977,-2.7215851737556842,-3.2634773617075399,-3.9184279990047917,-0.84003489351767768,0.30640867929838],"z":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"type":"scatter3d","mode":"markers","marker":{"color":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"colorscale":"Viridis","size":5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Data Points","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="multiple-linear-regression.html#cb197-1" tabindex="-1"></a><span class="co">#VIF(lm(Y ~ X1 + X2)) # error</span></span>
<span id="cb197-2"><a href="multiple-linear-regression.html#cb197-2" tabindex="-1"></a><span class="co">#check_model(lm(Y ~ X1 + X2)) # error</span></span></code></pre></div>
<p>There is no unique solution for a plane in this case.
Infinitely many planes can be defined using the “line” in space.
The problem collapses into the simple linear regression problem.
One can just plug in the formula for <span class="math inline">\(X2\)</span> into the model, which yields the identical result:</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="multiple-linear-regression.html#cb198-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb198-2"><a href="multiple-linear-regression.html#cb198-2" tabindex="-1"></a>X1 <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb198-3"><a href="multiple-linear-regression.html#cb198-3" tabindex="-1"></a><span class="co"># Y = 1 + 2*X1 + 3*(2*X1 -1) + rnorm(100,0,1) = </span></span>
<span id="cb198-4"><a href="multiple-linear-regression.html#cb198-4" tabindex="-1"></a>Y <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">8</span><span class="sc">*</span>X1 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb198-5"><a href="multiple-linear-regression.html#cb198-5" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X1))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.20347 -0.60278 -0.01114  0.61898  2.60970 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.97795    0.10353  -19.11   &lt;2e-16 ***
## X1           8.09454    0.09114   88.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.011 on 98 degrees of freedom
## Multiple R-squared:  0.9877, Adjusted R-squared:  0.9876 
## F-statistic:  7888 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In reality, you only have one parameter (<span class="math inline">\(X1\)</span> or <span class="math inline">\(X2\)</span>) in the case of perfect multicollinearity.</p>
</div>
</div>
</div>
<div id="more-than-2-predictors" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> More than 2 predictors<a href="multiple-linear-regression.html#more-than-2-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In practice, you often want more than two predictors in the model.
If you listen closely to the research question raised by yourself or
your colleagues, often it won’t be clearly stated what goal you want to achieve
with the regression model:
<a href="https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full">prediction or explanation</a>.</p>
<p>If we want to be rigorous about the true relationships between variables,
we need to think about the causal structure of the variables,
as <a href="https://www.youtube.com/watch?v=YvhuYONl1o0&amp;ab_channel=KI-Campus">Richard McElreath</a>
argues.</p>
<p>In order not to just throw in variables into our regression model and hope for the best,
in order to have a fighting chance, we use the information provided
in Chapter 5 in the Statistical Rethinking book.</p>
<p><strong>Overall strategy for modeling:</strong></p>
<ul>
<li><p><strong>Have a research question</strong> and decide how this question can be answered:
Quantitatively (experiment or observational study) or
qualitatively.
Remember: Statistical modeling is not a catch-all approach to
turn data into truth.
And: We never want to play <em>find-the-right-statistical-test-Bingo</em>.
Statistical tests should make sense and not just thrown at data.
What does the current literature say about the topic?</p></li>
<li><p><strong>Define goal</strong>: Prediction or explanation?
<em>Prediction</em>: Use everything and every model type you can get your hands
on to “best” predict the outcome (neural nets, random forests, etc.).
<em>Explanation</em>: How do the variables relate to each other,
how do they influence each other? Temporal relationships are important.
And we want interpretable models.</p></li>
<li><p><strong>Do</strong> exploratory data analysis (<strong>EDA</strong>) to get a feeling for the data:</p>
<ul>
<li>This includes (among others) scatterplots, histograms, boxplots,
correlation matrices, etc.</li>
<li>How are the raw correlations/associations between the variables?</li>
<li>How are the variables distributed?</li>
<li>Are there outliers?</li>
<li>Are there missing values and why are the values missing?</li>
<li>Think about the data generating process. How did the data come about?</li>
<li>There is no shame in fitting exploratory models too.</li>
</ul></li>
<li><p><strong>Draw a DAG</strong> (directed acyclic graph) of the hypthesized relationships between
the variables, even though we will not do formal
<a href="https://miguelhernan.org/whatifbook">causal inference</a> in this lecture yet
(hopefully in the next).
The drawn DAG has testable implications (more below). See 5.1.2 in Rethinking.
Nicely enough, <code>dagitty</code>spits out:</p>
<ul>
<li>The <a href="https://rdrr.io/cran/dagitty/man/adjustmentSets.html">adjustment sets</a>
for the regression model based on a DAG. Which variables should I include
as covariates in my model, <em>if</em> the DAG is correct?</li>
<li>The <a href="https://search.r-project.org/CRAN/refmans/dagitty/html/impliedConditionalIndependencies.html">implied conditional dependencies</a>
coming from the DAG. These
can be checked with the data. It is not a proof that we have the true
relationships depicted by the DAG, but it is a good start.</li>
</ul></li>
<li><p><strong>Decide on a statistical model</strong> (more on that later).</p></li>
<li><p><strong>Define priors</strong> for the parameters of the model:
If we do not know much, we choose vague priors (i.e., wide range of plausible
parameter values). In the best case,
priors should be well argued for, especially in a <em>low-data</em> setting
(if we do not have many observations).</p></li>
<li><p><strong>Prior predictive checks</strong>: Does the model produce outcomes (<span class="math inline">\(Y\)</span>) that
are at all plausible? If not, we might have to rethink the model and/or priors.</p></li>
<li><p><strong>Fit and check the model</strong>:
If the models makes certain assumptions, check if these are met.
For the classical regression model, see Chapter 4 in Westfall.
Posterior predictive check: Check if the model produces new data that looks
like the observed data.</p></li>
<li><p><strong>Interpret and report</strong> the results.</p></li>
</ul>
<div id="example_nhanes" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Example in NHANES data<a href="multiple-linear-regression.html#example_nhanes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will now try to invent a not too exotic example with
<a href="https://www.cdc.gov/nchs/nhanes/index.html">NHANES</a> data.
The National Health and Nutrition Examination Survey (NHANES) is a large,
ongoing study conducted by the CDC to assess the health and nutritional status
of the U.S. population. It combines interviews, physical examinations, and
laboratory tests to collect data on demographics, diet, chronic diseases, and
physical activity. NHANES uses a complex, nationally representative sampling
design, making it a valuable resource for public health research and policy
development.</p>
<p>Open data is <strong>great</strong> and NHANES provides all the data to download for free
without any restrictions or even a registration. For convenience, Randall Pruim
created an <a href="https://cran.r-project.org/web/packages/NHANES/NHANES.pdf">R package</a>
called <code>NHANES</code> that contains a cleaned-up version of the
NHANES data from 2009-2012. We will use this and ignore the complex sampling
design for now.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="multiple-linear-regression.html#cb200-1" tabindex="-1"></a><span class="co">#install.packages(&quot;NHANES&quot;)</span></span>
<span id="cb200-2"><a href="multiple-linear-regression.html#cb200-2" tabindex="-1"></a><span class="fu">library</span>(pacman)</span>
<span id="cb200-3"><a href="multiple-linear-regression.html#cb200-3" tabindex="-1"></a><span class="fu">p_load</span>(NHANES, tidyverse)</span>
<span id="cb200-4"><a href="multiple-linear-regression.html#cb200-4" tabindex="-1"></a><span class="fu">data</span>(NHANES)</span>
<span id="cb200-5"><a href="multiple-linear-regression.html#cb200-5" tabindex="-1"></a><span class="fu">head</span>(NHANES)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 76
##      ID SurveyYr Gender   Age AgeDecade AgeMonths Race1 Race3 Education   
##   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;       
## 1 51624 2009_10  male      34 &quot; 30-39&quot;        409 White &lt;NA&gt;  High School 
## 2 51624 2009_10  male      34 &quot; 30-39&quot;        409 White &lt;NA&gt;  High School 
## 3 51624 2009_10  male      34 &quot; 30-39&quot;        409 White &lt;NA&gt;  High School 
## 4 51625 2009_10  male       4 &quot; 0-9&quot;           49 Other &lt;NA&gt;  &lt;NA&gt;        
## 5 51630 2009_10  female    49 &quot; 40-49&quot;        596 White &lt;NA&gt;  Some College
## 6 51638 2009_10  male       9 &quot; 0-9&quot;          115 White &lt;NA&gt;  &lt;NA&gt;        
## # ℹ 67 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;,
## #   Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;,
## #   Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;,
## #   BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;,
## #   BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;,
## #   BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;,
## #   TotChol &lt;dbl&gt;, UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, …</code></pre>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="multiple-linear-regression.html#cb202-1" tabindex="-1"></a><span class="fu">unique</span>(NHANES<span class="sc">$</span>SurveyYr)</span></code></pre></div>
<pre><code>## [1] 2009_10 2011_12
## Levels: 2009_10 2011_12</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="multiple-linear-regression.html#cb204-1" tabindex="-1"></a><span class="fu">colnames</span>(NHANES)</span></code></pre></div>
<pre><code>##  [1] &quot;ID&quot;               &quot;SurveyYr&quot;         &quot;Gender&quot;           &quot;Age&quot;             
##  [5] &quot;AgeDecade&quot;        &quot;AgeMonths&quot;        &quot;Race1&quot;            &quot;Race3&quot;           
##  [9] &quot;Education&quot;        &quot;MaritalStatus&quot;    &quot;HHIncome&quot;         &quot;HHIncomeMid&quot;     
## [13] &quot;Poverty&quot;          &quot;HomeRooms&quot;        &quot;HomeOwn&quot;          &quot;Work&quot;            
## [17] &quot;Weight&quot;           &quot;Length&quot;           &quot;HeadCirc&quot;         &quot;Height&quot;          
## [21] &quot;BMI&quot;              &quot;BMICatUnder20yrs&quot; &quot;BMI_WHO&quot;          &quot;Pulse&quot;           
## [25] &quot;BPSysAve&quot;         &quot;BPDiaAve&quot;         &quot;BPSys1&quot;           &quot;BPDia1&quot;          
## [29] &quot;BPSys2&quot;           &quot;BPDia2&quot;           &quot;BPSys3&quot;           &quot;BPDia3&quot;          
## [33] &quot;Testosterone&quot;     &quot;DirectChol&quot;       &quot;TotChol&quot;          &quot;UrineVol1&quot;       
## [37] &quot;UrineFlow1&quot;       &quot;UrineVol2&quot;        &quot;UrineFlow2&quot;       &quot;Diabetes&quot;        
## [41] &quot;DiabetesAge&quot;      &quot;HealthGen&quot;        &quot;DaysPhysHlthBad&quot;  &quot;DaysMentHlthBad&quot; 
## [45] &quot;LittleInterest&quot;   &quot;Depressed&quot;        &quot;nPregnancies&quot;     &quot;nBabies&quot;         
## [49] &quot;Age1stBaby&quot;       &quot;SleepHrsNight&quot;    &quot;SleepTrouble&quot;     &quot;PhysActive&quot;      
## [53] &quot;PhysActiveDays&quot;   &quot;TVHrsDay&quot;         &quot;CompHrsDay&quot;       &quot;TVHrsDayChild&quot;   
## [57] &quot;CompHrsDayChild&quot;  &quot;Alcohol12PlusYr&quot;  &quot;AlcoholDay&quot;       &quot;AlcoholYear&quot;     
## [61] &quot;SmokeNow&quot;         &quot;Smoke100&quot;         &quot;Smoke100n&quot;        &quot;SmokeAge&quot;        
## [65] &quot;Marijuana&quot;        &quot;AgeFirstMarij&quot;    &quot;RegularMarij&quot;     &quot;AgeRegMarij&quot;     
## [69] &quot;HardDrugs&quot;        &quot;SexEver&quot;          &quot;SexAge&quot;           &quot;SexNumPartnLife&quot; 
## [73] &quot;SexNumPartYear&quot;   &quot;SameSex&quot;          &quot;SexOrientation&quot;   &quot;PregnantNow&quot;</code></pre>
<p>The variable descriptions can be found
<a href="https://cran.r-project.org/web/packages/NHANES/NHANES.pdf">here</a>.
As an exercise, please do exploratory data analysis;
see <a href="multiple-linear-regression.html#exercise14_multiple_regression">exercise 14</a>.</p>
<div id="research-question" class="section level4 hasAnchor" number="4.4.1.1">
<h4><span class="header-section-number">4.4.1.1</span> Research question<a href="multiple-linear-regression.html#research-question" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Does <em>Physical activity</em> (PhysActive) <strong>influence</strong> the <em>average systolic
blood pressure</em> (BPSysAve) in adults (<span class="math inline">\(\ge 20\)</span> years)?</p>
<p>We propose the following relationships among the variables
(and limit ourselves to 4 covariates):</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="multiple-linear-regression.html#cb206-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> NHANES <span class="co"># shorter</span></span>
<span id="cb206-2"><a href="multiple-linear-regression.html#cb206-2" tabindex="-1"></a></span>
<span id="cb206-3"><a href="multiple-linear-regression.html#cb206-3" tabindex="-1"></a><span class="co"># Define the DAG</span></span>
<span id="cb206-4"><a href="multiple-linear-regression.html#cb206-4" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag {</span></span>
<span id="cb206-5"><a href="multiple-linear-regression.html#cb206-5" tabindex="-1"></a><span class="st">  PhysActive -&gt; BPSysAve</span></span>
<span id="cb206-6"><a href="multiple-linear-regression.html#cb206-6" tabindex="-1"></a><span class="st">  Age -&gt; PhysActive</span></span>
<span id="cb206-7"><a href="multiple-linear-regression.html#cb206-7" tabindex="-1"></a><span class="st">  Age -&gt; BPSysAve</span></span>
<span id="cb206-8"><a href="multiple-linear-regression.html#cb206-8" tabindex="-1"></a><span class="st">  PhysActive -&gt; BMI</span></span>
<span id="cb206-9"><a href="multiple-linear-regression.html#cb206-9" tabindex="-1"></a><span class="st">  BMI -&gt; BPSysAve</span></span>
<span id="cb206-10"><a href="multiple-linear-regression.html#cb206-10" tabindex="-1"></a><span class="st">  Gender -&gt; PhysActive</span></span>
<span id="cb206-11"><a href="multiple-linear-regression.html#cb206-11" tabindex="-1"></a><span class="st">  Gender -&gt; BMI</span></span>
<span id="cb206-12"><a href="multiple-linear-regression.html#cb206-12" tabindex="-1"></a><span class="st">  Gender -&gt; BPSysAve</span></span>
<span id="cb206-13"><a href="multiple-linear-regression.html#cb206-13" tabindex="-1"></a><span class="st">}&#39;</span>)</span>
<span id="cb206-14"><a href="multiple-linear-regression.html#cb206-14" tabindex="-1"></a></span>
<span id="cb206-15"><a href="multiple-linear-regression.html#cb206-15" tabindex="-1"></a><span class="co"># Set node coordinates for a nice layout</span></span>
<span id="cb206-16"><a href="multiple-linear-regression.html#cb206-16" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>(dag) <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb206-17"><a href="multiple-linear-regression.html#cb206-17" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="at">PhysActive =</span> <span class="dv">0</span>, <span class="at">BPSysAve =</span> <span class="dv">2</span>, <span class="at">Age =</span> <span class="dv">1</span>, <span class="at">BMI =</span> <span class="dv">1</span>, <span class="at">Gender =</span> <span class="fl">0.5</span>),</span>
<span id="cb206-18"><a href="multiple-linear-regression.html#cb206-18" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(<span class="at">PhysActive =</span> <span class="dv">1</span>, <span class="at">BPSysAve =</span> <span class="dv">1</span>, <span class="at">Age =</span> <span class="dv">2</span>, <span class="at">BMI =</span> <span class="fl">1.5</span>, <span class="at">Gender =</span> <span class="dv">2</span>)</span>
<span id="cb206-19"><a href="multiple-linear-regression.html#cb206-19" tabindex="-1"></a>)</span>
<span id="cb206-20"><a href="multiple-linear-regression.html#cb206-20" tabindex="-1"></a></span>
<span id="cb206-21"><a href="multiple-linear-regression.html#cb206-21" tabindex="-1"></a><span class="co"># Plot the DAG with larger node labels and bubbles</span></span>
<span id="cb206-22"><a href="multiple-linear-regression.html#cb206-22" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> </span>
<span id="cb206-23"><a href="multiple-linear-regression.html#cb206-23" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb206-24"><a href="multiple-linear-regression.html#cb206-24" tabindex="-1"></a>  <span class="fu">geom_dag_point</span>(<span class="at">size =</span> <span class="dv">20</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span>  <span class="co"># Increase node size</span></span>
<span id="cb206-25"><a href="multiple-linear-regression.html#cb206-25" tabindex="-1"></a>  <span class="fu">geom_dag_text</span>(<span class="at">size =</span> <span class="fl">2.5</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span>    <span class="co"># Increase label size</span></span>
<span id="cb206-26"><a href="multiple-linear-regression.html#cb206-26" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Hypothesized Relationships&quot;</span>) <span class="sc">+</span> </span>
<span id="cb206-27"><a href="multiple-linear-regression.html#cb206-27" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>This DAG illustrates the hypothesized relationships between physical activity,
age, BMI, gender, and blood pressure (BPSysAve). Physical activity potentially directly
influences blood pressure, as regular exercise is hypothesized to improve
cardiovascular health and lower blood pressure. Age has a direct effect on
blood pressure, as arterial stiffness and other age-related physiological changes
contribute to higher blood pressure over time. BMI also plays a role, with higher
BMI being associated with increased blood pressure due to greater vascular
resistance and metabolic factors. Additionally, gender affects blood pressure,
as men and women often have different baseline levels due to hormonal and
physiological differences.</p>
<p>There are also indirect pathways that contribute to these relationships.
Age influences both physical activity and BMI, as older individuals tend to
be less active and may experience weight gain due to metabolic changes (confounder).
BMI and physical activity are also interconnected, physical activity influences BMI,
further influencing blood
pressure (mediation). Gender plays a role in both BMI and physical activity,
as men and women tend to have different average BMI distributions
and physical activity levels due to both biological and societal factors (confounder).</p>
<p>Note, that we have a very large sample size (6,919 people). As exercise for later,
we randomly select 50 to 100 participants and repeat the analysis.
This would also give us the opportunity to study how well we can
infer the relationships in the larger sample from the smaller one,
which could be an eye-opener.</p>
<p><strong>Which variables should be included in the model?</strong></p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="multiple-linear-regression.html#cb207-1" tabindex="-1"></a><span class="fu">adjustmentSets</span>(dag, <span class="at">exposure =</span> <span class="st">&quot;PhysActive&quot;</span>, <span class="at">outcome =</span> <span class="st">&quot;BPSysAve&quot;</span>)</span></code></pre></div>
<pre><code>## { Age, Gender }</code></pre>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="multiple-linear-regression.html#cb209-1" tabindex="-1"></a><span class="fu">impliedConditionalIndependencies</span>(dag)</span></code></pre></div>
<pre><code>## Age _||_ BMI | Gndr, PhyA
## Age _||_ Gndr</code></pre>
<p>The one adjustment set (using the command <code>adjustmentSets</code>) proposed by <code>dagitty</code> tells us that
we should include age and gender as covariates in the model (in addition to <em>PhysActive</em>),
<em>if</em> the DAG is correct.
Note that <strong>we do not include all three covariables in the model here</strong>, just two of them.</p>
<p><em>If</em> the DAG is correct, this would imply some conditional independencies
(using the command <code>impliedConditionalIndependencies</code>) .
We should probably check them to see how this is done:</p>
<p>First, we need to test, if age and BMI are independent,
<em>if</em> we condition on gender and physical activity;
i.e., just add the to the regression model.
This is the case:</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="multiple-linear-regression.html#cb211-1" tabindex="-1"></a><span class="co"># Age _||_ BMI | Gnder, PhyA</span></span>
<span id="cb211-2"><a href="multiple-linear-regression.html#cb211-2" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Age <span class="sc">~</span> BMI <span class="sc">+</span> Gender <span class="sc">+</span> PhysActive, <span class="at">data =</span> df <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(Age <span class="sc">&gt;=</span> <span class="dv">20</span>)))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Age ~ BMI + Gender + PhysActive, data = df %&gt;% dplyr::filter(Age &gt;= 
##     20))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -31.53 -13.92  -0.97  12.23  36.67 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   49.31700    0.94791  52.027  &lt; 2e-16 ***
## BMI            0.04997    0.02982   1.676  0.09380 .  
## Gendermale    -1.18815    0.39311  -3.022  0.00252 ** 
## PhysActiveYes -5.83325    0.39761 -14.671  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 16.62 on 7168 degrees of freedom
##   (63 observations deleted due to missingness)
## Multiple R-squared:  0.03282,    Adjusted R-squared:  0.03242 
## F-statistic: 81.09 on 3 and 7168 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The implication can be confirmed.</p>
<p><strong>Interpretation</strong>: If we know gender and physical activity (yes/no) of a person,
BMI does not offer any <em>additional</em> information about age.</p>
<p>Second, I would go out on a limb and say that there is no relationship
between age and gender, at least not in a statistically relevant sense.</p>
</div>
<div id="goal" class="section level4 hasAnchor" number="4.4.1.2">
<h4><span class="header-section-number">4.4.1.2</span> Goal<a href="multiple-linear-regression.html#goal" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>goal</strong> is to better understand (explain) the relationship between
physical activity and blood pressure while considering age, BMI and gender.
I would suggest that in most cases relevant for us, one wants to understand
relationships rather to predict an outcome.</p>
</div>
<div id="eda" class="section level4 hasAnchor" number="4.4.1.3">
<h4><span class="header-section-number">4.4.1.3</span> EDA<a href="multiple-linear-regression.html#eda" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="multiple-linear-regression.html#exercise14_multiple_regression">Exercise 14</a></p>
</div>
<div id="statistical-model" class="section level4 hasAnchor" number="4.4.1.4">
<h4><span class="header-section-number">4.4.1.4</span> Statistical model<a href="multiple-linear-regression.html#statistical-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The outcome is a continuous variable (blood pressure), the exposure is a binary
variable (physical activity yes/no).
Surprise, surprise, we could try a <strong>multiple linear regression model</strong> here.</p>
<p>As a first approximation, let’s assume <em>BPSysAve</em> is (sufficiently) normally distributed,
which can be critized. Let’s quickly visualize the distribution of<em>BPSysAve</em> and overlay
a normal density with paramaters estimated from the data (<span class="math inline">\(\mu\)</span> sample mean of <em>BPSysAve</em>,
<span class="math inline">\(\sigma\)</span> sample standard deviation of <em>BPSysAve</em>):</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="multiple-linear-regression.html#cb213-1" tabindex="-1"></a>df_age <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(Age <span class="sc">&gt;=</span> <span class="dv">20</span>)</span>
<span id="cb213-2"><a href="multiple-linear-regression.html#cb213-2" tabindex="-1"></a>df_age <span class="sc">%&gt;%</span></span>
<span id="cb213-3"><a href="multiple-linear-regression.html#cb213-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> BPSysAve)) <span class="sc">+</span></span>
<span id="cb213-4"><a href="multiple-linear-regression.html#cb213-4" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), </span>
<span id="cb213-5"><a href="multiple-linear-regression.html#cb213-5" tabindex="-1"></a>  <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span>  </span>
<span id="cb213-6"><a href="multiple-linear-regression.html#cb213-6" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span>  </span>
<span id="cb213-7"><a href="multiple-linear-regression.html#cb213-7" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb213-8"><a href="multiple-linear-regression.html#cb213-8" tabindex="-1"></a>    <span class="at">fun =</span> dnorm, </span>
<span id="cb213-9"><a href="multiple-linear-regression.html#cb213-9" tabindex="-1"></a>    <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="fu">mean</span>(df_age<span class="sc">$</span>BPSysAve, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), </span>
<span id="cb213-10"><a href="multiple-linear-regression.html#cb213-10" tabindex="-1"></a>    <span class="at">sd =</span> <span class="fu">sd</span>(df_age<span class="sc">$</span>BPSysAve, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)), </span>
<span id="cb213-11"><a href="multiple-linear-regression.html#cb213-11" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span></span>
<span id="cb213-12"><a href="multiple-linear-regression.html#cb213-12" tabindex="-1"></a>  ) <span class="sc">+</span>  <span class="co"># Theoretical normal curve</span></span>
<span id="cb213-13"><a href="multiple-linear-regression.html#cb213-13" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb213-14"><a href="multiple-linear-regression.html#cb213-14" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Systolic Blood Pressure (BPSysAve)&quot;</span>, </span>
<span id="cb213-15"><a href="multiple-linear-regression.html#cb213-15" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>, </span>
<span id="cb213-16"><a href="multiple-linear-regression.html#cb213-16" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Distribution of Systolic Blood Pressure with Normal Curve&quot;</span></span>
<span id="cb213-17"><a href="multiple-linear-regression.html#cb213-17" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb213-18"><a href="multiple-linear-regression.html#cb213-18" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb213-19"><a href="multiple-linear-regression.html#cb213-19" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-88-1.png" width="672" />
Not quite normal, but we will assume it for now.</p>
</div>
<div id="priors-1" class="section level4 hasAnchor" number="4.4.1.5">
<h4><span class="header-section-number">4.4.1.5</span> Priors<a href="multiple-linear-regression.html#priors-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now, let’s write down the statistical model.
We choose vague priors since we have a vast data set.
The advantage of the Bayesian approach remains (that we have a fully
probabilistic model). Admitted, using Bayes is much more technical, at least
at first glance.</p>
<p>We center age again, which makes the interpretation of the intercept easier.</p>
<ul>
<li><p>The intercept (<span class="math inline">\(\beta_0\)</span>) is the expected value of <em>BPSysAve</em> for a person
of average age, not physically active (reference level of the factor) and
female gender (reference level of the factor).</p></li>
<li><p>The coefficient for <em>PhysActive</em> (<span class="math inline">\(\beta_1\)</span>) is the expected difference in
<em>BPSysAve</em> between a physically active person and a non-physically active person,
holding all other predictors constant. <em>BPSysAve</em> ranges from 78 to 226 mmHg.
If we do not know enough, we can just take a very vague prior for now:
<span class="math inline">\(\beta_1 \sim \text{Normal}(0, 50)\)</span>. With 95% probability, the effect of
physical activity on <em>BPSysAve</em> is between -100 and 100.</p></li>
<li><p><span class="math inline">\(beta_2\)</span>: The expected change in <em>BPSysAve</em> for a one-year increase in age
(above the mean age), holding all other predictors constant.
<span class="math inline">\(\beta_2 \sim \text{Normal}(0, 10)\)</span>. This effect should be comparatively small,
since we are dealing with a one-year increase in age. We could further
standardize age, then the effect would be in relation to one standard
deviations increase in age.</p></li>
<li><p><span class="math inline">\(\sigma\)</span>: The standard deviation of the residuals.
<span class="math inline">\(\sigma \sim \text{Uniform}(0, 50)\)</span>. We use a vague prior.
95% of the residuals are between -100 and 100. This seems plausible enough.</p></li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
BPSysAve_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma)\\
\mu_i &amp;=&amp; \beta_0 + \beta_1 \cdot PhysActive + \beta_2 \cdot Age_{centered} + \beta_3 \cdot Gender\\
\beta_0 &amp;\sim&amp; \text{Normal}(140, 20)\\
\beta_1 &amp;\sim&amp; \text{Normal}(0, 50)\\
\beta_2 &amp;\sim&amp; \text{Normal}(0, 10)\\
\beta_3 &amp;\sim&amp; \text{Normal}(0, 10)\\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<p>Note that age and gender are not influenced by each other.</p>
</div>
<div id="prior_predictive_checks_NHANES" class="section level4 hasAnchor" number="4.4.1.6">
<h4><span class="header-section-number">4.4.1.6</span> Prior Predictive Checks<a href="multiple-linear-regression.html#prior_predictive_checks_NHANES" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now we simply draw from the priors and thereby generate systolic blood pressure values.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="multiple-linear-regression.html#cb214-1" tabindex="-1"></a><span class="co"># Prior predictive checks</span></span>
<span id="cb214-2"><a href="multiple-linear-regression.html#cb214-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb214-3"><a href="multiple-linear-regression.html#cb214-3" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="fu">dim</span>(df_age)[<span class="dv">1</span>] <span class="co"># 7235 participants</span></span>
<span id="cb214-4"><a href="multiple-linear-regression.html#cb214-4" tabindex="-1"></a>beta_0_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">140</span>, <span class="dv">20</span>)</span>
<span id="cb214-5"><a href="multiple-linear-regression.html#cb214-5" tabindex="-1"></a>beta_1_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb214-6"><a href="multiple-linear-regression.html#cb214-6" tabindex="-1"></a>beta_2_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb214-7"><a href="multiple-linear-regression.html#cb214-7" tabindex="-1"></a>beta_3_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb214-8"><a href="multiple-linear-regression.html#cb214-8" tabindex="-1"></a>sigma_vec <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_sims, <span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb214-9"><a href="multiple-linear-regression.html#cb214-9" tabindex="-1"></a>BPSysAve_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, beta_0_vec <span class="sc">+</span> beta_1_vec <span class="sc">+</span> beta_2_vec <span class="sc">+</span> beta_3_vec, sigma_vec)</span></code></pre></div>
<p>This yields also negative values (please verify) for* BPSysAve*, which is not plausible.</p>
<p>In <a href="multiple-linear-regression.html#exercise15_multiple_regression">exercise 15</a> we will play around with the priors
until the prior predictive check producues plausible values.
<strong>Admitted</strong>, one should probably not look at the distribution of data while doing this.
On the other hand, we could have just Googled the distribution of blood pressure values.
This is the result:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="multiple-linear-regression.html#cb215-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb215-2"><a href="multiple-linear-regression.html#cb215-2" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="fu">dim</span>(df_age)[<span class="dv">1</span>] <span class="co"># 7235</span></span>
<span id="cb215-3"><a href="multiple-linear-regression.html#cb215-3" tabindex="-1"></a>beta_0_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">120</span>, <span class="dv">7</span>)</span>
<span id="cb215-4"><a href="multiple-linear-regression.html#cb215-4" tabindex="-1"></a>beta_1_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb215-5"><a href="multiple-linear-regression.html#cb215-5" tabindex="-1"></a>beta_2_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">5</span>, <span class="dv">10</span>)</span>
<span id="cb215-6"><a href="multiple-linear-regression.html#cb215-6" tabindex="-1"></a>beta_3_vec <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb215-7"><a href="multiple-linear-regression.html#cb215-7" tabindex="-1"></a>sigma_vec <span class="ot">&lt;-</span> <span class="fu">runif</span>(n_sims, <span class="dv">0</span>, <span class="dv">20</span>)</span>
<span id="cb215-8"><a href="multiple-linear-regression.html#cb215-8" tabindex="-1"></a>BPSysAve_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_sims, beta_0_vec <span class="sc">+</span> beta_1_vec <span class="sc">+</span> beta_2_vec <span class="sc">+</span> beta_3_vec, sigma_vec)</span>
<span id="cb215-9"><a href="multiple-linear-regression.html#cb215-9" tabindex="-1"></a><span class="fu">length</span>(BPSysAve_sim) <span class="co"># 7235</span></span></code></pre></div>
<pre><code>## [1] 7235</code></pre>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="multiple-linear-regression.html#cb217-1" tabindex="-1"></a>df_sim_vs_obs <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb217-2"><a href="multiple-linear-regression.html#cb217-2" tabindex="-1"></a>  <span class="at">BPSysAve_sim =</span> BPSysAve_sim,</span>
<span id="cb217-3"><a href="multiple-linear-regression.html#cb217-3" tabindex="-1"></a>  <span class="at">BPSysAve_obs =</span> df_age<span class="sc">$</span>BPSysAve</span>
<span id="cb217-4"><a href="multiple-linear-regression.html#cb217-4" tabindex="-1"></a>)</span>
<span id="cb217-5"><a href="multiple-linear-regression.html#cb217-5" tabindex="-1"></a></span>
<span id="cb217-6"><a href="multiple-linear-regression.html#cb217-6" tabindex="-1"></a><span class="co"># Combine observed and simulated values into one long-format data frame</span></span>
<span id="cb217-7"><a href="multiple-linear-regression.html#cb217-7" tabindex="-1"></a>df_long <span class="ot">&lt;-</span> df_sim_vs_obs <span class="sc">%&gt;%</span></span>
<span id="cb217-8"><a href="multiple-linear-regression.html#cb217-8" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">everything</span>(), </span>
<span id="cb217-9"><a href="multiple-linear-regression.html#cb217-9" tabindex="-1"></a>  <span class="at">names_to =</span> <span class="st">&quot;Type&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;BPSysAve&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb217-10"><a href="multiple-linear-regression.html#cb217-10" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Type =</span> <span class="fu">factor</span>(Type, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;BPSysAve_obs&quot;</span>, <span class="st">&quot;BPSysAve_sim&quot;</span>), </span>
<span id="cb217-11"><a href="multiple-linear-regression.html#cb217-11" tabindex="-1"></a>  <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Observed&quot;</span>, <span class="st">&quot;Simulated&quot;</span>)))</span>
<span id="cb217-12"><a href="multiple-linear-regression.html#cb217-12" tabindex="-1"></a></span>
<span id="cb217-13"><a href="multiple-linear-regression.html#cb217-13" tabindex="-1"></a><span class="co"># Plot densities of both observed and simulated values</span></span>
<span id="cb217-14"><a href="multiple-linear-regression.html#cb217-14" tabindex="-1"></a><span class="fu">ggplot</span>(df_long, <span class="fu">aes</span>(<span class="at">x =</span> BPSysAve, <span class="at">fill =</span> Type)) <span class="sc">+</span></span>
<span id="cb217-15"><a href="multiple-linear-regression.html#cb217-15" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span>  <span class="co"># Semi-transparent density curves</span></span>
<span id="cb217-16"><a href="multiple-linear-regression.html#cb217-16" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb217-17"><a href="multiple-linear-regression.html#cb217-17" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Systolic Blood Pressure&quot;</span>, </span>
<span id="cb217-18"><a href="multiple-linear-regression.html#cb217-18" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>, </span>
<span id="cb217-19"><a href="multiple-linear-regression.html#cb217-19" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Observed vs. Simulated Systolic Blood Pressure&quot;</span></span>
<span id="cb217-20"><a href="multiple-linear-regression.html#cb217-20" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb217-21"><a href="multiple-linear-regression.html#cb217-21" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Observed&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;Simulated&quot;</span> <span class="ot">=</span> <span class="st">&quot;red&quot;</span>)) <span class="sc">+</span> </span>
<span id="cb217-22"><a href="multiple-linear-regression.html#cb217-22" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb217-23"><a href="multiple-linear-regression.html#cb217-23" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<pre><code>## Warning: Removed 264 rows containing non-finite outside the scale range
## (`stat_density()`).</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<p>Looks decent. Not quite there yet.
The blood pressure values have a smaller variances and are skewed.
Let’s nevertheless fit the model.</p>
</div>
<div id="fit-and-check-model" class="section level4 hasAnchor" number="4.4.1.7">
<h4><span class="header-section-number">4.4.1.7</span> Fit and check model<a href="multiple-linear-regression.html#fit-and-check-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We use <code>quap</code> for the model fitting.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="multiple-linear-regression.html#cb219-1" tabindex="-1"></a><span class="fu">library</span>(data.table)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;data.table&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:lubridate&#39;:
## 
##     hour, isoweek, mday, minute, month, quarter, second, wday, week,
##     yday, year</code></pre>
<pre><code>## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     between, first, last</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     transpose</code></pre>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="multiple-linear-regression.html#cb224-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> NHANES</span>
<span id="cb224-2"><a href="multiple-linear-regression.html#cb224-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.table</span>(df)</span>
<span id="cb224-3"><a href="multiple-linear-regression.html#cb224-3" tabindex="-1"></a></span>
<span id="cb224-4"><a href="multiple-linear-regression.html#cb224-4" tabindex="-1"></a><span class="co"># Compute age mean for centering</span></span>
<span id="cb224-5"><a href="multiple-linear-regression.html#cb224-5" tabindex="-1"></a>Age_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(df[Age <span class="sc">&gt;=</span> <span class="dv">20</span>,]<span class="sc">$</span>Age, <span class="at">na.rm =</span> <span class="cn">TRUE</span>) <span class="co"># adults</span></span>
<span id="cb224-6"><a href="multiple-linear-regression.html#cb224-6" tabindex="-1"></a></span>
<span id="cb224-7"><a href="multiple-linear-regression.html#cb224-7" tabindex="-1"></a><span class="co"># Filter age</span></span>
<span id="cb224-8"><a href="multiple-linear-regression.html#cb224-8" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(Age <span class="sc">&gt;=</span> <span class="dv">20</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb224-9"><a href="multiple-linear-regression.html#cb224-9" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(BPSysAve, PhysActive, Age, Gender, BMI) <span class="sc">%&gt;%</span></span>
<span id="cb224-10"><a href="multiple-linear-regression.html#cb224-10" tabindex="-1"></a>  <span class="fu">drop_na</span>()</span>
<span id="cb224-11"><a href="multiple-linear-regression.html#cb224-11" tabindex="-1"></a><span class="fu">dim</span>(df) <span class="co"># 6919    7</span></span></code></pre></div>
<pre><code>## [1] 6919    5</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="multiple-linear-regression.html#cb226-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(df)) <span class="co"># 0</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="multiple-linear-regression.html#cb228-1" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb228-2"><a href="multiple-linear-regression.html#cb228-2" tabindex="-1"></a>m_NHANES <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb228-3"><a href="multiple-linear-regression.html#cb228-3" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb228-4"><a href="multiple-linear-regression.html#cb228-4" tabindex="-1"></a>    BPSysAve <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma), </span>
<span id="cb228-5"><a href="multiple-linear-regression.html#cb228-5" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1[PhysActive] <span class="sc">+</span> beta_2 <span class="sc">*</span> (Age <span class="sc">-</span> Age_mean) <span class="sc">+</span> beta_3[Gender],</span>
<span id="cb228-6"><a href="multiple-linear-regression.html#cb228-6" tabindex="-1"></a>    beta_0 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">120</span>, <span class="dv">7</span>),</span>
<span id="cb228-7"><a href="multiple-linear-regression.html#cb228-7" tabindex="-1"></a>    beta_1[PhysActive] <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),  </span>
<span id="cb228-8"><a href="multiple-linear-regression.html#cb228-8" tabindex="-1"></a>    beta_2 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">5</span>, <span class="dv">10</span>),</span>
<span id="cb228-9"><a href="multiple-linear-regression.html#cb228-9" tabindex="-1"></a>    beta_3[Gender] <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),  </span>
<span id="cb228-10"><a href="multiple-linear-regression.html#cb228-10" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">20</span>) </span>
<span id="cb228-11"><a href="multiple-linear-regression.html#cb228-11" tabindex="-1"></a>  ),</span>
<span id="cb228-12"><a href="multiple-linear-regression.html#cb228-12" tabindex="-1"></a>  <span class="at">data =</span> df</span>
<span id="cb228-13"><a href="multiple-linear-regression.html#cb228-13" tabindex="-1"></a>)</span>
<span id="cb228-14"><a href="multiple-linear-regression.html#cb228-14" tabindex="-1"></a></span>
<span id="cb228-15"><a href="multiple-linear-regression.html#cb228-15" tabindex="-1"></a><span class="fu">precis</span>(m_NHANES, <span class="at">depth =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                  mean         sd        5.5%       94.5%
## beta_0    120.4849312 5.73478808 111.3196322 129.6502302
## beta_1[1]   0.4483506 5.76628806  -8.7672915   9.6639926
## beta_1[2]  -0.2878748 5.76604721  -9.5031319   8.9273823
## beta_2      0.4194979 0.01115025   0.4016776   0.4373181
## beta_3[1]  -1.7776671 5.76694920 -10.9943658   7.4390315
## beta_3[2]   2.7074118 5.76700142  -6.5093703  11.9241939
## sigma      15.4150610 0.13103997  15.2056338  15.6244882</code></pre>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="multiple-linear-regression.html#cb230-1" tabindex="-1"></a><span class="co"># We want to know the expected difference for the levels of PhysActive and Gender:</span></span>
<span id="cb230-2"><a href="multiple-linear-regression.html#cb230-2" tabindex="-1"></a><span class="co"># (which is not directly visible in the summary)</span></span>
<span id="cb230-3"><a href="multiple-linear-regression.html#cb230-3" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="fu">extract.samples</span>(m_NHANES)</span>
<span id="cb230-4"><a href="multiple-linear-regression.html#cb230-4" tabindex="-1"></a>post<span class="sc">$</span>diff_PhysActive <span class="ot">&lt;-</span> post<span class="sc">$</span>beta_1[,<span class="dv">2</span>] <span class="sc">-</span> post<span class="sc">$</span>beta_1[,<span class="dv">1</span>]</span>
<span id="cb230-5"><a href="multiple-linear-regression.html#cb230-5" tabindex="-1"></a>post<span class="sc">$</span>diff_G <span class="ot">&lt;-</span> post<span class="sc">$</span>beta_3[,<span class="dv">2</span>] <span class="sc">-</span> post<span class="sc">$</span>beta_3[,<span class="dv">1</span>]</span>
<span id="cb230-6"><a href="multiple-linear-regression.html#cb230-6" tabindex="-1"></a></span>
<span id="cb230-7"><a href="multiple-linear-regression.html#cb230-7" tabindex="-1"></a><span class="fu">library</span>(conflicted)</span>
<span id="cb230-8"><a href="multiple-linear-regression.html#cb230-8" tabindex="-1"></a><span class="fu">conflicts_prefer</span>(posterior<span class="sc">::</span>sd)</span></code></pre></div>
<pre><code>## [conflicted] Will prefer posterior::sd over any other package.</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="multiple-linear-regression.html#cb232-1" tabindex="-1"></a><span class="fu">precis</span>(post, <span class="at">depth =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                        mean         sd        5.5%       94.5%   histogram
## beta_0          120.5532300 5.79551096 111.3020105 129.8048366   ▁▁▃▇▇▃▁▁▁
## beta_2            0.4196907 0.01127082   0.4016948   0.4374156 ▁▁▁▁▃▇▇▃▁▁▁
## sigma            15.4161590 0.13110866  15.2076867  15.6279605  ▁▁▁▃▇▇▅▂▁▁
## beta_1[1]         0.3487591 5.80968339  -8.9591156   9.5907618  ▁▁▁▃▇▇▃▁▁▁
## beta_1[2]        -0.3866548 5.81170379  -9.7471463   8.8316285  ▁▁▁▃▇▇▃▁▁▁
## beta_3[1]        -1.7463108 5.77133890 -11.1387559   7.3752075  ▁▁▂▅▇▅▂▁▁▁
## beta_3[2]         2.7422693 5.77545948  -6.6404321  11.8947750   ▁▁▂▅▇▅▂▁▁
## diff_PhysActive  -0.7354140 0.37500248  -1.3324591  -0.1257030     ▁▁▃▇▃▁▁
## diff_G            4.4885801 0.37263071   3.8813342   5.0880217      ▁▂▇▇▁▁</code></pre>
<p>The summary tells us that <code>diff_PhysActive</code> is <span class="math inline">\(-0.74\)</span> with a large proportion
of the posterior <span class="math inline">\(&lt;0\)</span>. Please research if this is a clinically relevant difference.
My guess is: no.</p>
</div>
<div id="fit-model-using-lm-frequentist-approach" class="section level4 hasAnchor" number="4.4.1.8">
<h4><span class="header-section-number">4.4.1.8</span> Fit model using <code>lm</code> (Frequentist approach)<a href="multiple-linear-regression.html#fit-model-using-lm-frequentist-approach" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can also fit the model using the Frequentist approach.
No priors. The results should be very similar due to the large sample size.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="multiple-linear-regression.html#cb234-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb234-2"><a href="multiple-linear-regression.html#cb234-2" tabindex="-1"></a><span class="fu">dim</span>(df) </span></code></pre></div>
<pre><code>## [1] 6919    5</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="multiple-linear-regression.html#cb236-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">Age_center =</span> Age <span class="sc">-</span> <span class="fu">mean</span>(Age))</span>
<span id="cb236-2"><a href="multiple-linear-regression.html#cb236-2" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(BPSysAve <span class="sc">~</span> PhysActive <span class="sc">+</span> Age_center <span class="sc">+</span> Gender, <span class="at">data =</span> df)</span>
<span id="cb236-3"><a href="multiple-linear-regression.html#cb236-3" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BPSysAve ~ PhysActive + Age_center + Gender, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -55.736  -9.245  -1.160   8.254 103.561 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   119.14860    0.32495 366.665   &lt;2e-16 ***
## PhysActiveYes  -0.73693    0.37750  -1.952    0.051 .  
## Age_center      0.41949    0.01115  37.610   &lt;2e-16 ***
## Gendermale      4.48810    0.37134  12.086   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.42 on 6915 degrees of freedom
## Multiple R-squared:  0.1875, Adjusted R-squared:  0.1872 
## F-statistic:   532 on 3 and 6915 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="multiple-linear-regression.html#cb238-1" tabindex="-1"></a><span class="fu">check_model</span>(mod)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-92-1.png" width="672" /></p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="multiple-linear-regression.html#cb239-1" tabindex="-1"></a><span class="fu">qqPlot</span>(mod) <span class="co"># bad</span></span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-92-2.png" width="672" /></p>
<pre><code>## [1] 1232 5501</code></pre>
<p>The results are similar to the Bayesian approach.
In the summary output we see that the effect of physical activity is <span class="math inline">\(-0.73693\)</span>
with a small <span class="math inline">\(p\)</span>-value (<span class="math inline">\(0.051\)</span>). This is one example for the absurdity of strict cutoffs
for <span class="math inline">\(p\)</span>-values. The other coefficients including the estimate for <span class="math inline">\(\sigma\)</span> are also similar.
It’s alsways a good idea to double-check.</p>
<p>Model fit could be better, the model thinks the blood pressure is normally distributed.
There also seems to be some heteroscadasticity and the residuals are not normally distributed.</p>
<p><strong>Usually</strong>, one would fit this model by throwing in all variables into the model, including BMI.
Let’s see if this approach changes the results:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="multiple-linear-regression.html#cb241-1" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(BPSysAve <span class="sc">~</span> PhysActive <span class="sc">+</span> Age <span class="sc">+</span> Gender  <span class="sc">+</span> BMI, <span class="at">data =</span> df) <span class="co"># add BMI</span></span>
<span id="cb241-2"><a href="multiple-linear-regression.html#cb241-2" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BPSysAve ~ PhysActive + Age + Gender + BMI, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -56.295  -9.414  -1.043   8.004 102.952 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   91.38776    1.04337  87.589   &lt;2e-16 ***
## PhysActiveYes -0.22870    0.37854  -0.604    0.546    
## Age            0.41728    0.01108  37.658   &lt;2e-16 ***
## Gendermale     4.43779    0.36887  12.031   &lt;2e-16 ***
## BMI            0.27237    0.02790   9.764   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.32 on 6914 degrees of freedom
## Multiple R-squared:  0.1986, Adjusted R-squared:  0.1981 
## F-statistic: 428.3 on 4 and 6914 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="multiple-linear-regression.html#cb243-1" tabindex="-1"></a><span class="fu">check_model</span>(mod)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="multiple-linear-regression.html#cb244-1" tabindex="-1"></a><span class="fu">qqPlot</span>(mod) </span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-93-2.png" width="672" /></p>
<pre><code>## [1] 1232 5501</code></pre>
<p><strong>This changed the results notably</strong>. Before, the effect of physical activity was <span class="math inline">\(-0.73693\)</span>
with a rather small <span class="math inline">\(p\)</span>-value,
now it is <span class="math inline">\(-0.22870\)</span> with a <span class="math inline">\(p\)</span>-value ten times as large.</p>
<p><strong>Remember this</strong>: The interpretation of a whole paper could change by falsely including
an additional variable (in this case BMI).</p>
</div>
<div id="improve_model_NHANES" class="section level4 hasAnchor" number="4.4.1.9">
<h4><span class="header-section-number">4.4.1.9</span> Improve model<a href="multiple-linear-regression.html#improve_model_NHANES" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now, that we have seen that the model fit is probably not sufficient, we could try to improve it.</p>
<p><strong>First</strong> order of business is the <strong>non-normality</strong> of the blood pressure values (outcome).
The model predicts normally distributed values, which is not the case.
It seems that the blood pressure values are right-skewed.</p>
<p>Let’s try a log-normal distribution for the outcome variable, which only
takes positive values and is skewed.</p>
<p>We would <strong>further</strong> try to model <span class="math inline">\(\sigma\)</span> as a linear function of <span class="math inline">\(\mu\)</span>.
For higher expected values of <em>BPSysAve</em>, we would expect higher variability.
Hence, we try to <strong>explicitely model heteroscadasticity</strong>.</p>
<p>Note, that we have no problem with adding parameters to the model
because of the large sample size.</p>
<p>See <a href="multiple-linear-regression.html#exercise16_multiple_regression">exercise 16</a>.</p>
<p><span class="math display">\[\begin{eqnarray*}
BPSysAve_i &amp;\sim&amp; \text{Log-Normal}(\mu_i, \sigma_i)\\
\sigma_i &amp;=&amp; \text{exp}(\beta_4 + \beta_5 \cdot \mu_i)\\
\mu_i &amp;=&amp; \beta_0 + \beta_1 \cdot PhysActive + \beta_2 \cdot Age_{centered} + \beta_3 \cdot Gender\\
\beta_0 &amp;\sim&amp; \text{Normal}(140, 20)\\
\beta_1 &amp;\sim&amp; \text{Normal}(0, 50)\\
\beta_2 &amp;\sim&amp; \text{Normal}(0, 10)\\
\beta_3 &amp;\sim&amp; \text{Normal}(0, 10)\\
\beta_4 &amp;\sim&amp; \text{Normal}(0, 10)\\
\beta_5 &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<p>We now allow the standard deviation <span class="math inline">\(\sigma_i\)</span> to be different for different values of the mean.
With <code>exp()</code> we ensure that <span class="math inline">\(\sigma_i\)</span> is positive.
This time, we assume: <span class="math inline">\(log(BPSysAve_i) \sim \text{Normal}(\mu_i, \sigma_i)\)</span>. That’s what it means
to be log-normally distributed.</p>
<p><strong>Fit improved model</strong></p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="multiple-linear-regression.html#cb246-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(df)) <span class="co"># 0</span></span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="multiple-linear-regression.html#cb248-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">122</span>)</span>
<span id="cb248-2"><a href="multiple-linear-regression.html#cb248-2" tabindex="-1"></a>m_NHANES_lnorm <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb248-3"><a href="multiple-linear-regression.html#cb248-3" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb248-4"><a href="multiple-linear-regression.html#cb248-4" tabindex="-1"></a>    BPSysAve <span class="sc">~</span> <span class="fu">dlnorm</span>(lmu, lsd), </span>
<span id="cb248-5"><a href="multiple-linear-regression.html#cb248-5" tabindex="-1"></a>    lsd <span class="ot">&lt;-</span> <span class="fu">exp</span>(beta_4 <span class="sc">+</span> beta_5 <span class="sc">*</span> lmu),</span>
<span id="cb248-6"><a href="multiple-linear-regression.html#cb248-6" tabindex="-1"></a>    lmu <span class="ot">&lt;-</span> beta_0 <span class="sc">+</span> beta_1[PhysActive] <span class="sc">+</span> beta_2 <span class="sc">*</span> (Age <span class="sc">-</span> Age_mean) <span class="sc">+</span> beta_3[Gender],</span>
<span id="cb248-7"><a href="multiple-linear-regression.html#cb248-7" tabindex="-1"></a>    beta_0 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">140</span>, <span class="dv">10</span>),  <span class="co"># </span></span>
<span id="cb248-8"><a href="multiple-linear-regression.html#cb248-8" tabindex="-1"></a>    beta_1[PhysActive] <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),  <span class="co"># </span></span>
<span id="cb248-9"><a href="multiple-linear-regression.html#cb248-9" tabindex="-1"></a>    beta_2 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb248-10"><a href="multiple-linear-regression.html#cb248-10" tabindex="-1"></a>    beta_3[Gender] <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),  <span class="co"># </span></span>
<span id="cb248-11"><a href="multiple-linear-regression.html#cb248-11" tabindex="-1"></a>    beta_4 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),  <span class="co"># </span></span>
<span id="cb248-12"><a href="multiple-linear-regression.html#cb248-12" tabindex="-1"></a>    beta_5 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>)  <span class="co">#</span></span>
<span id="cb248-13"><a href="multiple-linear-regression.html#cb248-13" tabindex="-1"></a>  ),</span>
<span id="cb248-14"><a href="multiple-linear-regression.html#cb248-14" tabindex="-1"></a>  <span class="at">data =</span> df,</span>
<span id="cb248-15"><a href="multiple-linear-regression.html#cb248-15" tabindex="-1"></a>  <span class="at">start =</span> <span class="fu">list</span>(<span class="at">beta_0 =</span> <span class="dv">140</span>, <span class="at">beta_1 =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">0</span>), <span class="at">beta_2 =</span> <span class="dv">0</span>, <span class="at">beta_3 =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">0</span>), <span class="at">beta_4 =</span> <span class="dv">1</span>, <span class="at">beta_5 =</span> <span class="dv">0</span>)</span>
<span id="cb248-16"><a href="multiple-linear-regression.html#cb248-16" tabindex="-1"></a>)</span>
<span id="cb248-17"><a href="multiple-linear-regression.html#cb248-17" tabindex="-1"></a></span>
<span id="cb248-18"><a href="multiple-linear-regression.html#cb248-18" tabindex="-1"></a><span class="fu">precis</span>(m_NHANES_lnorm, <span class="at">depth =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                    mean           sd          5.5%         94.5%
## beta_0     69.940815482 7.0688162635  58.643481822  81.238149141
## beta_1[1] -33.089453949 6.1218905701 -42.873417461 -23.305490436
## beta_1[2] -33.092785217 6.1218905660 -42.876748723 -23.308821711
## beta_2      0.003334919 0.0000845877   0.003199732   0.003470107
## beta_3[1] -32.083866696 6.1249525109 -41.872723781 -22.295009611
## beta_3[2] -32.042155145 6.1249525254 -41.831012254 -22.253298037
## beta_4    -14.545840087 0.7396301298 -15.727911886 -13.363768287
## beta_5      2.594743962 0.1546084882   2.347649737   2.841838187</code></pre>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="multiple-linear-regression.html#cb250-1" tabindex="-1"></a><span class="co"># difference in levels of PhysActive and Gender (catorical variables):</span></span>
<span id="cb250-2"><a href="multiple-linear-regression.html#cb250-2" tabindex="-1"></a>post_lnorm <span class="ot">&lt;-</span> <span class="fu">extract.samples</span>(m_NHANES_lnorm)</span>
<span id="cb250-3"><a href="multiple-linear-regression.html#cb250-3" tabindex="-1"></a>post_lnorm<span class="sc">$</span>diff_PhysActive <span class="ot">&lt;-</span> post_lnorm<span class="sc">$</span>beta_1[,<span class="dv">2</span>] <span class="sc">-</span> post_lnorm<span class="sc">$</span>beta_1[,<span class="dv">1</span>]</span>
<span id="cb250-4"><a href="multiple-linear-regression.html#cb250-4" tabindex="-1"></a>post_lnorm<span class="sc">$</span>diff_G <span class="ot">&lt;-</span> post_lnorm<span class="sc">$</span>beta_3[,<span class="dv">2</span>] <span class="sc">-</span> post_lnorm<span class="sc">$</span>beta_3[,<span class="dv">1</span>]</span>
<span id="cb250-5"><a href="multiple-linear-regression.html#cb250-5" tabindex="-1"></a><span class="fu">precis</span>(post_lnorm, <span class="at">depth =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                          mean           sd          5.5%         94.5%
## beta_0           69.802740693 7.085286e+00  58.450093685  8.100613e+01
## beta_2            0.003333900 8.420378e-05   0.003198976  3.469766e-03
## beta_4          -14.557397328 7.340126e-01 -15.730856478 -1.339366e+01
## beta_5            2.597170489 1.534331e-01   2.353734581  2.842822e+00
## beta_1[1]       -33.013184105 6.159037e+00 -42.880222934 -2.329850e+01
## beta_1[2]       -33.016534396 6.159079e+00 -42.884893477 -2.330076e+01
## beta_3[1]       -32.022042725 6.166508e+00 -41.656953623 -2.200235e+01
## beta_3[2]       -31.980336296 6.166495e+00 -41.611161302 -2.196006e+01
## diff_PhysActive  -0.003350291 2.668752e-03  -0.007636024  9.712922e-04
## diff_G            0.041706430 2.640623e-03   0.037460961  4.588615e-02
##                      histogram
## beta_0           ▁▁▁▁▂▅▇▇▃▂▁▁▁
## beta_2          ▁▁▁▁▃▇▇▇▃▂▁▁▁▁
## beta_4            ▁▁▁▂▅▇▇▃▂▁▁▁
## beta_5          ▁▁▁▁▂▅▇▇▅▂▁▁▁▁
## beta_1[1]           ▁▁▁▂▅▇▅▂▁▁
## beta_1[2]           ▁▁▁▂▅▇▅▂▁▁
## beta_3[1]           ▁▁▂▅▇▅▂▁▁▁
## beta_3[2]           ▁▁▂▅▇▅▂▁▁▁
## diff_PhysActive    ▁▁▁▃▇▇▅▂▁▁▁
## diff_G             ▁▁▁▂▅▇▇▃▁▁▁</code></pre>
<p><code>diff_PhysActive</code> is now <span class="math inline">\(-0.003350291\)</span> with a very tight credible interval
containing <span class="math inline">\(0\)</span> as a credible effect. The model is pretty sure, that the effect
is practically zero.</p>
<p>Let’s vizualize the posterior predictive distributions of 100 samples:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="multiple-linear-regression.html#cb252-1" tabindex="-1"></a><span class="co"># posterior predictive checks</span></span>
<span id="cb252-2"><a href="multiple-linear-regression.html#cb252-2" tabindex="-1"></a>sample_BP <span class="ot">&lt;-</span> <span class="fu">sim</span>(m_NHANES_lnorm, <span class="at">n =</span> <span class="dv">1000</span>)</span>
<span id="cb252-3"><a href="multiple-linear-regression.html#cb252-3" tabindex="-1"></a></span>
<span id="cb252-4"><a href="multiple-linear-regression.html#cb252-4" tabindex="-1"></a><span class="co"># Convert the first 100 rows of the posterior samples into a long format for ggplot</span></span>
<span id="cb252-5"><a href="multiple-linear-regression.html#cb252-5" tabindex="-1"></a>df_posterior <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(sample_BP[<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,])) <span class="sc">%&gt;%</span></span>
<span id="cb252-6"><a href="multiple-linear-regression.html#cb252-6" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">&quot;Simulation&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;BPSysAve_sim&quot;</span>)</span>
<span id="cb252-7"><a href="multiple-linear-regression.html#cb252-7" tabindex="-1"></a></span>
<span id="cb252-8"><a href="multiple-linear-regression.html#cb252-8" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb252-9"><a href="multiple-linear-regression.html#cb252-9" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb252-10"><a href="multiple-linear-regression.html#cb252-10" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">data =</span> df_posterior, <span class="fu">aes</span>(<span class="at">x =</span> BPSysAve_sim, <span class="at">group =</span> Simulation), </span>
<span id="cb252-11"><a href="multiple-linear-regression.html#cb252-11" tabindex="-1"></a>               <span class="at">color =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb252-12"><a href="multiple-linear-regression.html#cb252-12" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> BPSysAve), <span class="at">color =</span> <span class="st">&quot;green&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb252-13"><a href="multiple-linear-regression.html#cb252-13" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Density Estimation: Original vs. Posterior Samples&quot;</span>,</span>
<span id="cb252-14"><a href="multiple-linear-regression.html#cb252-14" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Systolic Blood Pressure&quot;</span>,</span>
<span id="cb252-15"><a href="multiple-linear-regression.html#cb252-15" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb252-16"><a href="multiple-linear-regression.html#cb252-16" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb252-17"><a href="multiple-linear-regression.html#cb252-17" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<p>With respect to the distribution of blood pressure, the model looks much better now.
Note, that we did not assume that the residuals are normally distributed, neither
did we assume homoscedasticity (on the contrary).</p>
<p>See <a href="multiple-linear-regression.html#exercise17_multiple_regression">exercise 17</a>.</p>
</div>
<div id="interpret-and-report-results" class="section level4 hasAnchor" number="4.4.1.10">
<h4><span class="header-section-number">4.4.1.10</span> Interpret and report results<a href="multiple-linear-regression.html#interpret-and-report-results" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Summarizing the results, we find the expected direction of association in our first model.
The effect of physical activity on blood pressure was negative.
After improving the model fit, we concluded that the small but clinically irrelevant effect
from the first model dissapeared.
It would be interesting to see how other variables containing more information
influence blood pressure, like <em>PhysActiveDays</em> (Number of days in a typical week
that participant does
moderate or vigorousintensity activity).</p>
</div>
</div>
<div id="concluding-remarks" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Concluding remarks<a href="multiple-linear-regression.html#concluding-remarks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The NHANES example was a somewhat realistic instance of a multiple regression model
which could easily be encountered in practice. In a Master thesis, we are often in
a low-data setting with 20-150 observations. Here, the priors are much more important.</p>
<p><a href="multiple-linear-regression.html#exercise18_multiple_regression">Exercise 18</a> asks you to repeat the analysis
with a smaller sample size.</p>
<p>We saw that we can readily adapt the model, if assumptions of the classical
linear regression model are violated. In our case, we had to model the outcome
as log-normal in order to allow for the model to predict skewed blood pressure values.
In addition, we explicitly modeled heteroscedasticity by allowing the standard deviation
to grow with the mean. Theoretically, one could build arbitrarily complex models
this way. Of course, syntax problems and convergence issues could arise.
Even in this example some starting values were not fit for purpose, which is why
I had to add the <code>set.seed()</code> command in the improved model.</p>
</div>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Exercises<a href="multiple-linear-regression.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>[E] Easy, [M] Medium, [H] Hard</p>
<p>(Some) solutions to exercises can be found in the git-repo <a href="https://github.com/jdegenfellner/Script_QM2_ZHAW/tree/main/Solutions_Exercises">here</a>.</p>
<div id="exercise1_multiple_regression" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> [M] Exercise 1<a href="multiple-linear-regression.html#exercise1_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Fit a model with a cubic term for weight and height of the !Kung San people.</li>
<li>Add the prediction bands as seen in the book.</li>
<li>Come up with an explanation for the functional form of this relationship.</li>
<li>Could there be reasons to for taking a less complicated model
(<a href="https://en.wikipedia.org/wiki/Statistical_model_specification">1</a>, <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">2</a>)?</li>
</ul>
</div>
<div id="exercise2_multiple_regression" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> [E] Exercise 2<a href="multiple-linear-regression.html#exercise2_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the model equations from <a href="multiple-linear-regression.html#adding_transformed_predictor_bayes">above</a>
where we used polynomial regression to model the relationship between
weight and height:</p>
<ul>
<li>Draw the model hierarchy for the model.</li>
</ul>
</div>
<div id="exercise3_multiple_regression" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> [H] Exercise 3<a href="multiple-linear-regression.html#exercise3_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Invent a data set (or use the first 3 lines of a previous data set)
with 3 observations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1, X_2\)</span> and <span class="math inline">\(X_3\)</span>. You have a data frame
with 3 rows and 4 columns.</p>
<ul>
<li>Fit a model with <span class="math inline">\(Y\)</span> as the dependent variable and <span class="math inline">\(X_1, X_2, X_3\)</span> as predictors.</li>
<li>How big is <span class="math inline">\(R^2\)</span>?</li>
<li>Could you have calulated this without <code>lm</code>and R?</li>
</ul>
</div>
<div id="exercise4_multiple_regression" class="section level3 hasAnchor" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> [E] Exercise 4<a href="multiple-linear-regression.html#exercise4_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the <a href="multiple-linear-regression.html#interaction_term">section about the interaction term</a> in the linear model.</p>
<ul>
<li>Use the code provided.</li>
<li>Standardise the predictors. How are the <span class="math inline">\(\beta\)</span>s changing and what is their interpration now?</li>
<li>Change the relative sizes of the true but usually unknown <span class="math inline">\(\beta\)</span>s.
What happens to the estimates and the graph?</li>
<li>What happens if you change the error term and increase or decrease its variance?</li>
</ul>
</div>
<div id="exercise5_multiple_regression" class="section level3 hasAnchor" number="4.5.5">
<h3><span class="header-section-number">4.5.5</span> [E] Exercise 5<a href="multiple-linear-regression.html#exercise5_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Draw the interaction plot from the <a href="multiple-linear-regression.html#interaction_plot">section about the interaction plot</a>
for the case when there is no interaction, i.e. <span class="math inline">\(\beta_3 = 0\)</span>.</p>
</div>
<div id="exercise6_multiple_regression" class="section level3 hasAnchor" number="4.5.6">
<h3><span class="header-section-number">4.5.6</span> [M] Exercise 6<a href="multiple-linear-regression.html#exercise6_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the model assumptions checks <a href="multiple-linear-regression.html#check_model_bayes">above</a>.</p>
<ul>
<li>Create the same two plots for the simple mean model without predictors, just with the intercept.</li>
<li>Which model fits the data better according to these posterior predictive checks?</li>
</ul>
</div>
<div id="exercise7_multiple_regression" class="section level3 hasAnchor" number="4.5.7">
<h3><span class="header-section-number">4.5.7</span> [E] Exercise 7<a href="multiple-linear-regression.html#exercise7_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the Simpson’s paradox <a href="multiple-linear-regression.html#simpsons_paradox">section</a>.</p>
<ul>
<li>Invent your own example for the pipe, fork and collider.</li>
</ul>
</div>
<div id="exercise8_multiple_regression" class="section level3 hasAnchor" number="4.5.8">
<h3><span class="header-section-number">4.5.8</span> [M] Exercise 8<a href="multiple-linear-regression.html#exercise8_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Take a data set of your choosing with many columns, say 10 or so, either from the internet
or from R (if available)</li>
<li>Fit a model for an arbitrary outcome, add more and more variables to predict the outcome
and verify that the <span class="math inline">\(R^2\)</span> increases.</li>
</ul>
</div>
<div id="exercise9_multiple_regression" class="section level3 hasAnchor" number="4.5.9">
<h3><span class="header-section-number">4.5.9</span> [M] Exercise 9<a href="multiple-linear-regression.html#exercise9_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Exponential curve fitting. Go back to the <a href="multiple-linear-regression.html#adding_transformed_predictor_freq">section</a> about adding a transformed predictor in the Frequentist
setting. Hint: You can use the <code>optim</code> function in R.</p>
<ul>
<li>Assume the relationship between weight and height looks like this:
<span class="math inline">\(height_i = \alpha + \beta_1 e^{\beta_2 weight_i}\)</span>.</li>
<li>Use R and the least squares method to estimate the parameters <span class="math inline">\(\alpha, \beta_1, \beta_2\)</span>.</li>
<li>Note that the sum of squared errors is this:
<span class="math inline">\(\sum_{i=1}^n (height_i - \alpha - \beta_1 e^{\beta_2 weight_i})^2\)</span>.</li>
<li>What happens if you do not constrain the parameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> to be negative?</li>
<li>Calculate the <span class="math inline">\(R^2\)</span> for this model.</li>
</ul>
</div>
<div id="exercise10_multiple_regression" class="section level3 hasAnchor" number="4.5.10">
<h3><span class="header-section-number">4.5.10</span> [E] Exercise 10<a href="multiple-linear-regression.html#exercise10_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s try to verify if Berkson’s paradox (which we have mentioned in the
<a href="multiple-linear-regression.html#collider">collider-section</a>) also works for three variables in a sum score.</p>
<ul>
<li>Now, we assume some college admits only applicants in the top 20% of a score
consisting of the sum of three variables: <span class="math inline">\(W, X, Z\)</span> (grade point average, math score, verbal score).</li>
<li>All three scores are individually normally distributed with mean 100 and standard deviation 15.</li>
<li>Calculate the correlation matrix <code>cor()</code> of all students and the admitted students.</li>
<li>Are any of the three variables correlated?</li>
<li>Plot a scatterplot of the math score and the verbal score und color the points according
to being admitted or not. You can add trendlines for the two groups.</li>
</ul>
</div>
<div id="exercise11_multiple_regression" class="section level3 hasAnchor" number="4.5.11">
<h3><span class="header-section-number">4.5.11</span> [E] Exercise 11<a href="multiple-linear-regression.html#exercise11_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the multicollinearity <a href="multiple-linear-regression.html#multicollinearity">section</a> and the <a href="multiple-linear-regression.html#mcElreath_6.1">example</a>
from McElreath 6.1.</p>
<ul>
<li>Verify that the coefficient is correct when leaving out the right leg length from the model.</li>
</ul>
</div>
<div id="exercise12_multiple_regression" class="section level3 hasAnchor" number="4.5.12">
<h3><span class="header-section-number">4.5.12</span> [M] Exercise 12<a href="multiple-linear-regression.html#exercise12_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go backt to the interpretation of the intercept in the quadratic height model <a href="multiple-linear-regression.html#interpretation_output_freq_quadratic">above</a>.</p>
<ul>
<li>Why is the intercept not equal to the sample mean of the heights?</li>
</ul>
</div>
<div id="exercise13_multiple_regression" class="section level3 hasAnchor" number="4.5.13">
<h3><span class="header-section-number">4.5.13</span> [H] Exercise 13<a href="multiple-linear-regression.html#exercise13_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the section about the <a href="multiple-linear-regression.html#fork">fork</a> and the example about smoking at the end.</p>
<ul>
<li>Draw a DAG for this example.</li>
<li>Create data in R, where you assume that the probability of carrying a lighter is higher in smokers,
the probability of lung cancer is higher in smokers.</li>
<li>Show that the association between carrying a lighter and lung cancer disappears when conditioning on smoking.</li>
<li>You may also invent an example relevant to the field of physiotherapy.</li>
</ul>
</div>
<div id="exercise14_multiple_regression" class="section level3 hasAnchor" number="4.5.14">
<h3><span class="header-section-number">4.5.14</span> [H] Exercise 14<a href="multiple-linear-regression.html#exercise14_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Perform extensive EDA on the NHANES data set implemented in R.
See <a href="multiple-linear-regression.html#example_nhanes">above</a>.</li>
</ul>
</div>
<div id="exercise15_multiple_regression" class="section level3 hasAnchor" number="4.5.15">
<h3><span class="header-section-number">4.5.15</span> [H] Exercise 15<a href="multiple-linear-regression.html#exercise15_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the NHANES model <a href="multiple-linear-regression.html#prior_predictive_checks_NHANES">above</a>.</p>
<ul>
<li>Play around with the priors until the prior predictive check produces plausible values.</li>
</ul>
</div>
<div id="exercise16_multiple_regression" class="section level3 hasAnchor" number="4.5.16">
<h3><span class="header-section-number">4.5.16</span> [M] Exercise 16<a href="multiple-linear-regression.html#exercise16_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go to the improved model fit of our NHANES example <a href="multiple-linear-regression.html#improve_model_NHANES">above</a>.</p>
<ul>
<li>Draw the model hierarchy for the model.</li>
</ul>
</div>
<div id="exercise17_multiple_regression" class="section level3 hasAnchor" number="4.5.17">
<h3><span class="header-section-number">4.5.17</span> [H] Exercise 17<a href="multiple-linear-regression.html#exercise17_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the improved model fit of our NHANES example <a href="multiple-linear-regression.html#improve_model_NHANES">above</a>.</p>
<ul>
<li>Draw observed vs. model-predicted blood pressure values.</li>
<li>add the <span class="math inline">\(y=x\)</span> line to the plot.</li>
<li>Is the fit better compared to the first model?</li>
</ul>
</div>
<div id="exercise18_multiple_regression" class="section level3 hasAnchor" number="4.5.18">
<h3><span class="header-section-number">4.5.18</span> [H] Exercise 18<a href="multiple-linear-regression.html#exercise18_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the NHANES example <a href="multiple-linear-regression.html#example_nhanes">above</a>.</p>
<ul>
<li>Repeat the NHANES analysis with a smaller sample size.</li>
<li>Draw 50-100 rows randomly from the adult NHANES data set we have used for the full
analysis.</li>
<li>What do you observe?</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reliability-and-validity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-Multiple_Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
